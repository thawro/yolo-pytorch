| Year | Done | Approach | Link | Used in | Title |
| :--- | :---: | :---- | :--- | :--- | :---- |
| 2015 | ‚úÖ | [SPP](#SPP) | [paper](https://arxiv.org/pdf/1406.4729.pdf) | v4, v5, PP | Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition |
| 2017 | ‚úÖ | [FPN](#FPN) | [paper](https://arxiv.org/pdf/1612.03144.pdf) | v4 | Feature Pyramid Networks for Object Detection |
| 2017 | ‚úÖ | [Soft NMS](#Soft-NMS) | [paper](https://arxiv.org/pdf/1704.04503.pdf) | v4 | Improving Object Detection With One Line of Code |
| 2017 | ‚úÖ | [Cosine Annealing](#Cosing-Annealing) | [paper](https://arxiv.org/pdf/1608.03983.pdf) | v4 | SGDR: Stochastic Gradient Descent with Warm Restarts |
| 2017 | üî≥ | [SiLU](#SiLU) | [paper](https://arxiv.org/pdf/1702.03118v3) | X | Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning |
| 2017 | ‚úÖ | [Deformable Convolution](#Deformable-Convolution) | [paper](https://arxiv.org/pdf/1703.06211v3) | PP-v1, PP-v2 | Deformable Convolutional Networks |
| 2018 | ‚úÖ | [DropBlock](#DropBlock) | [paper](https://arxiv.org/pdf/1810.12890.pdf) | v4, PP | DropBlock: A regularization method for convolutional networks |
| 2018 | ‚úÖ | [MixUp](#MixUp) | [paper](http://arxiv.org/pdf/1710.09412) | X, PP | MixUp: Beyond Empirical Risk Minimization |
| 2018 | ‚úÖ | [Focal Loss / RetinaNet](#FocalLoss) | [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | v4 | Focal Loss for Dense Object Detection |
| 2018 | ‚úÖ | [CoordConv](#CoordConv) | [paper](https://arxiv.org/pdf/1807.03247v2) | PP | An intriguing failing of convolutional neural networks and the CoordConv solution |
| 2018 | ‚úÖ | [Linear LR Scaling](#Linear-LR-Scaling) | [paper](https://arxiv.org/pdf/1706.02677) | X, PP-E | Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour |
| 2018 | ‚úÖ | [PAN](#PAN) | [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | v6, X, PP | Path Aggregation Network for Instance Segmentation |
| 2018 | ‚úÖ | [GIoU](#GIoU) | [paper](https://arxiv.org/pdf/1902.09630) | PP-E | Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression |
| 2019 | ‚úÖ | [DIoU](#DIoU) | [paper](https://arxiv.org/pdf/1911.08287.pdf) | v4, PP-E | Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression |
| 2019 | ‚úÖ | [Mish](#Mish) | [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | v4, PP-v2 | Mish: A Self Regularized Non-Monotonic Neural Activation Function |
| 2019 | ‚úÖ | [Bag Of Freebies](#Bag-Of-Freebies) | [paper](https://arxiv.org/pdf/1902.04103) | X | Bag of Freebies for Training Object Detection Neural Networks |
| 2019 | ‚úÖ | [FCOS](#FCOS) | [paper](https://arxiv.org/pdf/1904.01355) | X, PP-E | FCOS: Fully Convolutional One-Stage Object Detection |
| 2019 | ‚úÖ | [CSP](#CSP) | [paper](https://arxiv.org/pdf/1911.11929.pdf) | X, PP-v2, v6, v9 | CSPNet: A new backbone that can enhance learning capability of CNN |
| 2020 | ‚úÖ | [ATSS](#ATSS) | [paper](https://arxiv.org/pdf/1912.02424) | v6.3 | Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection |
| 2020 | ‚úÖ | [Effective Squeeze and Excitation](#Effective-Squeeze-And-Excitation) | [paper](https://arxiv.org/pdf/1911.06667v6) | PP-E | CenterMask : Real-Time Anchor-Free Instance Segmentation |
| 2020 | ‚úÖ | [EfficientDet](#EfficientDet) | [paper](https://arxiv.org/pdf/1911.09070) | v6.3 | EfficientDet: Scalable and Efficient Object Detection |
| 2020 | ‚úÖ | [IoU-Aware](#IoU-Aware) | [paper](https://arxiv.org/pdf/1912.05992) | PP-v2, PP-E | IoU-aware Single-stage Object Detector for Accurate Localization |
| 2020 | üî≥ | [Distribution/Quality Focal Loss (DFL/QFL)](#Distribution-Focal-Loss) | [paper](https://arxiv.org/pdf/2006.04388) | PP-E | Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection |
| 2020 | üî≥ | [Matrix-NMS](#Matrix-NMS) | [paper](https://arxiv.org/pdf/2003.10152v3) | PP-v1, PP-v2 | SOLOv2: Dynamic and Fast Instance Segmentation |
| 2021 | üî≥ | [NoNMS-YOLO](#NoNMS-YOLO) | [paper](https://arxiv.org/pdf/2101.11782) | X | Object Detection Made Simpler by Eliminating Heuristic NMS |
| 2021 | üî≥ | [CopyPaste](#CopyPaste) | [paper](https://arxiv.org/pdf/2012.07177) | X | Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation |
| 2021 | üî≥ | [OTA](#OTA) | [paper](https://arxiv.org/pdf/2103.14259) | X | OTA: Optimal Transport Assignment for Object Detection |
| 2021 | üî≥ | [Varifocal Loss](#Varifocal-Loss) | [paper](https://arxiv.org/pdf/2008.13367) | PP-E | VarifocalNet: An IoU-aware Dense Object Detector |
| 2021 | üî≥ | [Task Aligned Learning](#Task-Aligned-Learning) | [paper](https://arxiv.org/pdf/2108.07755) | PP-E | TOOD: Task-aligned One-stage Object Detection |
| 2021 | üî≥ | [PP-PicoDet](#PP-PicoDet) | [paper](https://arxiv.org/pdf/2111.00902) | PP-E | PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices |
| 2021 | ‚úÖ | [RepVGG](#RepVGG) | [paper](https://arxiv.org/pdf/2101.03697.pdf) | PP-E, v6 | RepVGG: Making VGG-style ConvNets Great Again |
| 2022 | üî≥ | [ELAN](#ELAN) | [paper](https://arxiv.org/pdf/2211.04800) | v9 | Designing Network Design Strategies Through Gradient Path Analysis |
| 2023 | üî≥ | [PRB-FPN](#PRB-FPN) | [paper](https://arxiv.org/pdf/2012.01724) | v6.3 | Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection |


# SPP
2015 | [paper](https://arxiv.org/pdf/1406.4729.pdf) | _Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition_

Authors of the paper introduced a Spatial Pyramid Pooling (**SPP**) layer to remove the fixed-size constraint of the network. Specifically, added an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed length outputs, which are then fed into the fullyconnected layers (or other classifiers). In other words, we perform some information ‚Äúaggregation‚Äù at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20e4130d-8b7d-40be-9e03-3de729989654" alt="SPP" height="350"/>
</p>

Spatial pyramid pooling (also knows as spatial pyramid matching or SPM), as an extension of the Bag-of-Words (BoW) model, is one of the most successful methods in computer vision. It partitions the image into divisions from finer to coarser levels, and aggregates local features in them. SPP has long been a key component in the leading and competition-winning systems for classification and detection  before the prevalence of CNNs. Nevertheless, SPP has not been considered in the context of CNNs before this paper. Authors noted that SPP has several remarkable properties for deep CNNs: 

*  SPP is able to generate a fixed-length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot
* SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size. Multi-level pooling has been shown to be robust to object deformations
* SPP can pool features extracted at variable scales thanks to the flexibility of input scales. Through experiments authors have shown that all these factors elevate the recognition accuracy of deep networks.

SPP-net not only makes it possible to generate representations from arbitrarily sized images/windows for testing, but also allows us to feed images with varying sizes or scales during training. Training with variable-size images increases scale-invariance and reduces over-fitting. 

> **_NOTE:_** Most of the YOLO approaches uses only the idea of modified **SPP block** (e.g. SPPF in YOLOv5) - applying _max-pooling_ operations with different kernel sizes (with padding) and then concatenating the results to form a single feature map, which is passed to next blocks.




# FPN
2017 | [paper](https://arxiv.org/pdf/1612.03144.pdf) | _Feature Pyramid Networks for Object Detection_

Authors of the paper exploited the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (**FPN**), shows significant improvement as a generic feature extractor in several applications. 

Pyramid approaches comparison:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/4ee4eeee-c9a4-4873-8dce-150ab73c7262" alt="pyramids_comparison" height="350"/>
</p>

A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi- scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition

FPN is a feature pyramid that has strong semantics at all scales and is built quickly from a single input image scale. It combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.

FPN takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. The construction of the pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, as introduced in the following:

FPN compared to [similar](https://arxiv.org/pdf/1603.08695) approach:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8b5553e8-d1f0-4a4e-afab-91cd51e6b079" alt="FPN" height="350"/>
</p>


* **Bottom-up pathway** - the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of $2$. There are often many layers producing output maps of the same size - these layers are in the same network stage. For the feature pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is chosen as the reference set of feature maps, which will be enriched to create the pyramid. This choice is natural since the deepest layer of each stage should have the strongest features. Specifically, for ResNets the feature activations output by each stage‚Äôs last residual block are used. The output of these last residual blocks are denoted as ${C_2, C_3, C_4, C_5}$ for _conv2_, _conv3_, _conv4_, and _conv5_ outputs, with corresponding strides of ${4, 8, 16, 32}$ pixels with respect to the input image. The _conv1_ is not included into the pyramid due to its large memory footprint.

* **Top-down pathway and lateral connections** - it hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times. The building block shown below constructs the top-down feature maps.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c20d0886-7ea0-4e47-89ef-3655d6e8e5df" alt="FPN_block" height="350"/>
</p>

With a coarser-resolution feature map, the spatial resolution is upsampled by a factor of $2$ (using nearest neighbor upsampling for simplicity). The upsampled map is then merged with the corresponding bottom-up map (which undergoes a $1 √ó 1$ convolutional layer to reduce channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated. Iteration is started by simply attaching a $1 √ó 1$ convolutional layer on $C_5$ to produce the coarsest resolution map. Finally, a $3 √ó 3$ convolution is appended on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. This final set of feature maps is called ${P_2, P_3, P_4, P_5}$, corresponding to ${C_2, C_3, C_4, C_5}$ that are respectively of the same spatial sizes. Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, the feature dimension (numbers of channels, denoted as $d$) is fixed in all the feature maps. $d = 256$ in this paper and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which has been empirically found to have minor impacts. The more sophisticated blocks (e.g., using multi-layer residual blocks as the connections) have been tested and observed marginally better results.

> **_NOTE:_** Most of the YOLO approaches uses the idea of connecting the backbone path (**Bottom-up pathway**) with the neck path **Top-down pathway** by the use of the **lateral connections**. The FPN is further improved to PAN and finally to BiFPN neck configurations.


# Soft NMS
2017 | [paper](https://arxiv.org/pdf/1704.04503.pdf) | _Improving Object Detection With One Line of Code_

Non-maximum suppression (**NMS**) is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box $M$ with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with $M$ are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. 

Authors proposed an improved version of NMS, that is a _**Soft-NMS**_, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with _M_. Hence, no object is eliminated in this process.

During the analysis of old NMS, authors wanted the improved metric to take the following conditions into account:
* Score of neighboring detections should be decreased to an extent that they have a smaller likelihood of increasing the false positive rate, while being above obvious false positives in the ranked list of detections.
* Removing neighboring detections altogether with a low NMS threshold would be sub-optimal and would increase the miss-rate when evaluation is performed at high overlap thresholds.
* Average precision measured over a range of overlap thresholds would drop when a high NMS threshold is used

It would be ideal if the penalty function was continuous, otherwise it could lead to abrupt changes to the ranked list of detections. A continuous penalty function should have no penalty when there is no overlap and very high penalty at a high overlap. Also, when the overlap is low, it should increase the penalty gradually, as $M$ should not affect the scores of boxes which have a very low overlap with it. However, when overlap of a box $b_i$ with $M$ becomes close to one, $b_i$ should be significantly penalized. Taking this into consideration, authors proposed a Gaussian penalty function:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/24ac7cb8-9152-49b3-9127-b13b03f98bf7" alt="soft_NMS_eq" height="60"/>
</p>

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/db925ea6-ff65-4af1-9be3-023f1b4f5a0f" alt="soft_NMS_algo" height="350"/>
</p>

This update rule (top) is applied in each iteration and scores of all remaining detection boxes are updated

> **_NOTE:_** Some of the YOLO approaches use the idea of **Soft-NMS** for post processing. Soft-NMS is improved later with the use of DIoU.


# Cosine Annealing
2017 | [paper](https://arxiv.org/pdf/1608.03983.pdf) | _SGDR: Stochastic Gradient Descent with Warm Restarts_

Cosine Annealing with warm restarts is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a "warm restart" in contrast to a "cold restart" where a new set of small random numbers may be used as a starting point.

Authors of the work considered one of the simplest warm restart approaches. They simulated a new warm-started run / restart of SGD once $T_i$ epochs are performed, where $i$ is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing the learning rate $Œ∑_t$ while the old value of $x_t$ is used as an initial solution. The amount of this increase controls to which extent the previously acquired information (e.g., momentum) is used. Within the $i$-th run, the learning rate is decayed with a cosine annealing for each batch as follows:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/a9c1d5c7-2912-451c-bd63-5b0006052bd8" alt="cosine_annealing_eq" height="70"/>
</p>

where $Œ∑^i_{min}$ and $Œ∑^i_{max}$ are ranges for the learning rate, and $T_{cur}$ accounts for how many epochs have been performed since the last restart. Since $T_{cur}$ is updated at each batch iteration $t$, it can take discredited values such as $0.1$, $0.2$, etc. Thus, $Œ∑_t = Œ∑^i_{max}$ when $t = 0$ and $T_{cur} = 0$. Once $T_{cur} = T_i$, the cos function will output $‚àí1$ and thus $Œ∑_t = Œ∑^i_{min}$. The decrease of the learning rate is shown below (left - without restart, right - with restart every ~2k iterations).

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2d2c4971-fd4a-4ab6-a6f8-bc068b60c624" alt="cosine_annealing" height="350"/>
</p>

> **_NOTE:_** Some of the YOLO approaches use the idea of **Cosine Annealing** (without warm restarts) for learning rate scheduling.



# SiLU
2017 | [paper](https://arxiv.org/pdf/1702.03118v3) | _Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning_
TODO



# Deformable Convolution
2017 | [paper](https://arxiv.org/pdf/1703.06211v3) | _Deformable Convolutional Networks_

Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, authors introduce two new modules to enhance the transformation modeling capability of CNNs, namely, _deformable convolution_ and _deformable RoI pooling_. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks

* **Deformable Convolution** - it adds 2D offsets to the regular grid sampling locations in the stan-
dard convolution. It enables free form deformation of the sampling grid. It is illustrated in figure above. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner

* **Deformable RoI pooling**  - it adds an offset to each bin position in the regular bin partition of the previous RoI pooling. Similarly, the offsets are learned from the preceding feature maps and the RoIs, enabling adaptive part localization for objects with different shapes. Both modules are light weight. They add small amount of parameters and computation for the offset learning. They can readily replace their plain counterparts in deep CNNs and can be easily trained end-to-end with standard back- propagation. The resulting CNNs are called deformable convolutional networks, or deformable ConvNets

## Deformable Convolution

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/70c791c6-fc3a-497f-a5a0-3054c6382703" alt="DeformConv_offsets" height="350"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/cc97f072-7dde-4493-a65e-e828f13a3178" alt="DeformConv_block" height="350"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/4ac49c06-2352-4fb3-956e-86f7f9a642ee" alt="DeformConv_receptive_field" height="350"/>
</p>

The 2D convolution consists of two steps: 

1. sampling using a regular grid R over the input feature map x
2. summation of sampled values weighted by $w$

The grid $R$ defines the receptive field size and dilation. For example

$$ R = {(‚àí1, ‚àí1), (‚àí1, 0), . . . , (0, 1), (1, 1)}$$

defines a $3 √ó 3$ kernel with $dilation = 1$. For each location $p_0$ on the output feature map $y$, we
have

$$ y(p_0) = \sum_{p_n ‚àà R}w(p_n) ¬∑ x(p_0 + p_n) $$

where $p_n$ enumerates the locations in $R$. In deformable convolution, the regular grid $R$ is augmented with offsets ${‚àÜp_n | n = 1, ..., N }$, where $N = |R|$. Equation above becomes

$$ y(p_0) = \sum_{p_n ‚àà R}w(p_n) ¬∑ x(p_0 + p_n + ‚àÜp_n) $$

,the sampling is on the irregular and offset locations $p_n + ‚àÜp_n$. As the offset $‚àÜp_n$ is typically fractional, Equation above is implemented via bilinear interpolation as:

$$ x(p) = \sum_{q} G(q, p) ¬∑ x(q) $$

where $p$ denotes an arbitrary (fractional) location ($p = p_0 + p_n + ‚àÜp_n$ for Equation above), $q$ enumerates all integral spatial locations in the feature map $x$, and $G(¬∑, ¬∑)$ is the bilinear interpolation kernel. Note that $G$ is two dimensional. It is separated into two one dimensional kernels as

$$ G(q, p) = g(q_x, p_x) ¬∑ g(q_y , p_y) $$

where $g(a, b) = max(0, 1 ‚àí |a ‚àí b|)$. Eq. (3) is fast to compute as $G(q, p)$ is non-zero only for a few $q_s$.

As illustrated in figures above, the offsets are obtained by applying a convolutional layer over the same input feature map. The convolution kernel is of the same spatial resolution and dilation as those of the current convolutional layer (e.g., also $3 √ó 3$ with dilation 1 in figures). The output offset fields have the same spatial resolution with the input feature map. The channel dimension $2N$ corresponds to $N$ 2D offsets. During training, both the convolutional kernels for generating the output features and the offsets are learned simultaneously. To learn the offsets, the gradients are backpropagated through the bilinear operations.

## Deformable ConvNets

To integrate deformable ConvNets with the SoTA CNN architectures, authors note that these architectures consist of two stages. First, a deep fully convolutional network generates feature maps over the whole input image. Second, a shallow task specific network generates results from the feature maps.



# DropBlock
2018 | [paper](https://arxiv.org/pdf/1810.12890.pdf) | _DropBlock: A regularization method for convolutional networks_

Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers (in most cases it was used at the FC layers of the conv networks). This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. The main drawback of dropout is that it drops out features randomly. While this can be effective for fully connected layers, it is less effective for convolutional layers, where features are correlated spatially. When the features are correlated, even with dropout, information about the input can still be sent to the next layer, which causes the networks to overfit. This intuition suggests that a more structured form of dropout is needed to better regularize convolutional networks

**DropBlock** is a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d8718292-fb1e-44f8-9103-4034e1b73ade" alt="drop_block" height="350"/>
</p>

DropBlock is inspired by [Cutout](https://arxiv.org/pdf/1708.04552), a data augmentation method where parts of the input examples are zeroed out. DropBlock generalizes Cutout by applying Cutout at every feature map in a convolutional networks. In our experiments, having a fixed zero-out ratio for DropBlock during training is not as robust as having an increasing schedule for the ratio during training. In other words, it‚Äôs better to set the DropBlock ratio to be small initially during training, and linearly increase it over time during training.

DropBlock is a simple method similar to dropout. Its main difference from dropout is that it drops contiguous regions from a feature map of a layer instead of dropping out independent random units. Pseudocode of DropBlock is shown below. 

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/eab7cb36-5aad-40bb-8b61-3d06a3e9744f" alt="drop_block_algo" height="450"/>
</p>

DropBlock has two main parameters which are _block\_size_ and $Œ≥$. block\_size_ is the size of the block to be dropped, and $Œ≥$, controls how many activation units to drop. Authors experimented with a shared DropBlock mask across different feature channels or each feature channel with its DropBlock mask. The above Algorithm corresponds to the latter, which tends to work better in the experiments

Applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices

> **_NOTE:_** Some of the YOLO approaches (newer ones) use the idea of **DropBlock** for regularization purposes.


# MixUp
2018 | [paper](http://arxiv.org/pdf/1710.09412) | _MixUp: Beyond Empirical Risk Minimization_

Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. The paper introduces _**mixup**_, a simple learning principle to alleviate these issues. In essence, _mixup_ trains a neural network on convex combinations of pairs of examples and their labels. By doing so, _mixup_ regularizes the neural network to favor simple linear behavior in-between training examples

The contribution of _mixup_ paper is to propose a generic vicinal distribution:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/fcfb7d1e-8fc7-4a49-a48c-5652314015c8" alt="mixup_hard_eq" height="60"/>
</p>

where $Œª ‚àº Beta(Œ±, Œ±)$, for $Œ± ‚àà (0, ‚àû)$. In a nutshell, sampling from the _mixup_ vicinal distribution produces virtual feature-target vectors:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5992a2fd-34a6-4565-aa5c-9687e1dee702" alt="mixup" height="90"/>
</p>

where $(x_i, y_i)$ and $(x_j , y_j)$ are two feature-target vectors drawn at random from the training data, and $Œª ‚àà [0, 1]$. The mixup hyper-parameter Œ± controls the strength of interpolation between feature-target pairs, recovering the ERM (Empirical Risk Minimization) principle as $Œ± ‚Üí 0$.

**What is mixup doing?** 

The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model f to behave linearly in-between training examples. Authors of the paper argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam‚Äôs razor since it is one of the simplest possible behaviors. _mixup_ leads to decision boundaries that transition linearly from class to class, providing a smoother estimate of uncertainty. Experiments show, that the models trained with mixup are more stable in terms of model predictions and gradient norms in-between training samples.

Mixup example:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6f5f48d7-74cd-4da0-b953-191a12521b0e" alt="mixup_example" height="350"/>
</p>

> **_NOTE:_** Most of the YOLO approaches (new ones) use **MixUp** as one of data augmentation techniques for regularization purposes.



# CBAM
2018 | [paper](https://arxiv.org/pdf/1807.06521.pdf) | _CBAM: Convolutional Block Attention Module_

Convolutional Block Attention Module (**CBAM**) is a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, CBAM sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/055969ff-3ef1-43bc-9d19-1da921f87680" alt="CBAM_overview" height="300"/>
</p>

Given an intermediate feature map $F$ (shape: $C √ó H √ó W$) as input, CBAM sequentially infers a $1D$ channel attention map $M_c$ (shape: $C √ó 1 √ó 1$) and a $2D$ spatial attention map $M_s$ (shape: $1 √ó H √ó W$) as illustrated above. The overall attention process can be summarized as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/503f9b0d-ed52-4094-bc7e-6988e1765968" alt="CBAM_overview_eq" height="80"/>
</p>

where $‚äó$ denotes element-wise multiplication. During multiplication, the attention values are broadcasted (copied) accordingly: channel attention values are broadcasted along the spatial dimension, and vice versa. $F^{‚Ä≤‚Ä≤}$ is the final refined output. The figure below depicts the computation process of each attention map. The following describes the details of each attention module.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/12a9a4ed-4484-4207-99f0-c826c8b87253" alt="CBAM_detail" height="400"/>
</p>

**Channel attention module** - channel attention map is produced by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on **‚Äòwhat‚Äô** is meaningful given an input image. The spatial dimension of the input feature map is squeezed to compute the channel attention efficiently. For aggregating spatial information, _average-pooling_ has been commonly adopted so far. Authors argued that _max-pooling_ gathers another important clue about distinctive object features to infer finer channel-wise attention. Thus, they used both average-pooled and max-pooled features simultaneously and empirically confirmed that exploiting both features greatly improves representation power of networks rather than using each independently.

The first step is to aggregate spatial information of a feature map by using both _average-pooling_ and _max-pooling_ operations, generating two different spatial context descriptors: _Fc{avg}_ and _Fc{max}_, which denote average-pooled features and max-pooled features respectively. Both descriptors are then forwarded to a shared network to produce the channel attention map $M_c$ (shape $C √ó 1 √ó 1$). The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $C/r √ó 1 √ó 1$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, the output feature vectors are merged using element-wise summation. In short, the channel attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/605f2500-ef28-4e3b-a0e8-b63fc4f5fdcc" alt="CBAM_channel_att" height="80"/>
</p>

where $œÉ$ denotes the sigmoid function, $W_0$ has shape $C/r √ó C$, and $W_1$ has shape $C √ó C/r$. Note that the MLP weights, $W_0$ and $W_1$, are shared for both inputs and the ReLU activation function is followed by $W_0$.


**Spatial attention module** - the spatial attention map is generated by utilizing the inter-spatial relationship of features. Different from the channel attention, the spatial attention focuses on **‚Äòwhere‚Äô** is an informative part, which is complementary to the channel attention. To compute the spatial attention, the first step is to apply _average-pooling_ and _max-pooling_ operations along the channel axis and concatenate them to generate an efficient feature descriptor. Applying pooling operations along the channel axis is shown to be effective in highlighting informative regions. On the concatenated feature descriptor, the convolution layer is  applied to generate a spatial attention map $M_s(F)$ (shape $H √ó W$) which encodes where to emphasize or suppress.

The channel information of a feature map is aggregated by using two pooling operations, generating two 2D maps: $F^s_{avg}$ (shape $1 √ó H √ó W$) and $F^s_{max}$ (shape $1 √ó H √ó W$). Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the $2D$ spatial attention map. In short, the spatial attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20b00305-c0af-42ea-9d5b-a449f2f7d825" alt="CBAM_spatial_att" height="80"/>
</p>

where $œÉ$ denotes the sigmoid function and $f$ $7 √ó 7$ represents a convolution operation with the filter size of $7 √ó 7$.

**Arrangement of attention modules** - given an input image, two attention modules, channel and spatial, compute complementary attention, focusing on **‚Äòwhat‚Äô** and **‚Äòwhere‚Äô** respectively. Considering this, two modules can be placed in a parallel or sequential manner. Authours have found that the sequential arrangement gives a better result than a parallel arrangement. For the arrangement of the sequential process, the experimental result shows that the channel-first order is slightly better than the spatial-first

Example of CBAM integrated with a ResBlock in ResNet:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5f286642-9e32-4823-8b21-2d8debe9c269" alt="resblock_CBAM" height="270"/>
</p>

> **_NOTE:_** Some of the new YOLO approaches use either **Channel-Attention** or **Spatial-Attention** to enhance feature maps representations.


# Focal Loss
2018 | [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | _Focal Loss for Dense Object Detection_

In this paper, the authors addressed the challenge of low accuracy in one-stage object detectors compared to two-stage detectors. They identify the imbalance between foreground and background classes during training as the main issue. To mitigate this, they introduce Focal Loss, which prioritizes hard examples, leading to their proposed detector, RetinaNet, achieving both the speed of traditional one-stage detectors and surpassing the accuracy of all existing two-stage detectors

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/17c08773-46e7-46c2-8ce4-724c6498e899" alt="focal_loss_fig" height="270"/>
</p>

In R-CNN-like detectors, class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g., Selective Search, EdgeBoxes, DeepMask, RPN) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k). In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM), are performed to maintain a manageable balance between foreground and background

In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating ‚àº100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping or hard example mining.

Focal Loss proposed in this paper acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases (see above). Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that the proposed Focal Loss enables to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous SoTA techniques for training one-stage detectors. To demonstrate the effectiveness of the proposed focal loss, authors designed a simple one-stage object detector called _RetinaNet_, named for its dense sampling of object locations in an input image. Its design features an efficient in-network feature pyramid and use of anchor boxes. It draws on a variety of recent ideas from SSD, RPN and FPN.

The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000).

More formally, authors added a modulating factor $(1 ‚àí p_t)^Œ≥$ to the cross entropy loss, with tunable focusing parameter $Œ≥ ‚â• 0$ and defined the Focal Loss as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2c67cbcc-6837-4135-8e66-0bf1a28aec7a" alt="focal_loss_eq" height="60"/>
</p>

The focal loss is visualized for several values of $Œ≥ ‚àà [0, 5]$ in figure above. Authors noted two properties of the focal loss:
* When an example is misclassified and _pt_ is small, the modulating factor is near 1 and the loss is unaffected. As $p_t ‚Üí 1$, the factor goes to $0$ and the loss for well-classified examples is down-weighted
* The focusing parameter $Œ≥$ smoothly adjusts the rate at which easy examples are down-weighted. When $Œ≥ = 0$, _FL_ is equivalent to _CE_, and as $Œ≥$ is increased the effect of the modulating factor is likewise increased (found $Œ≥ = 2$ to work best in experiments). Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with $Œ≥ = 2$, an example classified with $p_t = 0.9$ would have $100√ó$ lower loss compared with _CE_ and with $pt ‚âà 0.968$ it would have $1000√ó$ lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most $4√ó$ for $p_t ‚â§ .5$ and $Œ≥ = 2$). In practice authors used an _Œ±-balanced_ variant of the focal loss:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/372b592a-7464-40ac-96d0-1004801da292" alt="focal_loss_alpha_eq" height="60"/>
</p>

and adopted this form in the experiments as it yields slightly improved accuracy over the _non-Œ±-balanced_ form. Finally, they note that the implementation of the loss layer combines the sigmoid operation for computing _p_ with the loss computation, resulting in greater numerical stability.

Another huge contribution of the paper is the RetinaNet architecture shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/0b044e91-ea60-4801-a6be-d74f8bc999b5" alt="retina_net" height="400"/>
</p>

> **_NOTE:_** Some of the YOLO approaches check if using Focal Loss helps to achieve better results, but in most cases it does not. YOLO is already handling background examples via set of multipliers for loss calculation and/or picking bboxes for loss calculation. 



# CoordConv
2018 | [paper](https://arxiv.org/pdf/1807.03247v2) | _An intriguing failing of convolutional neural networks and the CoordConv solution_

In this paper authors show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in $(x, y)$ _Cartesian space_ and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, authors show that they fail spectacularly. They fix this problem with a solution called CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/a61ccf9f-06f6-46f6-a79c-1f1ff3d976d0" alt="CoordConv" height="400"/>
</p>

The proposed CoordConv layer is a simple extension to the standard convolutional layer. Authors assume in the paper the case of two spatial dimensions, though operators in other dimensions follow trivially. Convolutional layers are used in a myriad of applications because they often work well, perhaps due to some combination of three factors:

* they have relatively few learned parameters,
* they are fast to compute on modern GPUs,
* they learn a function that is translation invariant (a translated input produces a translated output).

The CoordConv layer keeps the first two of these properties - few parameters and efficient computation ‚Äî but allows the network to learn to keep or to discard the third‚Äîtranslation invariance - as is needed for the task being learned. It may appear that doing away with translation invariance will hamper networks‚Äô abilities to learn generalizable functions. However, allocating a small amount of network capacity to model non-translation invariant aspects of a problem can enable far more trainable models that also generalize far better.

The CoordConv layer can be implemented as a simple extension of standard convolution in which extra channels are instantiated and filled with (constant, untrained) coordinate information, after which they are concatenated channel-wise to the input representation and a standard convolutional layer is applied. Figure above depicts the operation where two coordinates, $i$ and $j$, are added. Concretely, the $i$ coordinate channel is an $h √ó w$ rank-1 matrix with its first row filled with 0‚Äôs, its second row with 1‚Äôs, its third with 2‚Äôs, etc. The $j$ coordinate channel is similar, but with columns filled in with constant values instead of rows. In all experiments, we apply a final linear scaling of both $i$ and $j$ coordinate values to make them fall in the range $[‚àí1, 1]$. For convolution over two dimensions, two $(i, j)$ coordinates are sufficient to completely specify an input pixel, but if desired, further channels can be added as well to bias models toward learning particular solutions. In some of the experiments that follow, we have also used a third channel for an $r$ coordinate, where $r = \sqrt{(i ‚àí h/2)^2 + (j ‚àí w/2)^2}$

**Number of parameters** - ignoring bias parameters (which are not changed), a standard convolutional layer with square kernel size $k$ and with $c$ input channels and $c‚Ä≤$ output channels will contain $cc^{\prime}k^2$ weights, whereas the corresponding CoordConv layer will contain $(c + d)c^{\prime}k^2$ weights, where $d$ is the number of coordinate dimensions used (e.g. 2 or 3). The relative increase in parameters is small to moderate, depending on the original number of input channels. 


> A CoordConv layer implemented via the channel concatenation discussed entails an increase of $dc^{\prime}k^2$ weights. However, if $k > 1$, not all $k^2$ connections from coordinates to each output unit are necessary, as spatially neighboring coordinates do not provide new information. Thus, if one cares acutely about minimizing the number of parameters and operations, a $k √ó k$ _conv_ may be applied to the input data and a $1 √ó 1$ _conv_ to the coordinates, then the results added. In this paper authors have used the simpler, if marginally inefficient, channel concatenation version that applies a single convolution to both input data and coordinates.


**Translation invariance** - CoordConv with weights connected to input coordinates set by initialization or learning to zero will be translation invariant and thus mathematically equivalent to ordinary convolution. If weights are nonzero, the function will contain some degree of translation dependence, the precise form of which will ideally depend on the task being solved. Similar to locally connected layers with unshared weights, CoordConv allows learned translation dependence, but by contrast it requires far fewer parameters: $(c + d)c^{\prime}k^2$ vs. $hwcc'k^2$ for spatial input size $h √ó w$. Note that all CoordConv weights, even those to coordinates, are shared across all positions, so translation dependence comes only from the specification of coordinates; one consequence is that, as with ordinary convolution but unlike locally connected layers, the operation can be expanded outside the original spatial domain if the appropriate coordinates are extrapolated.






# Linear LR Scaling
2018 | [paper](https://arxiv.org/pdf/1706.02677) | _Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour_

In this paper, authors empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, they show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, they adopt a hyperparameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. The below figure shows an example of results for various mini-batch sizes on the ImageNet dataset.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/97f1964a-2ad7-4b99-9d27-0be8fc7cb4eb" alt="Linear_Scaling_Error" height="400"/>
</p>

The goal of this report is to demonstrate the feasibility of, and to communicate a practical guide to, large-scale training with distributed synchronous stochastic gradient descent (SGD). As an example, authors scale ResNet-50 training, originally performed with a minibatch size of 256 images (using 8 Tesla P100 GPUs, training time is 29 hours), to larger minibatches (see above figure). In particular, they show that with a large minibatch size of 8192, it is possible to train ResNet-50 in 1 hour using 256 GPUs while maintaining the same level of accuracy as the 256 minibatch baseline. While distributed synchronous SGD is now commonplace, no existing results show that generalization accuracy can be maintained with minibatches as large as 8192 or that such high-accuracy models can be trained in such short time.

To tackle this unusually large minibatch size, authors employ a simple and hyper-parameter-free linear scaling rule to adjust the learning rate. To successfully apply this rule, authors present a new warmup strategy, i.e., a strategy of using lower learning rates at the start of training, to overcome early optimization difficulties. Importantly, not only does the approach match the baseline validation error, but also yields training error curves that closely match the small minibatch baseline.


## Large Minibatch SGD

Authors start by reviewing the formulation of Stochastic Gradient Descent (SGD). Consider supervised learning by minimizing a loss $L(w)$ of the form:

$$ L(w) = \frac{1}{|X|}\sum_{x ‚àà X}l(x, w) $$

Here $w$ are the weights of a network, $X$ is a labeled training set, and $l(x, w)$ is the loss computed from samples $x ‚àà X$ and their labels $y$. Typically $l$ is the sum of a classification loss (e.g., cross-entropy) and a regularization loss on $w$. Minibatch Stochastic Gradient Descent, usually referred to as simply as SGD in recent literature even though it operates on minibatches, performs the following update:

$$ w_{t+1} = w_t - Œ∑\frac{1}{n} \sum_{x ‚àà B}\nabla l(x, w_t) $$

Here $B$ is a minibatch sampled from $X$ and $n = |B|$ is the minibatch size, $Œ∑$ is the learning rate, and $t$ is the iteration index. Note that in practice we use momentum SGD.


### Learning Rates for Large Minibatches

As is shown in comprehensive experiments, authors found that the following learning rate scaling rule is surprisingly effective for a broad range of minibatch sizes:

> _**Linear Scaling Rule**: When the minibatch size is multiplied by k, multiply the learning rate by k._

All other hyper-parameters (weight decay, etc.) are kept unchanged. The linear scaling rule can help to not only match the accuracy between using small and large minibatches, but equally importantly, to largely match their training curves, which enables rapid debugging and comparison of experiments prior to convergence.

**Interpretation** - Consider a network at iteration $t$ with weights $w_t$, and a sequence of $k$ minibatches $B_j$ for $0 ‚â§ j < k$ each of size $n$. We compare the effect of executing $k$ SGD iterations with small minibatches $B_j$ and learning rate $Œ∑$ versus a single iteration with a large minibatch $‚à™_j B_j$ of size $kn$ and learning rate $\hat{Œ∑}$. According to previous equation, after $k$ iterations of SGD with learning rate $Œ∑$ and a minibatch size of $n$ we have:

$$ w_{t+k} = w_t - Œ∑ \frac{1}{n} \sum_{j < k} \sum_{x ‚àà B_j} ‚àál(x, w_{t+j}) $$


On the other hand, taking a single step with the large minibatch $‚à™_j B_j$ of size $kn$ and learning rate $\hat{Œ∑}$ yields:


$$ \hat{w_{t+1}} = w_t - \hat{Œ∑} \frac{1}{kn} \sum_{j < k} \sum_{x ‚àà B_j} ‚àál(x, w_{t}) $$

As expected, the updates differ, and it is unlikely that $\hat{w_{t+1}} = w_{t+k}$. However, if we could assume $‚àál(x, w_t) ‚âà ‚àál(x, w_{t+j})$ for $j < k$, then setting $\hat{Œ∑} = kŒ∑$ would yield $\hat{w_{t+1}} ‚âà w_{t+k}$, and the updates from small and large minibatch SGD would be similar. Although this is a strong assumption, authors emphasize that if it were true the two updates are similar only if we set $\hat{Œ∑} = kŒ∑$.

The above interpretation gives intuition for one case where we may hope the linear scaling rule to apply. In the experiments with $\hat{Œ∑} = kŒ∑$ (and warmup), small and large minibatch SGD not only result in models with the same final accuracy, but also, the training curves match closely. The empirical results suggest that the above approximation might be valid in large-scale, real-world data.

However, there are at least two cases when the condition $‚àál(x, w_t) ‚âà ‚àál(x, w_{t+j})$ will clearly not hold. First, in initial training when the network is changing rapidly, it does not hold. Authors address this by using a warmup phase. Second, minibatch size cannot be scaled indefinitely: while results are stable for a large range of sizes, beyond a certain point accuracy degrades rapidly. This point is as large as ‚àº8k in ImageNet experiments

### Warmup

**Gradual warmup** - Authors present an alternative warmup that gradually ramps up the learning rate from a small to a large value. This ramp avoids a sudden increase of the learning rate, allowing healthy convergence at the start of training. In practice, with a large minibatch of size $kn$, they start from a learning rate of $Œ∑$ and increment it by a constant amount at each iteration such that it reaches $\hat{Œ∑} = kŒ∑$ after 5 epochs (results are robust to the exact duration of warmup). After the warmup, the original learning rate schedule is used.



# GIoU
2018 | [paper](https://arxiv.org/pdf/1902.09630) | _Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression_

Intersection over Union ($IoU$) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned $2D$ bounding boxes, it can be shown that $IoU$ can be directly used as a regression loss. However, $IoU$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of $IoU$ by introducing a generalized version as both a new loss and a new metric.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2fa93d74-942c-4021-abea-a9b506c14c88" alt="giou_comparison" height="400"/>
</p>

However, it can be shown that there is not a strong correlation between minimizing the commonly used losses, e.g. $l_n$-norms, defined on parametric representation of two bounding boxes in $2D$ / $3D$ and improving their $IoU$ values. For example, consider the simple $2D$ scenario in figure above, where the predicted bounding box (black rectangle), and the ground truth box (green rectangle), are represented by their top-left and bottom-right corners, i.e. $(x_1, y_1, x_2, y_2)$. For simplicity, let‚Äôs assume that the distance, e.g. $l_2$-norm, between one of the corners of two boxes is fixed. Therefore any predicted bounding box where the second corner lies on a circle with a fixed radius centered on the second corner of the green rectangle (shown by a gray dashed line circle) will have exactly the same $l_2$-norm distance from the ground truth box; however their $IoU$ values can be significantly different. The same argument can be extended to any other representation and loss, e.g. figure above (b). It is intuitive that a good local optimum for these types of objectives may not necessarily be a local optimum for $IoU$. Moreover, in contrast to $IoU$ , $l_n$-norm objectives defined based on the aforementioned parametric representations are not invariant to the scale of the problem. To this end, several pairs of bounding boxes with the same level of overlap, but different scales due to e.g. perspective, will have different objective values. In addition, some representations may suffer from lack of regularization between the different types of parameters used for the representation. For example, in the center and size representation, $(x_c, y_c)$ is defined on the location space while $(w, h)$ belongs to the size space. Complexity increases as more parameters are incorporated, e.g. rotation, or when adding more dimensions to the problem. To alleviate some of the aforementioned problems, SoTA object detectors introduce the concept of an anchor box as a hypothetically good initial guess. They also define a non-linear representation to naively compensate for the scale changes. Even with these handcrafted changes, there is still a gap between optimizing the regression losses and $IoU$ values.

In this paper, authors explore the calculation of $IoU$ between two axis aligned rectangles, or generally two axis aligned n-orthotopes, which has a straightforward analytical solution and in contrast to the prevailing belief, $IoU$ in this case can be backpropagated, i.e. it can be directly used as the objective function to optimize. It is therefore preferable to use $IoU$ as the objective function for $2D$ object detection tasks. Given the choice between optimizing a metric itself vs. a surrogate loss function, the optimal choice is the metric itself. However, $IoU$ as both a metric and a loss has a major issue: **if two objects do not overlap, the $IoU$ value will be zero and will not reflect how far the two shapes are from each other**. In this case of non-overlapping objects, if $IoU$ is used as a loss, its gradient will be zero and cannot be optimized.


## Generalized Intersection over Union

Intersection over Union ($IoU$) for comparing similarity between two arbitrary shapes (volumes) $A, B ‚äÜ S ‚àà R^n$ is attained by:

$$ IoU = \frac{|A ‚à© B|}{|A ‚à™ B|} $$

Two appealing features, which make this similarity measure popular for evaluating many $2D$ / $3D$ computer vision tasks are as follows:

* $IoU$ as a distance, e.g. $L_{IoU} = 1‚àíIoU$ , is a metric (by mathematical definition). It means $L_{IoU}$ fulfills all
properties of a metric such as non-negativity, identity of indiscernibles, symmetry and triangle inequality

* $IoU$ is invariant to the scale of the problem. This means that the similarity between two arbitrary shapes $A$ and $B$ is independent from the scale of their space $S$

However, IoU has a major weakness:

* If $|A ‚à© B| = 0$, $IoU(A, B) = 0$. In this case, $IoU$ does not reflect if two shapes are in vicinity of each other or
very far from each other

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/29cdd1c3-84d3-4458-8899-ed0e6046f351" alt="giou_algo_1" height="250"/>
</p>

To address this issue, authors propose a general extension to $IoU$ , namely Generalized Intersection over Union $GIoU$. For two arbitrary convex shapes (volumes) $A, B ‚äÜ S ‚àà R^n$, they first find the smallest convex shapes $C ‚äÜ S ‚àà R^n$ enclosing both $A$ and $B$. For comparing two specific types of geometric shapes, $C$ can be from the same type. For example, two arbitrary ellipsoids, $C$ could be the smallest ellipsoids enclosing them. Then we calculate a ratio between the volume (area) occupied by $C$ excluding $A$ and $B$ and divide by the total volume (area) occupied by $C$. This represents a normalized measure that focuses on the empty volume (area) between $A$ and $B$. Finally $GIoU$ is attained by subtracting this ratio from the $IoU$ value. The calculation of $GIoU$ is summarized in Algorithm above. $GIoU$ as a new metric has the following properties: 

* Similar to $IoU$ , $GIoU$ as a distance, e.g. $L_{GIoU} = 1 ‚àí GIoU$ , holding all properties of a metric such as non-negativity, identity of indiscernibles, symmetry and triangle inequality.

* Similar to $IoU$ , $GIoU$ is invariant to the scale of the problem.

* $GIoU$ is always a lower bound for $IoU$ , i.e. $‚àÄA, B ‚äÜ S, GIoU (A, B) ‚â§ IoU (A, B)$, and this lower bound becomes tighter when $A$ and $B$ have a stronger shape similarity and proximity, i.e. $\lim_{A \to B} GIoU(A, B) = IoU(A, B)$.

* $‚àÄA, B ‚äÜ S, 0 ‚â§ IoU(A, B) ‚â§ 1$, but $GIoU$ has a symmetric range, i.e. $‚àÄA, B ‚äÜ S$, $‚àí1 ‚â§ GIoU(A, B) ‚â§ 1$.
	* similar to $IoU$ , the value 1 occurs only when two objects overlay perfectly, i.e. if $|A ‚à™ B| = |A ‚à© B|$, then $GIoU = IoU = 1$
	* $GIoU$ value asymptotically converges to -1 when the ratio between occupying regions of two shapes, $|A ‚à™ B|$, and the volume (area) of the enclosing shape $|C|$ tends to zero, i.e. $\lim_{\frac{|A ‚à™ B|}{|C|} \to 0} GIoU (A, B) = ‚àí1$


In summary, this generalization keeps the major properties of $IoU$ while rectifying its weakness. Therefore, $GIoU$ can be a proper substitute for $IoU$ in all performance measures used in $2D$/$3D$ computer vision tasks

### GIoU as Loss for Bounding Box Regression

So far, we introduced $GIoU$ as a metric for any two arbitrary shapes. However as is the case with $IoU$ , there is no analytical solution for calculating intersection between two arbitrary shapes and/or for finding the smallest enclosing convex object for them. Fortunately, for the $2D$ object detection task where the task is to compare two axis aligned bounding boxes, we can show that $GIoU$ has a straightforward solution. In this case, the intersection and the smallest enclosing objects both have rectangular shapes. It can be shown that the coordinates of their vertices are simply the coordinates of one of the two bounding boxes being compared, which can be attained by comparing each vertices‚Äô coordinates using min and max functions. To check if two bounding boxes overlap, a condition must also be checked. Therefore, we have an exact solution to calculate $IoU$ and $GIoU$. Algorithm below shows how to calculate the $IoU$ and $GIoU$ as loss functions. 

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d85ebff7-b728-4ddd-8fe7-a3561136eeb5" alt="giou_algo_2" height="400"/>
</p>



# DIoU
2018 | [paper](https://arxiv.org/pdf/1911.08287.pdf) | _Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression_

Intersection over Union (IoU) is the most popular metric for object detection:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/cdb085ad-22ab-4f06-8bea-433f4ae96177" alt="iou" height="60"/>
</p>

where $B_{gt} = (x^{gt}, y^{gt}, w^{gt}, h^{gt})$ is the ground-truth, and
$B = (x, y, w, h)$ is the predicted box. Conventionally, $l_n$-norm (e.g., $n = 1$ or $n = 2$) loss is adopted on the coordinates of $B$ and $B_{gt}$ to measure the distance between bounding boxes. However, $l_n$-norm loss is not a suitable choice to obtain the optimal IoU metric. In earlier works, the IoU loss was suggested to be adopted for improving the IoU metric:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/eab21c19-7365-4626-adc2-755f5cf0087c" alt="iou_loss" height="60"/>
</p>

However, IoU loss only works when the bounding boxes have overlap, and would not provide any moving gradient for non-overlapping cases. And then generalized IoU loss
(GIoU) is proposed by adding a penalty term:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8435920f-e666-41c8-8eac-5c933a11178b" alt="giou_loss" height="60"/>
</p>

where $C$ is the smallest box covering $B$ and $B_{gt}$. Due to the introduction of penalty term, the predicted box will move towards the target box in non-overlapping cases. Although GIoU can relieve the gradient vanishing problem for non-overlapping cases, it still has several limitations. As shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/51db5636-3187-4b90-8e68-2dfb450a23e0" alt="giou_vs_diou" height="300"/>
</p>


 one can see that GIoU loss intends to increase the size of predicted box at first, making it have overlap with target box, and then the IoU term in GIoU equation will work to maximize the overlap area of bounding box. And from figure below:
 
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/9298cee5-1c55-4549-956d-8b9899a73803" alt="iou_vs_giou_vs_diou" height="400"/>
</p>
 
GIoU loss will totally degrade to IoU loss for enclosing bounding boxes. Due to heavily relying on the IoU term, GIoU empirically needs more iterations to converge, especially for horizontal and vertical bounding boxes. Usually GIoU loss cannot well converge in the SoTA detection algorithms, yielding inaccurate detection. 


To sum up, IoU loss converges to bad solutions for nonoverlapping cases, while GIoU loss is with slow convergence especially for the boxes at horizontal and vertical orientations. And when incorporating into object detection pipeline, both IoU and GIoU losses cannot guarantee the accuracy of regression


**Distance-IoU (DIoU)**
In this paper, authors proposed a Distance-IoU (DIoU) loss for bounding box regression. In particular, they simply added a penalty term on IoU loss to directly minimize the normalized distance between central points of two bounding boxes, leading to much faster convergence than GIoU loss. From figure above it can be seen, that DIoU loss can be deployed to directly minimize the distance between two bounding boxes:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/1c60033c-eac0-481e-afed-f47936cc5286" alt="diou" height="60"/>
</p>

where $b$ and $b_{gt}$ denote the central points of $B$ and $B_{gt}$, $œÅ(¬∑)$ is the Euclidean distance, and c is the diagonal length of the smallest enclosing box covering the two boxes. 

**Comparison with IoU and GIoU losses** -  The proposed DIoU loss inherits some properties from IoU and GIoU loss:
1. DIoU loss is still invariant to the scale of regression problem.
2. Similar to GIoU loss, DIoU loss can provide moving directions for bounding boxes when non-overlapping with target box.
3. When two bounding boxes perfectly match, $L_{IoU} = L_{GIoU} = L_{DIoU} = 0$. When two boxes are far away, $L_{GIoU} = L_{DIoU} ‚Üí 2$.

And DIoU loss has several merits over IoU loss and GIoU loss, which can be evaluated by simulation experiment.

**Complete IoU Loss (CIoU)**
A good loss for bounding box regression should consider three important geometric factors, i.e., overlap area, central point distance and aspect ratio. By uniting the coordinates, IoU loss considers the overlap area, and GIoU loss heavily relies on IoU loss. The proposed DIoU loss aims at considering simultaneously the overlap area and central point distance of bounding boxes. However, the consistency of aspect ratios for bounding boxes is also an important geometric factor. Therefore, based on DIoU loss, the CIoU loss is proposed by imposing the consistency of aspect ratio:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/1413b442-6736-4a7f-b8eb-df968287a0fd" alt="ciou_loss" height="60"/>
</p>

where $Œ±$ is a positive trade-off parameter, and $v$ measures the consistency of aspect ratio:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/f38a6af0-6008-40cd-96da-eea27fe2e568" alt="ciou_alpha" height="60"/>
</p>
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2e8976f5-3792-4cd1-a73b-db5b96fdcc78" alt="ciou_v" height="60"/>
</p>

> **_NOTE:_** Most of the new YOLO approaches use **DIoU** or **CIoU** as loss functions for bounding box regression.



# PAN
2018 | [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | _Path Aggregation Network for Instance Segmentation_

The way that information propagates in neural networks is of great importance. In this paper, authors proposed Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, they enhanced the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. The adaptive feature pooling was presented, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.

During the research authors notes that information propagation in SoTA Mask R-CNN can be further improved. Specifically, features in low levels are helpful for large instance identification. But there is a long path from low-level structure to topmost features, increasing difficulty to access accurate localization information. Further, each proposal is predicted based on feature grids pooled from one feature level, which is assigned heuristically. This process can be updated since information discarded in other levels may be helpful for final prediction. Finally, mask prediction is made on a single view, losing the chance to gather more diverse information.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/affbff14-7d90-4515-812d-1c6965e9fd82" alt="PANet" height="350"/>
</p>

Inspired by these principles and observations, wthey proposed PANet, illustrated above, for instance segmentation with followint properties:
* To shorten information path and enhance feature pyramid with accurate localization signals existing in low-levels, bottom-up path augmentation is created. In fact, features in low-layers were utilized in the earlier works, but propagating low-level features to enhance entire feature hierarchy for instance recognition was not explored
* Second, to recover broken information path between each proposal and all feature levels, they develop adaptive feature pooling. It is a simple component to aggregate features from all feature levels for each proposal, avoiding arbitrarily assigned results. With this operation, cleaner paths are created.
* Finally, to capture different views of each proposal, they augmented mask prediction with tiny fully-connected (fc) layers, which possess complementary properties to FCN originally used by Mask R-CNN. By fusing predictions from these two views, information diversity increases and masks with better quality are produced. 

The first two components are shared by both object detection and instance segmentation, leading to much enhanced performance of both tasks

## Framework

The framework is illustrated in figure above. Path augmentation and aggregation is conducted for improving performance. A bottom-up path is augmented to make low-layer information easier to propagate. adaptive feature pooling is designed to allow each proposal to access information from all levels for prediction. A complementary path is added to the mask-prediction branch. This new structure leads to decent performance. Similar to FPN, the improvement is independent of the CNN structure.

### Bottom-up Path Augmentation

**Motivation** - The insightful point from earlier works is that neurons in high layers strongly respond to entire objects while other neurons are more likely to be activated by local texture and patterns manifests the necessity of augmenting a top-down path to propagate semantically strong features and enhance all features with reasonable classification capability in FPN. PANet framework further enhances the localization capability of the entire feature hierarchy by propagating strong responses of low-level patterns based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances. PANet path is built with clean lateral connections from the low level to top ones. Therefore, there is a ‚Äúshortcut‚Äù (dashed green line in figure above), which consists of less than 10 layers, across these levels. In comparison, the CNN trunk in FPN gives a long path (dashed red line) passing through even 100+ layers from low layers to the topmost one. 

**Augmented Bottom-up Structure** - PANet framework first accomplishes bottom-up path augmentation. Authors followed FPN to define that layers producing feature maps with the same spatial size are in the same network stage. Each feature level corresponds to one stage. ResNet is used as the basic structure and ${P_2, P_3, P_4, P_5}$ denote feature levels generated by FPN. The augmented path starts from the lowest level $P_2$ and gradually approaches $P_5$ as shown in figure above. From $P_2$ to $P_5$, the spatial size is gradually down-sampled with factor 2. Authors use ${N_2, N_3, N_4, N_5}$ to denote newly generated feature maps corresponding to ${P_2, P_3, P_4, P_5}$. Note that $N_2$ is simply $P_2$, without any processing. The building block of PFANet is shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ec075dfe-c9db-4999-8d1a-80c6e1287281" alt="PANet_block" height="350"/>
</p> 

Each building block takes a higher resolution feature map $N_i$ and a coarser map $P_{i+1}$ through lateral connection and generates the new feature map $N_{i+1}$. Each feature map $N_i$ first goes through a $3 √ó 3$ convolutional layer with stride 2 to reduce the spatial size. Then each element of feature map $P_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 √ó 3$ convolutional layer to generate $N_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P_5$. In these building blocks, we consistently use channel 256 of feature maps. All convolutional layers are followed by a ReLU. The feature grid for each proposal is then pooled from new feature maps, i.e., ${N_2, N_3, N_4, N_5}$.

Comparison of FPN and PAN:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2b674b4f-a54d-44b1-8cf9-ef1b6ec61da3" alt="PAN_vs_FPN" height="350"/>
</p> 

### Adaptive Feature Pooling

Adaptive Feature Pooling is a component of the PANet framework designed for instance segmentation. It involves pooling features from all levels of the feature hierarchy for each proposal and fusing them for prediction. It was added to PANet to address the limitations of traditional methods where proposals were assigned to different feature levels based on their size, which could lead to suboptimal results. By pooling features from all levels for each proposal, Adaptive Feature Pooling allows the network to access context information from multiple levels, leading to more accurate predictions. This approach ensures that both small proposals can access high-level features with rich context information, and large proposals can access low-level features with fine details and high localization accuracy. Ultimately, Adaptive Feature Pooling enhances the network's ability to make accurate predictions for instance segmentation tasks

### Fully Connected Fusion

Fully-connected Fusion is a technique used in the PANet framework to improve mask prediction in instance segmentation. It involves adding a fully-connected branch to the mask prediction branch, which fuses predictions from both fully-connected layers and convolutional layers. It was added because fully-connected layers have different properties compared to convolutional layers. Fully-connected layers are location-sensitive and can adapt to different spatial locations, making them effective in predicting masks for instances. By fusing predictions from both fully-connected and convolutional layers, PANet aims to leverage the strengths of each type of layer to enhance mask prediction accuracy and differentiate between instances more effectively. The figure below shows how fully connected fusion works.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/9fa8c975-9f08-46c6-92c9-1c9aaa60d1d9" alt="PANet_fc_fusion" height="350"/>
</p>

> **_NOTE:_** Most of the new YOLO approaches make use of the **Bottom-up Path Augmentation** idea from PANet. It is improved to BiFPN in future work.



# CoordConv
2018 | [paper](https://arxiv.org/pdf/1807.03247v2) | _An intriguing failing of convolutional neural networks and the CoordConv solution_
TODO


# Mish
2019 | [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | _Mish: A Self Regularized Non-Monotonic Neural Activation Function_

The main contribution of this paper is the introduction of **Mish**, a novel neural activation function that outperforms both ReLU and Swish in terms of accuracy across various deep learning benchmarks. Mish is defined as a smooth and non-monotonic activation function, which combines the properties of self-gating and regularization. It is bounded below and unbounded above, making it suitable for avoiding saturation and providing strong regularization effects. Mish's smoothness and non-monotonic nature contribute to effective optimization and generalization in neural networks. Additionally, Mish's consistent improvement in accuracy over Swish and ReLU, as demonstrated in experiments on challenging datasets like CIFAR-10 and CIFAR-100, showcases its robustness and efficiency in enhancing deep learning models

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/bd26222b-6d56-4c2d-99fb-4eaabd271b0a" alt="mish" height="350"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/e21b25d3-58ed-42f6-b632-ee12a0678a6a" alt="mish_derivs" height="350"/>
</p>

The concept of non-linearity in a Neural Network is introduced by an activation function which serves an integral role in the training and performance evaluation of the network. Over the years of theoretical research, many activation functions have been proposed, however, only a few are widely used in mostly all applications which include ReLU (Rectified Linear Unit), TanH (Tan Hyperbolic), Sigmoid, Leaky ReLU and Swish. In this work, a novel, **smooth** and **non-monotonic** neural activation function, **Mish** is proposed, which can be defined as:

$$ ùëì(ùë•) = ùë• ‚ãÖ ùë°ùëéùëõ‚Ñé(ùë†ùëúùëìùë°ùëùùëôùë¢ùë†(ùë•))$$

Like both Swish and ReLU, Mish is bounded below and unbounded above with a range $[‚âà-0.31, ‚àû)$. Mish takes inspiration from Swish by using a property called Self Gating, where the scalar input is provided to the gate. The property of Self-gating is advantageous for replacing activation functions like ReLU (point-wise functions) which take in a single scalar input without requiring to change the network parameters

Although it‚Äôs difficult to explain the reason why one activation function performs better than another due to many other training factors, the properties of Mish like being **unbounded above**, **bounded below**, **smooth** and **non-monotonic**, all play a significant role in the improvement of results. Figure below shows the different commonly used activation functions along with the graph of Mish activation for comparison.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6f54bc11-4e3e-466a-aba3-6bf623cc2634" alt="mish_vs_all" height="350"/>
</p>

* **unbounded above** - a desirable property for any activation function since it avoids saturation which generally causes training to drastically slow down due to near-zero gradients
* **bounded below** - also advantageous since it results in strong regularization effects
* **non-monotonic** - causes small negative inputs to be preserved as negative outputs, which improves expressivity and gradient flow
* **order of continuity being infinite** - also a benefit over ReLU since ReLU has an order of continuity as 0 which means it‚Äôs not continuously differentiable causing some undesired problems in gradient-based optimization
* **smooth function** - it helps with effective optimization and generalization. 

The output landscape of 5 layer randomly initialized neural network was compared for ReLU, Swish, and Mish. The observation as shown in figure below, clearly depicts the sharp transition between the scalar magnitudes for the coordinates of ReLU as compared to Swish and Mish. Smoother transition results in smoother loss functions which are easier to optimize and hence the network generalizes better which partially explains why Mish outperforms ReLU. However, in this regards, Mish and Swish are extremely similar in their corresponding output landscapes.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20fbab38-7892-4418-8501-f7dd82137a95" alt="mish_landscape" height="350"/>
</p>

> **_NOTE:_** Most of the new YOLO approaches use either **Mish** or **Swish** as actiaction functions.



# Bag Of Freebies
2019 | [paper](https://arxiv.org/pdf/1902.04103) | _Bag of Freebies for Training Object Detection Neural Networks_

Authors explore training tweaks that apply to various models including Faster R-CNN and YOLOv3. These tweaks do not change the model architectures, therefore, the inference costs remain the same.

## Bag of Freebies

* **Visually Coherent Image Mixup for Object Detection** - the distribution of blending ratio in original _mixup_ algorithm is drawn from a beta distribution $B(0.2, 0.2)$. The majority of mixups are barely noises with such beta distributions. Authors choose a beta distribution with $Œ±$ and $Œ≤$ that are both at least 1, which is more visually coherent, instead of following the same practice in image classification. In particular, they use geometry preserved alignment for image mixup to avoid distort images at the initial steps. Beta distribution with $Œ±$ and $Œ≤$ both equal to 1.5 is marginally better than 1.0 (equivalent to uniform distribution) and better than fixed even mixup.

* **Classification Head Label Smoothing** - modify the classifica tion loss by comparing the output distribution $p$ against the
ground truth distribution $q$ with cross-entropy $L = ‚àí \sum_{i} q_i log(p_i) $. $q$ is often a one-hot distribution, where the correct class has probability one while all other classes have zero. Soft-max function, however, can only approach this distribution when $z_i >> z_j , ‚àÄj \neq i$ but never reach it. This encourages the model to be too confident in its predictions and is prone to over-fitting. Label smoothing was proposed as a form of regularization. The ground truth distribution is smoothed with

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/709e07b6-2904-4566-8b4a-44414da4404c" alt="bof_label_smoothing" height="100"/>
</p>

where $K$ is the total number of classes and $Œµ$ is a small constant. This technique reduces the model‚Äôs confidence, measured by the difference between the largest and smallest logits. 

* **Data Preprocessing* - authors check various data processing techniques, including:
	* Random geometry transformation - random cropping (with constraints), random expansion, random horizontal flip and random resize (with random interpolation).
	* Random color jittering - including brightness, hue, saturation, and contrast.


* **Training Schedule Revamping** - training with cosine schedule and proper warmup
* **Synchronized Batch Normalization**
* **Random shapes training** - a mini-batch of $N$ training images is resized to $N √ó 3 √ó H √ó W$ , where $H$ and $W$ are multipliers of network stride. For example, $H = W ‚àà {320, 352, 384, 416, 448, 480, 512, 544, 576, 608}$ for YOLOv3 training given the stride of feature map is 32.



# FCOS
2019 | [paper](https://arxiv.org/pdf/1904.01355) | _FCOS: Fully Convolutional One-Stage Object Detection_

The proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, authors also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. 

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/dabdf5f9-672a-48da-a660-6d8f66ffc401" alt="FCOS" height="350"/>
</p>

FCOS contributions:
* **Fully Convolutional One-Stage Object Detector** - reformulates object detection in a per-pixel prediction fashion
* **Multi-level Prediction with FPN for FCOS** - make use of multi-level prediction to improve the recall and resolve the ambiguity resulted from overlapped bounding boxes
* **Center-ness branch** - additional head branch, which helps suppress the low-quality detected bounding boxes

## Fully Convolutional One-Stage Object Detector

Let $F_i ‚àà R^{H √ó W √ó C}$ be the feature maps at layer $i$ of a backbone CNN and $s$ be the total stride until the layer. The ground-truth bounding boxes for an input image are defined as ${B_i}$, where $B_i = (x_0^{(i)}, y_0^{(i)}, x_1^{(i)}, y_1^{(i)}, c^{(i)}) ‚àà R^{4} \times \{1, 2, ..., C\}$. Here $(x_0^{(i)}, y_0^{(i)})$  and $(x_1^{(i)}, y_1^{(i)})$ denote the coordinates of the left-top and right-bottom corners of the bounding box. $c^{(i)}$ is the class that the object in the bounding box belongs to. $C$ is the number of classes, which is 80 for MS-COCO dataset. 

For each location $(x, y)$ on the feature map $F_i$, we can map it back onto the input image as $(\lfloor \frac{s}{2} \rfloor + xs, \lfloor \frac{s}{2} \rfloor + ys)$, which is near the center of the receptive field of the location $(x, y)$. Different from anchor-based detectors, which consider the location on the input image as the center of (multiple) anchor boxes and regress the target bounding box with these anchor boxes as references, FCOS directly regress the target bounding box at the location. In other words, our detector directly views locations as training samples instead of anchor boxes in anchor-based detectors, which is the same as FCNs for semantic segmentation.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6d572c35-15c8-4f49-abf8-47bafca2a057" alt="FCOS_distances" height="350"/>
</p>


Specifically, location $(x, y)$ is considered as a positive sample if it falls into any ground-truth box and the class label $c^\*$ of the location is the class label of the ground-truth box. Otherwise it is a negative sample and $c^* = 0$ (background class). Besides the label for classification, we also have a $4D$ real vector $t^\* = (l^\*, t^\*, r^\*, b^\*)$ being the regression targets for the location. Here $l^\*$, $t^\*$, $r^\*$ and $b^\*$ are the distances from the location to the four sides of the bounding box as shown in figure above. If a location falls into multiple bounding boxes, it is considered as an ambiguous sample. Authors simply choose the bounding box with minimal area as its regression target. With multi-level prediction, the number of ambiguous samples can be reduced significantly and thus they hardly affect the detection performance. Formally, if location $(x, y)$ is associated to a bounding box $B_i$, the training regression targets for the location can be formulated as:

$$ l^\* = x - x_0^{(i)} $$

$$ t^\* = y - y_0^{(i)} $$

$$ r^\* = x_1^{(i)} - x $$

$$ b^\* = y_1^{(i)} - y $$

It is worth noting that FCOS can leverage as many foreground samples as possible to train the regressor. It is different from anchor-based detectors, which only consider the anchor boxes with a highly enough IoU with ground-truth boxes as positive samples.

**Network Outputs** - corresponding to the training targets, the final layer of our networks predicts an $80D$ vector **$p$** of classification labels and a $4D$ vector $t = (l, t, r, b)$ bounding box coordinates. Instead of training a multi-class classifier, FCOS trains $C$ binary classifiers. FCOS adds four convolutional layers after the feature maps of the backbone networks respectively for classification and regression branches. Moreover, since the regression targets are always positive, FCOS employs $exp(x)$ to map any real number to $(0, ‚àû)$ on the top of the regression branch. _It is worth noting that FCOS has 9√ó fewer network output variables than the popular anchor-based detectors with 9 anchor boxes per location_.


**Loss Function**  The training loss function is defined as:

$$ L({p_{x, y}}, {t_{x, y}}) = \frac{1}{N_{pos}} \sum_{x, y} L_{cls}(p_{x, y}, c_{x, y}^\*) + \frac{\lambda}{N_{pos}} \sum_{x, y} 1_{c_{x, y}^\* > 0} L_{reg}(t_{x, y}, t_{x, y}^\*) $$

where $L_{cls}$ is focal loss and $L_{reg}$ is the IOU loss. $N_{pos}$ denotes the number of positive samples and $Œª$ being 1 in this paper is the balance weight for $L_{reg}$. The summation is calculated over all locations on the feature maps $F_i$. $1_{c^\*_i > 0}$ is the indicator function, being $1$ if $c^\*_i > 0$ and $0$ otherwise.

**Inference** the inference of FCOS is straightforward. Given an input images, we forward it through the network and obtain the classification scores $p_{x,y}$ and the regression prediction $t_{x,y}$ for each location on the feature maps $F_i$. Authors choose the location with $p_{x,y} > 0.05$ as positive samples and invert above equation to obtain the predicted bounding boxes.


## Multi-level Prediction with FPN for FCOS

Two possible issues of the proposed FCOS can be resolved with multi-level prediction with FPN:

* The large stride (e.g., $16√ó$) of the final feature maps in a CNN can result in a relatively low best possible recall (BPR). For anchor based detectors, low re
call rates due to the large stride can be compensated to some extent by lowering the required IoU scores for positive anchor boxes. For FCOS, at the first glance one may think that the BPR can be much lower than anchor-based detectors because it is impossible to recall an object which no location on the final feature maps encodes due to a large stride. Even with a large stride, FCN-based FCOS is still able to produce a good BPR. Therefore, the BPR is actually not
a problem of FCOS. Moreover, with multi-level FPN prediction, the BPR can be further improved.

* Overlaps in ground-truth boxes can cause intractable ambiguity, i.e., which bounding box should a location in the overlap regress? This ambiguity results in degraded performance of FCN-based detectors. The ambiguity can be greatly resolved with multi-level prediction.

Following FPN, FCOS detects different sizes of objects on different levels of feature maps. Specifically, authors make use of five levels of feature maps defined as $\{P_3, P_4, P_5, P_6, P_7\}$. $P_3, P_4$ and $P_5$ are produced by the backbone CNNs‚Äô feature maps $C_3, C_4$ and $C_5$ followed by a $1 √ó 1$ convolutional layer with the top-down connections, as shown in figure above. $P_6$ and $P_7$ are produced by applying one convolutional layer with the stride being 2 on $P_5$ and $P_6$, respectively. As a result, the feature levels $P_3, P_4, P_5, P_6$ and $P_7$ have strides $8, 16, 32, 64$ and $128$, respectively.

Unlike anchor-based detectors, which assign anchor boxes with different sizes to different feature levels, FCOS directly limits the range of bounding box regression for each level. More specifically, authors firstly compute the regression targets $l^\*, t^\*, r^\*, b^\*$ for each location on all feature levels. Next, if a location satisfies $max(l^\*, t^\*, r^\*, b^\*) > m_i$ or $max(l^\*, t^\*, r^\*, b^\*) < m_{i‚àí1}$, it is set as a negative sample and is thus not required to regress a bounding box anymore. Here $m_i$ is the maximum distance that feature level $i$ needs to regress. In this work, $m_2, m_3, m_4, m_5, m_6$ and $m_7$ are set as $0, 64, 128, 256, 512$ and $‚àû$, respectively. Since objects with different sizes are assigned to different feature levels and most overlapping happens between objects with considerably different sizes. If a location, even with multi-level prediction used, is still assigned to more than one ground-truth boxes, FCOS simply choose the groundtruth box with minimal area as its target.

FCOS also shares the heads between different feature levels, not only making the detector parameter-efficient but also improving the detection performance. However, authors observe that different feature levels are required to regress different size range (e.g., the size range is $[0, 64]$ for $P_3$ and $[64, 128]$ for $P_4$), and therefore it is not reasonable to make use of identical heads for different feature levels. As a result, instead of using the standard $exp(x)$, authors make use of $exp(s_ix)$ with a trainable scalar $s_i$ to automatically adjust the base of the exponential function for feature level $P_i$, which slightly improves the detection performance.


## Center-ness for FCOS

After using multi-level prediction in FCOS, there is still a performance gap between FCOS and anchor-based detectors. Authors observed that it is due to a lot of low-quality predicted bounding boxes produced by locations far away from the center of an object. They proposed a simple yet effective strategy to suppress these low-quality detected bounding boxes without introducing any hyper-parameters. Specifically, they add a single-layer branch, in parallel with the classification branch (as shown in figure above) to predict the ‚Äúcenter-ness‚Äù of a location. The center-ness depicts the normalized distance from the location to the center of the object that the location is responsible for. Given the regression targets $l^\*, t^\*, r^\*, b^\*$ for a location, the center-ness target is defined as:

$$ centerness^\* = \sqrt{\frac{min(l^\*, r^\*)}{max(l^\*, r^\*)} \times \frac{min(t^\*, b^\*)}{max(t^\*, b^\*)}} $$

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/08b5ec68-ce3c-434f-8e4e-ede2d1949830" alt="FCOS_centerness" height="350"/>
</p>

FCOS employs $sqrt$ here to slow down the decay of the center-ness. The center-ness ranges from 0 to 1 and is thus trained with binary cross entropy (BCE) loss. The loss is added to the loss function from previous equation. When testing, the final score (used for ranking the detected bounding boxes) is computed by multiplying the predicted center-ness with the corresponding classification score. Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. As a result, with high probability, these low-quality bounding boxes might be filtered out by the final non-maximum suppression (NMS) process, improving the detection performance remarkably. An alternative of the center-ness is to make use of only the central portion of ground-truth bounding box as positive samples with the price of one extra hyper-parameter, as shown in other works. After our submission, it has been shown that the combination of both methods can achieve a much better performance.


# CSP
2019 | [paper](https://arxiv.org/pdf/1911.11929.pdf) | _CSPNet: A new backbone that can enhance learning capability of CNN_

In this paper, authors propose Cross Stage Partial Network (**CSPNet**) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. They attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in the experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms SoTA approaches in terms of AP50 on the MS COCO object detection dataset.

The main purpose of designing CSPNet is to enable this architecture to achieve a richer gradient combination while reducing the amount of computation. This aim is achieved by partitioning feature map of the base layer into two parts and then merging them through a proposed cross-stage hierarchy. The main concept is to make the gradient flow propagate through different network paths by splitting the gradient flow. In this way, authors have confirmed that the propagated gradient information can have a large correlation difference by switching concatenation and transition steps. In addition, CSPNet can greatly reduce the amount of computation, and improve inference speed as well as accuracy. The proposed CSPNet-based object detector deals with the following three problems:
* **Strengthening learning ability of a CNN** - the accuracy of existing CNN is greatly degraded after lightweightening, so authors hoped to strengthen CNN‚Äôs learning ability, so that it can maintain sufficient accuracy while being lightweighted
* **Removing computational bottlenecks** - too high computational bottleneck will result in more cycles to complete the inference process, or some arithmetic units will often idle. Therefore, authors hoped that it is possible to evenly distribute the amount of computation at each layer in CNN so that they could effectively upgrade the utilization rate of each computation unit and thus reduce unnecessary energy consumption. It is noted that the proposed CSPNet makes the computational bottlenecks of PeleeNet cut into half. Moreover, in the MS COCO dataset-based object detection experiments, the proposed model can effectively reduce 80% computational bottleneck when test on YOLOv3-based models
* **Reducing memory costs** - the wafer fabrication cost of Dynamic Random-Access Memory (DRAM) is very expensive, and it also takes up a lot of space. If one can effectively reduce the memory cost, he/she will greatly reduce the cost of ASIC. In addition, a small area wafer can be used in a variety of edge computing devices. In reducing the use of memory usage, authors adopt cross-channel pooling to compress the feature maps during the feature pyramid generating process. In this way, the proposed CSPNet with the proposed object detector can cut down 75% memory usage on PeleeNet when generating feature pyramids

## Cross Stage Partial
CSP can be applied to many different architectures, e.g. for DenseNet it looks as following:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8b2f5b4b-0e56-47d4-98f3-d7619321094b" alt="CSP_densenet" height="300"/>
</p>

**DenseNet** - Each stage of a DenseNet contains a dense block and a transition layer, and each dense block is composed of $k$ dense layers. The output of the $i$-th dense layer will be concatenated with the input of the $i$-th dense layer, and the concatenated outcome will become the input of the $(i + 1)$-th dense layer

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/b182d102-e3c4-4a73-a813-f740389a7d48" alt="CSP_dense_0" height="100"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/16606c4c-ec5d-4291-a0eb-40d34e9d0390" alt="csp_dense_1" height="100"/>
</p>

The equations above show forward pass of DenseNet and how to gradients are calculated. We can find that large amount of gradient information are reused for updating weights of different dense layers. This will result in different dense layers repeatedly learn copied gradient information.

**Cross Stage Partial DenseNet** - the architecture of one-stage of the proposed CSPDenseNet is shown in figure above. A stage of CSPDenseNet is composed of a partial dense block and a partial transition layer. In a partial dense block, the feature maps of the base layer in a stage are split into two parts through channel $x_0 = [x^{‚Ä≤}_0, x^{‚Ä≤‚Ä≤}_0]$. Between $x^{‚Ä≤‚Ä≤}_0$ and $x^{‚Ä≤}_0$, the former is directly linked to the end of the stage, and the latter will go through a dense block. All steps involved in a partial transition layer are as follows:

* First, the output of dense layers, $[x^{‚Ä≤‚Ä≤}_0 , x_1, ..., x_k]$, will undergo a transition layer
* Second, the output of this transition layer, $x_T$ , will be concatenated with $x^{‚Ä≤‚Ä≤}_0$ and undergo another transition layer, and then generate output $x_U$

The equations of feed-forward pass and weight updating of CSPDenseNet are shown below.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/054b206c-8e84-4faf-8ed7-34e33ca911a3" alt="csp_csp_dense_0" height="100"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2ae4d07b-6c3b-42f2-aaa3-b909c3367188" alt="csp_csp_dense_1" height="100"/>
</p>

We can see that the gradients coming from the dense layers are separately integrated. On the other hand, the feature map $x^{‚Ä≤}_0$ that did not go through the dense layers is also separately integrated. As to the gradient information for updating weights, both sides do not contain duplicate gradient information that belongs to other sides.

Overall speaking, the proposed CSPDenseNet preserves the advantages of DenseNet‚Äôs feature reuse characteristics, but at the same time prevents an excessively amount of duplicate gradient information by truncating the gradient flow. This idea is realized by designing a hierarchical feature fusion strategy and used in a partial transition layer.

The figure below shows the effect of applying CSP to DenseBlock:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/1da99aab-5b9e-48a0-b865-16120ae63edf" alt="csp_vs_dense" height="200"/>
</p>


**Partial Transition Layer** - the purpose of designing partial transition layers is to maximize the difference of gradient combination. The partial transition layer is a hierarchical feature fusion mechanism, which uses the strategy of truncating the gradient flow to prevent distinct layers from learning duplicate gradient information. By using the split and merge strategy across stages, it is possible to effectively reduce the possibility of duplication during the information integration process.

**Applying CSP to Other Architectures**

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/094aef48-b87d-4040-9721-7c021a6cec24" alt="csp_resnext" height="200"/>
</p>


## Exact Fusion Model

Another contribution of this paper is the Exact Fusion Model (EFM) that captures an appropriate Field of View (FoV) for each anchor, which enhances the accuracy of the one-stage object detector. For segmentation tasks, since pixel-level labels usually do not contain global information, it is usually more preferable to consider larger patches for better information retrieval. However, for tasks like image classification and object detection, some critical information can be obscure when observed from image-level and bounding box-level labels. Earlier works found that CNN can be often distracted when it learns from image-level labels and concluded that it is one of the main reasons that two-stage object detectors outperform one-stage object detectors.

**Aggregate Feature Pyramid** the proposed EFM is able to better aggregate the initial feature pyramid. The EFM is based on YOLOv3, which assigns exactly one bounding-box prior to each ground truth object. Each ground truth bounding box corresponds to one anchor box that surpasses the threshold IoU. If the size of an anchor box is equivalent to the FoV of the grid cell, then for the grid cells of the sth scale, the corresponding bounding box will be lower bounded by the $(s ‚àí 1)$-th scale and upper bounded by the $(s + 1)$-th scale. Therefore, the EFM assembles features from the three scales.

**Balance Computation** - since the concatenated feature maps from the feature pyramid are enormous, it introduces a great amount of memory and computation cost. To alleviate the problem, authors incorporated the Maxout technique to compress the feature maps.

The figure below shows comparison of EFM with other feature pyramid fusion strategies.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c25f0349-4963-406d-8ace-bb3a8c4a63ab" alt="CSP_EFM" height="200"/>
</p>

> **_NOTE:_** Most of the new YOLO approaches (mostly from the same authors as CSP) use **CSP** only to update the computational blocks.



# ATSS
2020 | [paper](https://arxiv.org/pdf/1912.02424) | _Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection_

In this paper, authors discuss the differences between anchor-based and anchor-free object detectors and point out that the essential difference (leading to performance gap) is related to how to define positive and negative training samples. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, they propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them.

In recent years, object detection has been dominated by anchor-based detectors, which can be generally divided into one-stage methods and two-stage methods. Both of them first tile a large number of preset anchors on the image, then predict the category and refine the coordinates of these anchors by one or several times, finally output these refined anchors as detection results. Because two-stage methods refine anchors several times more than one-stage methods, the former one has more accurate results while the latter one has higher computational efficiency. SoTA results on common detection benchmarks are still held by anchor-based detectors. 

Recent academic attention has been geared toward anchor-free detectors due to the emergence of FPN and Focal Loss. Anchor-free detectors directly find objects without preset anchors in two different ways. One way is to first locate several pre-defined or self-learned keypoints and then bound the spatial extent of objects. We call this type of anchor-free detectors as keypoint-based methods. Another way is to use the center point or region of objects to define positives and then predict the four distances from positives to the object boundary. We call this kind of anchor-free detectors as center-based methods. Keypoint-based methods follow the standard keypoint estimation pipeline that is different from anchor-based detectors. These anchor-free detectors are able to eliminate those hyperparameters related to anchors and have achieved similar performance with anchor-based detectors, making them more potential in terms of generalization ability

After comprehensive experiments, authors indicate that the essential difference between one-stage anchor-based detectors and center-based anchor- free detectors is actually how to define positive and negative training samples, which is important for current object detection and deserves further study.

**Adaptive Training Sample Selection**

When training an object detector, we first need to define positive and negative samples for classification, and then use positive samples for regression. According to the previous analysis, the former one is crucial and the anchor-free detector FCOS improves this step. It introduces a new way to define positives and negatives, which achieves better performance than the traditional IoU-based strategy. Inspired by this, we delve into the most basic issue in object detection: _how to define positive and negative training samples_, and propose an Adaptive Training Sample Selection (ATSS). Compared with these traditional strategies, our method almost has no hyperparameters and is robust to different settings.

Previous sample selection strategies have some sensitive hyperparameters, such as IoU thresholds in anchor-based detectors and scale ranges in anchor-free detectors. After these hyperparameters are set, all ground-truth boxes must select their positive samples based on the fixed rules, which are suitable for most objects, but some outer objects will be neglected. Thus, different settings of these hyperparameters will have very different results.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d4413259-fa3e-40d9-b9f1-aa2784135d45" alt="ATSS_algo" height="600"/>
</p>

ATSS method that automatically divides positive and negative samples according to statistical characteristics of object almost without any hyperpa- rameter. Algorithm 1 (above) describes how the proposed method works for an input image. For each ground-truth box $g$ on the image, first find out its candidate positive samples. As described in Line 3 to 6, on each pyramid level, select $k$ anchor boxes whose center are closest to the center of $g$ based on $L_2$ distance. Supposing there are $L$ feature pyramid levels, the ground-truth box $g$ will have $k √ó L$ candidate positive samples. After that, compute the IoU between these candidates and the ground-truth $g$ as $D_g$ in Line 7, whose mean and standard deviation are computed as $m_g$ and $v_g$ in Line 8 and Line 9. With these statistics, the IoU threshold for this ground-truth $g$ is obtained as $t_g = m_g + v_g$ in Line 10. Finally, select these candidates whose IoU are greater than or equal to the threshold $t_g$ as final positive samples in Line 11 to 15. Notably, authors also limit the positive samples‚Äô center to the ground-truth box as shown in Line 12. Besides, if an anchor box is assigned to multiple ground-truth boxes, the one with the highest IoU will be selected. The rest are negative samples. Illustration of how ATSS works is shown below and some motivations behind ATSS method are explained as follows.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3237162d-f358-4f6a-a2c8-cfaeb1af5102" alt="ATSS" height="300"/>
</p>

* **Selecting candidates based on the center distance between anchor box and object** - for RetinaNet, the IoU is larger when the center of anchor box is closer to the center of object. For FCOS, the closer anchor point to the center of object will produce higher-quality detections. Thus, the closer anchor to the center of object is the better candidate

* **Using the sum of mean and standard deviation as the IoU threshold** - the IoU mean $m_g$ of an object is a measure of the suitability of the preset anchors for this object. A high $m_g$ as shown in figure above (left) indicates it has high-quality candidates and the IoU threshold is supposed to be high. A low $m_g$ as shown in figure above (right) indicates that most of its candidates are low-quality and the IoU threshold should be low. Besides, the IoU standard deviation $v_g$ of an object is a measure of which layers are suitable to detect this object. A high $v_g$ as shown in above figure (left) means there is a pyramid level specifically suitable for this object, adding $v_g$ to $m_g$ obtains a high threshold to select positives only from that level. A low $v_g$ as shown in above figure (right) means that there are several pyramid levels suitable for this object, adding $v_g$ to $m_g$ obtains a low threshold to select appropriate positives from these levels. Using the sum of mean $m_g$ and standard deviation $v_g$ as the IoU threshold $t_g$ can adaptively select enough positives for each object from appropriate pyramid levels in accordance of statistical characteristics of object.

* **Limiting the positive samples‚Äô center to object** - the anchor with a center outside object is a poor candidate and will be predicted by the features outside the object, which is not conducive to training and should be excluded.

* **Maintaining fairness between different objects** - according to the statistical theory, about 16% of samples are in the confidence interval $[m_g + v_g , 1]$ in theory. Although the IoU of candidates is not a standard normal distribution, the statistical results show that each object has about $0.2 ‚àó kL$ positive samples, which is invariant to its scale, aspect ratio and location. In contrast, strategies of RetinaNet and FCOS tend to have much more positive samples for larger objects, leading to unfairness between different objects.

* **Keeping almost hyperparameter-free** - ATSS method only has one hyperparameter $k$. Experiments from the paper prove that it is quite insensitive to the variations of $k$ and the proposed ATSS can be considered almost hyperparameter-free.

> **_NOTE:_** TODO.


# Effective Squeeze and Excitation
2020 | [paper](https://arxiv.org/pdf/1911.06667v6) | _CenterMask : Real-Time Anchor-Free Instance Segmentation_

Authors propose a simple yet efficient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchor-free one stage object detector (FCOS) in the same vein with Mask R-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise. Authors also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/28a5ad9b-3fe8-4cdb-8ba3-4ac2e11b81e0" alt="CenterMask" height="300"/>
</p>

CenterMask is an instance segmentation architecture composed of VoVNetV2 backbone with FPN as neck and equiped with FCOS detection head. On top of that there is a SAG-Mask component used to predict the segmentation masks on the FCOS-proposed RoI. The procedure of masking objects is composed of detecting objects from the FCOS box head and then predicting segmentation masks inside the cropped regions in a per-pixel manner.

The most important part in terms of object detection applications is the Effective Squeeze-Excitation (eSE), which improves the original SE. 

### Effective Squeeze-Excitation (eSE)

Squeeze-Excitation (SE) is a representative channel attention method adopted in CNN architectures, explicitly models the interdependency between the channels of feature maps to enhance its representation. The SE module squeezes the spatial dependency by global average pooling to learn a channel specific descriptor and then two fully-connected (FC) layers followed by a sigmoid function are used to rescale the input feature map to highlight only useful channels. In short, given input feature map $X_i ‚àà R^{C √ó W √ó H}$, the channel attention map $A_{ch}(X_i) ‚àà R^{C √ó 1 √ó 1}$ is computed as:

$$ A_{ch}(X_i) = œÉ(W_C (Œ¥(W_{C/r} (F_{gap}(X_i)))) $$

where 

$$ F_{gap}(X) = \frac{1}{WH} \sum_{i,j=1}^{W,H} X_{i, j} $$ 

is channel-wise global average pooling, $W_{C/r} , W_C ‚àà R^{C √ó 1 √ó 1}$ are weights of two fully-connected layers, $Œ¥$ denotes ReLU non-linear operator and $œÉ$ indicates sigmoid function. However, it is assumed that the SE module has a limitation: channel information loss due to dimension reduction. For avoiding high model complexity burden, two FC layers of the SE module need to reduce channel dimension. Specifically, while the first FC layer reduces input feature channels $C$ to $C/r$ using reduction ratio $r$, the second FC layer expands the reduced channels to original channel size $C$. As a result, this channel dimension reduction causes channel information loss.

Therefore, authors proposed effective SE (eSE) that uses only one FC layer with $C$ channels instead of two FCs without channel dimension reduction, which rather maintains channel information and in turn improves performance. the eSE process is defined as:

$$ A_{eSE}(X_{div} ) = œÉ(W_C (F_{gap}(X_{div}))) $$

$$ X_{refine} = A_{eSE}(X_{div}) ‚äó X_{div} $$

where $X_{div} ‚àà R^{C √ó W √ó H}$ is the diversified feature map computed by $1 √ó 1$ conv in OSA module. As a channel attentive feature descriptor, the $A_{eSE} ‚àà R^{C √ó 1 √ó 1}$ is applied to the diversified feature map $X_{div}$ to make the diversified feature more informative. Finally, when using the residual connection, the input feature map is element-wise added to the refined feature map $X_{refine}$. The details of How the eSE module is plugged into the OSA module are shown in
figure below.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/51fc37c6-4fbd-4bd7-8cea-a3dacd854d8f" alt="CenterMask_OSA" height="300"/>
</p>


# EfficientDet
2020 | [paper](https://arxiv.org/pdf/1911.09070) | _EfficientDet: Scalable and Efficient Object Detection_

In this paper, authors systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, they propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, they propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints.

Based on the one-stage detector paradigm, authors examine the design choices for backbone, feature fusion, and class/box network, and identify two main challenges:
* _Efficient multi-scale feature fusion_ ‚Äì FPN has been widely used for multiscale feature fusion. Recently, PANet, NAS-FPN, and other studies have developed more network structures for cross-scale feature fusion. While fusing different input features, most previous works simply sum them up without distinction; however, since these different input features are at different resolutions, it can be observed they usually contribute to the fused output feature unequally. To address this issue, authors propose a simple yet highly effective **Weighted Bi-Directional Feature Pyramid Network** (**BiFPN**), which introduces learnable weights to learn the importance of different input features, while repeatedly applying top-down and bottom-up multi-scale feature fusion.
* _model scaling_ - while previous works mainly rely on bigger backbone networks or larger input image sizes for higher accuracy, authors observed that scaling up feature network and box/class prediction network is also critical when taking into account both accuracy and efficiency. Inspired by recent works (EfficientNet, same authors), they propose a compound scaling method for object detectors, which jointly scales up the resolution/depth/width for all backbone, feature network, box/class prediction network.

Authors also combinined EfficientNet backbones with the proposed BiFPN and compound scaling, and have developed a new family of object detectors, named **EfficientDet**, which consistently achieve better accuracy with much fewer parameters and FLOPs than previous object detectors.

## BiFPN - Bidirectional cross-scale connections and weighted feature fusion

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/0f322730-fc06-430d-bb8e-138c60345e15" alt="efficient_det_bifpn_vs_all" height="400"/>
</p>

**Problem formulation**

Multi-scale feature fusion aims to aggregate features at different resolutions. Formally, given a list of multi-scale features $P^{in} = (P^{in}_{l_1} , P^{in}_{l_2} , ...), where $P^{in}_{l_i}$ represents the feature at level $l_i$, the goal is to find a transformation $f$ that can effectively aggregate different features and output a list of new features: $P^{out} = f(P^{in})$. As a concrete example, figure above shows the conventional top-down FPN. It takes level 3-7 input features $P^{in} = (P^{in}_3 , ... P^{in}_7)$, where $P^{in}_i$ represents a feature level with resolution of $1/{2^{i}}$ of the input images. For instance, if input resolution is $640 x 640$, then $P^{in}_3$ represents feature level 3 $(640/{2^3} = 80)$ with resolution $80 x 80$, while $P^{in}_7$ represents feature level 7 with resolution $5 x 5$. The conventional FPN aggregates multi-scale features in a top-down manner:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/33205b9f-5f82-4105-a61a-2cbdb356de59" alt="efficient_det_fpn_eq" height="150"/>
</p>

where _Resize_ is usually a upsampling or downsampling operation for resolution matching, and _Conv_ is usually a convolutional operation for feature processing.


**Cross-Scale Connections**

Conventional top-down FPN is inherently limited by the one-way information flow. To address this issue, PANet adds an extra bottom-up path aggregation network, as shown in Figure above (b). Recently, NAS-FPN employs neural architecture search to search for better cross-scale feature network topology, but it requires thousands of GPU hours during search and the found network is irregular and difficult to interpret or modify, as shown in figure above (c). By studying the performance and efficiency of these three networks, one can observe that PANet achieves better accuracy than FPN and NAS-FPN, but with the cost of more parameters and computations. To improve model efficiency, this paper proposes several optimizations for cross-scale connections: 

* First - remove those nodes that only have one input edge. The intuition is simple: if a node has only one input edge with no feature fusion, then it will have less contribution to feature network that aims at fusing different features. This leads to a simplified bi-directional network
* Second - add an extra edge from the original input to output node if they are at the same level, in order to fuse more features without adding much cost
* Third - unlike PANet that only has one top-down and one bottom-up path, treat each bidirectional (top-down & bottom-up) path as one feature network layer, and repeat the same layer multiple times to enable more high-level feature fusion. 

With these optimizations, the new feature network is named bidirectional feature pyramid network (BiFPN), as shown in figures above and below.


**Weighted Feature Fusion**
When fusing features with different resolutions, a common way is to first resize them to the same resolution and then sum them up. PAN introduces global self-attention upsampling to recover pixel localization. All previous methods treat all input features equally without distinction. However, authors observed that since different input features are at different resolutions, they usually contribute to the output feature unequally. To address this issue, they proposed to add an additional weight for each input, and let the network to learn the importance of each input feature. Based on this idea, they considered three weighted fusion approaches:

* **Unbounded fusion** - $O = \sum_{i} w_i * I_i$, where $w_i$ is a learnable weight.
* **Softmax-based fusion** - $O = \sum_{i} \frac{e^{w_i}}{\sum_{j}e^{w_j}} * I_i$ - an intuitive idea is to apply softmax to each weight, such that all weights are normalized to be a probability with value range from 0 to 1, representing the importance of each input. However, as shown later, the extra softmax leads to significant slowdown on GPU hardware
* **Fast normalized fusion** - $O = \sum_{i} \frac{w_i}{\epsilon + \sum_{j}w_j} * I_i$, where
$w_i ‚â• 0$ is ensured by applying a ReLU after each $w_i$, and $\epsilon = 0.0001$ is a small value to avoid numerical instability. Similarly, the value of each normalized weight also falls between 0 and 1, but since there is no softmax operation here, it is much more efficient. The ablation study shows this fast fusion approach has very similar learning behavior and accuracy as the softmax-based fusion, but runs up to 30% faster on GPUs.

The final BiFPN integrates both the bidirectional cross-scale connections and the fast normalized fusion. As a concrete example, here is described the two fused features at
level 6 for BiFPN shown in figure above (d):

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3b1d6b83-16aa-4852-a5aa-e76f21dd02a6" alt="efficient_det_bifpn_eq" height="120"/>
</p>

where $P^{td}_6$ is the intermediate feature at level 6 on the top-down pathway, and $P^{out}_6$ is the output feature at level 6 on the bottom-up pathway. All other features are constructed in a similar manner. Notably, to further improve the efficiency, authors used depthwise separable convolution for feature fusion, and add batch normalization and activation after each convolution.


## EfficientDet

Based on the BiFPN, authors have developed a new family of detection models named EfficientDet. Figure below shows the overall architecture of EfficientDet, which largely follows the one-stage detectors paradigm. Authors employ ImageNet-pretrained EfficientNets as the backbone network. The proposed BiFPN serves as the feature network, which takes level 3-7 features ${P_3, P_4, P_5, P_6, P_7}$ from the backbone network and repeatedly applies top-down and bottom-up bidirectional feature fusion. These fused features are fed to a class and box network to produce object class and bounding box predictions respectively. The class and box network weights are shared across all levels of features.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ea80cb8a-b8e5-47fe-a391-55bf993dd64f" alt="efficient_det" height="400"/>
</p>

## Compound Scaling

Compound scaling is a method proposed in [EfficientNet](https://arxiv.org/pdf/1905.11946v5),  that uniformly scales all dimensions of network width, depth, and resolution using a fixed ratio. It involves adjusting the network depth, width, and image resolution simultaneously with constant coefficients to achieve a balance between these dimensions. The compound scaling method simplifies the process of scaling ConvNets by determining how many more resources are available for model scaling and how to allocate these resources to width, depth, and resolution. By using compound scaling, the total FLOPS of the network can be approximately increased by a factor determined by the scaling coefficients


> **_NOTE:_** Most of the new YOLO approaches uses the BiFPN module and/or the feature pyramid weighting mechanism.



# IoU-Aware
2020 | [paper](https://arxiv.org/pdf/1912.05992) | _IoU-aware Single-stage Object Detector for Accurate Localization_

Single-stage object detectors have been widely applied in many computer vision applications due to their simpleness and high efficiency. However, the low correlation between the classification score and localization accuracy in detection results severely hurts the average precision of the detection model. To solve this problem, an IoU-aware single-stage object detector is proposed in this paper. Specifically, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the predicted IoU is multiplied by the classification score to compute the final detection confidence, which is more correlated with the localization accuracy. The detection confidence is then used as the input of the subsequent NMS and COCO AP computation, which substantially improves the localization accuracy of model


<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/582abcd5-f495-43a1-a388-b0d31fb44291" alt="iou_aware" height="350"/>
</p>

The IoU-aware single-stage object detector is mostly modified from RetinaNet with the same backbone and feature pyramid network (FPN) as above figure shows. Different from the RetinaNet, an IoU prediction head parallel with the regression head is designed in the last layer of regression branch to predict the IoU for each detected box while the classification branch is kept the same. To keep the model‚Äôs efficiency, the IoU prediction head consists of only a single $3 x 3$ convolution layer, followed by a sigmoid activation layer to ensure that the predicted IoU is in the range of $[0, 1]$. There are many other choices about the design of the IoU prediction head, such as designing an independent IoU prediction branch being the same as the classification branch and regression branch, but this kind of design will severely hurt the model‚Äôs efficiency. This design brings negligible computation burden to the whole model and can still substantially improve the model‚Äôs performance

## Training

As the same as RetinaNet, the focal loss is adopted for the classification loss and the smooth $L_1$-loss is adopted for the regression loss. The binary cross-entropy loss(BCE) is adopted for the IoU prediction loss and only the losses for the positive examples are computed. $IoU_i$ represents the predicted IoU for each detected box and $\hat{IoU}_i$ is the target IoU computed between the regressed positive example $b_i$ and the corresponding ground truth box $\hat{b}_i$. During training, whether to compute the gradient of $L_{IoU}$ with respect to $\hat{IoU}_i$ makes difference to the model‚Äôs performance. This is caused by that the gradient from IoU prediction head can be back-propagated to the regression head if the gradient of $L_{IoU}$ with respect to $\hat{IoU}_i$ is computed during training. The gradient is computed as shown in equations below. Two observations about the gradient can be obtained. Firstly, because the predicted IoU for most of the positive examples is not smaller than 0.5, the gradient is mostly non-positive and will guide the regression head to predict box $b_i$ that increases the target IoU ($\hat{IoU}_i$) which is computed between the predicted box $b_i$ and the corresponding ground truth box $\hat{b}_i$. Secondly, as the predicted IoU ($IoU_i$) increases, the magnitude of gradient that increases the target IoU ($\hat{IoU}_i$) increases. This reduces the gap between the predicted IoU ($IoU_i$) and the target IoU ($\hat{IoU}_i$) and makes the predicted IoU more correlated with the target IoU. These two effects make our method more powerful for accurate localization as demonstrated in the following experiments. Other kinds of loss functions can also be considered, such as $L_2$-loss and $L_1$-loss. These different loss functions are compared in the following experiments. During training, the IoU prediction head is trained jointly with the classification head and regression head.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3d288711-99ec-4249-9500-4a382fcc0ef5" alt="iou_aware_loss" height="130"/>
</p>
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/81b76a4c-0358-44b2-a1a3-26de39549056" alt="iou_aware_loss_2" height="220"/>
</p>


## Inference

During inference, the classification score $p_i$ is multiplied by the predicted IoU $IoU_i$ for each detected box to calculate the final detection confidence $S_{det}$. The parameter $Œ±$ in the range of $[0, 1]$ is designed to control the contribution of the classification score and predicted IoU to the final detection confidence. This detection confidence can simultaneously be aware of the classification score and localization accuracy and thus is more correlated with the localization accuracy than the classification score only. And it is used to rank all the detections in the subsequent NMS and AP computation. The rankings of poorly localized detections with high classification score decrease while the rankings of well localized detections with low classification score increase, thus improving the localization accuracy of the models.

$$ S_{det} = p_{i}^{\alpha} IoU_{i}^{1-\alpha} $$



# Distribution Focal Loss
2020 | [paper](https://arxiv.org/pdf/2006.04388) | _Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection_
TODO



# Matrix-NMS
2020 | [paper](https://arxiv.org/pdf/2003.10152v3) | _SOLOv2: Dynamic and Fast Instance Segmentation_
TODO



# NoNMS-YOLO
2021 | [paper](https://arxiv.org/pdf/2101.11782) | _Object Detection Made Simpler by Eliminating Heuristic NMS_
TODO



# CopyPaste
2021 | [paper](https://arxiv.org/pdf/2012.07177) | _Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation_
TODO




# OTA
2021 | [paper](https://arxiv.org/pdf/2103.14259) | _OTA: Optimal Transport Assignment for Object Detection_
TODO



# Varifocal Loss
2021 | [paper](https://arxiv.org/pdf/2008.13367) | _VarifocalNet: An IoU-aware Dense Object Detector_
TODO




# Task Aligned Learning
2021 | [paper](https://arxiv.org/pdf/2108.07755) | _TOOD: Task-aligned One-stage Object Detection_
TODO




# TODO
2021 | [paper](https://arxiv.org/pdf/2111.00902) | _PP-PicoDet: A Better Real-Time Object Detector on Mobile Devices_
TODO




# RepVGG
2021 | [paper](https://arxiv.org/pdf/2101.03697.pdf) | _RepVGG: Making VGG-style ConvNets Great Again_

Authors present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of $3 √ó 3$ convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named **RepVGG**.

It is challenging for a plain model to reach a comparable level of performance as the multi-branch architectures. An explanation is that a multi-branch topology, e.g., ResNet, makes the model an implicit ensemble of numerous shallower models, so that training a multi-branch model avoids the gradient vanishing problem. Since the benefits of multi-branch architecture are all for training and the drawbacks are undesired for inference, authors proposed to decouple the training-time multi-branch and inference-time plain architecture via structural re-parameterization, which means converting the architecture from one to another via transforming its parameters. To be specific, a network structure is coupled with a set of parameters, e.g., a conv layer is represented by a 4th-order kernel tensor. If the parameters of a certain structure can be converted into another set of parameters coupled by another structure, we can equivalently replace the former with the latter, so that the overall network architecture is changed.

Specifically, authors construct the training-time RepVGG using _identity_ and $1 √ó 1$ branches, which is inspired by ResNet but in a different way that the branches can be removed by structural re-parameterization (figures below). After training, they perform the transformation with simple algebra, as an identity branch can be regarded as a degraded $1 √ó 1$ _conv_, and the latter can be further regarded as a degraded $3 √ó 3$ _conv_, so that we can construct a single $3 √ó 3$ kernel with the trained parameters of the original $3 √ó 3$ kernel, _identity_ and $1 √ó 1$ branches and batch normalization (BN) layers. Consequently, the transformed model has a stack of $3 √ó 3$ _conv_ layers, which is saved for test and deployment. Notably, the body of an inference-time RepVGG only has one single type of operator: $3 √ó 3$ _conv_ followed by ReLU, which makes RepVGG fast on generic computing devices like GPUs. Even better, RepVGG allows for specialized hardware to achieve even higher speed because given the chip size and power consumption, the fewer types of operators we require, the more computing units we can integrate onto the chip. Consequently, an inference chip specialized for RepVGG can have an enormous number of $3 √ó 3$-ReLU units and fewer memory units (because the plain topology is memory-economical.

There are at least three reasons for using simple ConvNets, they are **fast**, **memory-economical** and **Flexible*

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/15e5d734-fb54-4f18-a076-2638bfada436" alt="rep_vgg_overview" height="450"/>
</p>

### Re-param for Plain Inference-time Model - how to convert a trained block into a single $3 x 3$ _conv_ layer for inference?

Note that authors use BN in each branch before the addition. Formally, we use $W^{(3)}$ (shape: $C_2 √ó C_1 √ó 3 √ó 3$) to denote the kernel of a $3 √ó 3$ _conv_ layer with $C_1$ input channels and $C_2$ output channels, and $W^{(1)}$ (shape: $C_2 √ó C_1$ for the kernel of $1 √ó 1$ branch. We use $Œº^{(3)}, œÉ^{(3)}, Œ≥^{(3)}, Œ≤^{(3)}$ as the accumulated _mean_, _standard deviation_ and learned _scaling factor_ and _bias_ of the BN layer following $3 √ó 3$ _conv_ $3 √ó 3$ _conv_, $Œº^{(1)}, œÉ^{(1)}, Œ≥^{(1)}, Œ≤^{(1)}$ for the BN following $1 √ó 1$ _conv_, and $Œº^{(0)}, œÉ^{(0)}, Œ≥^{(0)}, Œ≤^{(0)}$ for the identity branch. Let $M^{(1)}$ (shape: $N √ó C_1 √ó H_1 √ó W_1$), $M^{(2)}$ (shape: $N √ó C_2 √ó H_2 √ó W_2$ be the input and output, respectively, and $‚àó$ be the convolution operator. If $C_1 = C_2$, $H_1 = H_2$, $W_1 = W_2$, we have:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/25374565-6b06-41a0-952d-1e6b9d47bc1f" alt="RepVGG_M2_eq" height="120"/>
</p>

Otherwise, we simply use no identity branch, hence the above equation only has the first two terms. Here $b_n$ is the inference-time BN function, formally, $‚àÄ1 ‚â§ i ‚â§ C_2$,

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/13b7c59e-8a49-4e21-9241-ab5e25223b41" alt="RepVGG_bn_eq" height="60"/>
</p>

We first convert every BN and its preceding _conv_ layer into a _conv_ with a bias vector. Let ${W‚Ä≤, b‚Ä≤}$ be the kernel and bias converted from ${W, Œº, œÉ, Œ≥, Œ≤}$, we have

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/e22af79b-3969-4aea-a23c-ad5f04b26313" alt="RepVGG_Wi_eq" height="60"/>
</p>

Then it is easy to verify that $‚àÄ1 ‚â§ i ‚â§ C$

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/f0e8104d-fed5-47ac-9427-5b06ded5d58b" alt="RepVGG_bn_eq_2" height="60"/>
</p>

This transformation also applies to the identity branch because an identity can be viewed as a $1 √ó 1$ _conv_ with an identity matrix as the kernel. After such transformations, we will have one $3 √ó 3$ kernel, two $1 √ó 1$ kernels, and three bias vectors. Then we obtain the final bias by adding up the three bias vectors, and the final $3 √ó 3$ kernel by adding the $1 √ó 1$ kernels onto the central point of $3 √ó 3$ kernel, which can be easily implemented by first zero-padding the two $1 √ó 1$ kernels to $3 √ó 3$ and adding the three kernels up, as shown figure below. Note that the equivalence of such transformations requires the $3 √ó 3$ and $1 √ó 1$ layer to have the same stride, and the padding configuration of the latter shall be one pixel less than the former. For example, for a $3 √ó 3$ layer that pads the input by one pixel, which is the most common case, the $1 √ó 1$ layer should have $padding = 0$.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d60b5a88-7cd0-4581-b746-377b7ecfe96a" alt="RepVGG_Reparam" height="450"/>
</p>

## Architecture

RepVGG is VGG-style in the sense that it adopts a plain topology and heavily uses $3 √ó 3$ conv, but it does not use _max-pooling_ like VGG because authors desired the body to have only one type of operator. They arranged the $3 √ó 3$ layers into 5 stages, and the first layer of a stage down-samples with the $stride = 2$. For image classification, they use global average pooling followed by a fully-connected layer as the head. For other tasks, the task-specific heads can be used on the features produced by any layer.

Authors decided the numbers of layers of each stage following three simple guidelines:

* The first stage operates with large resolution, which is time-consuming, so they used only
one layer for lower latency
* The last stage shall have more channels, so they use only one layer to save the parameters
* They put the most layers into the second last stage (with $14 √ó 14$ output resolution on ImageNet), following ResNet and its recent variants (e.g., ResNet-101 uses 69 layers in its $14 √ó 14$-resolution stage).



# ELAN
2022 | [paper](https://arxiv.org/pdf/2211.04800) | _Designing Network Design Strategies Through Gradient Path Analysis_
TODO



# PRB-FPN
2023 | [paper](https://arxiv.org/pdf/2012.01724) | _Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection_
TODO


