# Object detectors

## Object detection pipelines

### One-stage 

Examples: YOLO, SSD, RetinaNet, CenterNet

### Two-stage 

Examples: RCNN, FasterRCNN

During inference, an object detection network performs a sequence of convolution operations on an image using a deep convolutional neural network (CNN). The network bifurcates into two branches at a layer _L_ — one branch generates region proposals while the other performs classification and regression by pooling convolutional features inside regions of interest (RoIs) generated by the proposal network. The proposal network generates classification scores and regression offsets for anchor boxes of multiple scales and aspect ratios placed at each pixel in the convolutional feature map. It then ranks these anchor boxes and selects the top _K_ (≈ 6000) anchors to which the bounding box regression offsets are added to obtain image level co-ordinates for each anchor. Greedy non-maximum suppression is applied to top K anchors which eventually generates region proposals.

The classification network generates classification and regression scores for each proposal generated by the proposal network. Since there is no constraint in the network which forces it to generate a unique RoI for an object, multiple proposals may correspond to the same object. Hence, other than the first correct bounding-box, all other boxes on the same object would generate false positives. To alleviate this problem, non-maximum-suppression is performed on detection boxes of each class independently, with a specified overlap threshold. Since the number of detections is typically small and can be further reduced by pruning detections which fall below a very small threshold, applying non-maximum suppression at this stage is not computationally expensive. We present an alternative approach to this non-maximum suppression algorithm in the object detection pipeline. An overview of the object detection pipeline is shown below

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6785054f-050d-441e-b15a-f17996e9c3d3" alt="two_stage_det" height="350"/>
</p>

## Bag of Freebies (BoF) and Bag of Specials (BoS)
Object detectors are constantly upgraded by addressing two main concepts, that is the Bag of Freebies (**BoF**) and Bag of Specials (**BoS**). 

### Bag of Freebies
Methods that only change the training strategy or only increase the training cost. Usually, a conventional object detector is trained offline. Therefore, researchers always like to take this advantage and develop better training methods which can make the object detector receive better accuracy without increasing the inference cost. The possible BoF include:
* **Data augmentation**
	* **photometric distortions** - adjust the brightness, contrast, hue, saturation and noise of an image
	* **geometric distortions** - add random scaling, cropping, flipping and rotating 
	* [**random erase**](100), [CutOut](https://arxiv.org/pdf/1708.04552) - randomly select the rectangle region in an image and fill a random or zero value
	* [**Hide and Seek**](https://arxiv.org/pdf/1811.02545), [Grid Mask](https://arxiv.org/pdf/2001.04086) - randomly or evenly select multiple rectangle regions in an image and replace with zeros
	* [**DropOut**](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), [DropConnect](http://proceedings.mlr.press/v28/wan13.pdf), [DropBlock](https://arxiv.org/pdf/1810.12890) - randomly or evenly select multiple rectangle regions in a feature map and replace with zeros
	* [**MixUp**](http://arxiv.org/pdf/1710.09412) - multiply and superimpose two images with different coefficients ratios and adjust the label with these superimposed ratios
	* [**CutMix**](https://arxiv.org/pdf/1905.04899) - cover the cropped image to rectangle region of other images and adjust the label according to the size of the mix area 
	* [**style transfer GAN**](https://arxiv.org/pdf/1811.12231) - use GAN to change style of the image (helps to reduce the texture bias learned by CNN)
* **Data imbalance** - Solving the problem of biased dataset semantic distribution (problem of data imbalance between different classes)
	* [**hard negative example mining**](https://ieeexplore.ieee.org/document/655648)
	* **Online Hard Example Mining** ([OHEM](https://arxiv.org/pdf/1604.03540)) - bootstrapping technique that modifies SGD to sample from examples in a non-uniform way depending on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution that favors diverse, high loss instances
 	* [**Focal loss**](https://arxiv.org/pdf/1708.02002) - reshaping the standard cross entropy loss such that it downweights the loss assigned to well classified examples
* **Target labels** - express the relationship of the degree of association between different categories with the one-hot hard representation:
	* [**Label smoothing**](https://arxiv.org/pdf/1512.00567) - convert hard label into soft label for training, which can make model more robust
 	* [**Label smoothing with refinement network**](https://arxiv.org/pdf/1703.00551) - via knowledge distillation
* **Objective function** of Bounding Box regression
	* MSE - directly perform regression on the center point coordinates and height and width of the bbox
 	* [**IoU loss**](https://arxiv.org/pdf/1608.01471) - puts the coverage of predicted bbox area and ground truth BBox area into consideration. The IoU loss computing process will trigger the calculation of the four coordinate points of the bbox by executing IoU with the ground truth, and then connecting the generated results into a whole code. Because IoU is a scale invariant representation, it can solve the problem that when traditional methods calculate the $L_1$ or $L_2$ loss of ${x, y, w, h}$ (MSE treat these points as independent variables), the loss will increase with the scale.
	* **General IoU Loss** ([GIoU loss](https://arxiv.org/pdf/1902.09630)) - include the shape and orientation of object in addition to the coverage area. GIoU proposes to find the smallest area BBox that can simultaneously cover the predicted bbox and ground truth bbox, and use this bbox as the denominator to replace the denominator originally used in IoU loss
 	* **Distance IoU Loss** ([DIoU loss](https://arxiv.org/pdf/1911.08287)) - additionally considers the distance of the center of an object
  	* **Complete IoU Loss** ([CIoU loss](https://arxiv.org/pdf/1911.08287)) - simultaneously considers the overlapping area, the distance between center points, and the aspect ratio. CIoU can achieve better convergence speed and accuracy on the bbox regression problem
 
### Bag of Specials
Those plugin modules and post-processing methods that only increase the inference cost by a small amount but can significantly improve the accuracy of object detection. Generally speaking, these plugin modules are for enhancing certain attributes in a model, such as enlarging receptive field, introducing attention mechanism, or strengthening feature integration capability, etc. The post-processing is a method for screening model prediction results. The possible BoS include:
* **Enhance receptive field**
	* **Spatial Pyramid Pooling** ([SPP](25)) - originated from Spatial Pyramid Matching ([SPM](39)). SPMs original method was to split feature map into several $d × d$ equal blocks, where d can be ${1, 2, 3, ...}$, thus forming spatial pyramid, and then extracting bag-of-word features. SPP integrates SPM into CNN and use _max-pooling_ operation instead of _bag-of-word_ operation. Since the SPP module will output one dimensional feature vector, it is infeasible to be applied in Fully Convolutional Network (FCN). Thus in the design of [YOLOv3](63), Redmon and Farhadi improved SPP module to the concatenation of max-pooling outputs with kernel size $k × k$, where $k = {1, 5, 9, 13}$, and stride equals to $1$. Under this design, a relatively large $k × k$ max-pooling effectively increase the receptive field of backbone feature
	* **Atrous Spatial Pyramid Pooling** ([ASPP](5)) - exploits multi-scale features by employing multiple dilated convolutions with _kernel\_size = 3 × 3_, _dilated\_ratio = k_, $stride = 1$
	* **Receptive Field Block** ([RFB](47)) - use several dilated convolutions of $k × k$ kernel, dilated ratio equals to $k$, and stride equals to $1$ to obtain a more comprehensive spatial coverage than ASPP. RFB makes use of multi-branch pooling with varying kernels corresponding to RFs of different sizes, applies dilated convolution layers to control their eccentricities, and reshapes them to generate final representation
* **Attention Module**
	* **Squeeze-and-Excitation** ([SE](https://arxiv.org/pdf/1709.01507)) - architectural unit designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature recalibration. Spatial dimensions of block input feature maps are squeezed into a single numeric value using _average pooling_, then a dense layer (with ReLU) adds non-linearity and output channel complexity is reduced by a ratio and another dense layer (with sigmoid) gives each channel a smooth gating (weight, attention) function to finally weight each input feature map of the block based on using the attention weights. 
	* **Spatial Attention Module** ([SAM](https://arxiv.org/pdf/1807.06521v2)) - A Spatial Attention Module (introduced in [CBAM](#cbam)) is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the channel attention, the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply _average-pooling_ and _max-pooling_ operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a convolution layer to generate a spatial attention map which encodes where to emphasize or suppress.
* **Feature Integration**
	* **Feature Pyramid Networks** ([FPN](https://arxiv.org/pdf/1612.03144)) -  feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection. The construction of the pyramid involves a bottom-up pathway and a top-down pathway. The **bottom-up** pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For ResNets we use the feature activations output by each stage’s last residual block. The **top-down** pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.
	* **Path Aggregation Networks** ([PAN](https://arxiv.org/pdf/1803.01534v4)) - aims to boost information flow in a proposal-based instance segmentation framework. Specifically, the feature hierarchy is enhanced with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. Additionally, adaptive feature pooling is employed, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.
	* **Scale-wise Feature Aggregation** Module ([SFAM](https://arxiv.org/pdf/1811.04533v3)) - use SE module to execute channel wise level re-weighting on multi-scale concatenated feature maps
	* **Adaptively Spatial Feature Fusion** ([ASFF](https://arxiv.org/pdf/1911.09516v2)) - learns the way to spatially filter conflictive information to suppress inconsistency across different feature scales, thus improving the scale-invariance of features. ASFF enables the network to directly learn how to spatially filter features at other levels so that only useful information is kept for combination. For the features at a certain level, features of other levels are first integrated and resized into the same resolution and then trained to find the optimal fusion. At each spatial location, features at different levels are fused adaptively, i.e., some features may be filter out as they carry contradictory information at this location and some may dominate with more discriminative clues. ASFF offers several advantages:
 		* as the operation of searching the optimal fusion is differential, it can be conveniently learned in back-propagation
   		* it is agnostic to the backbone model and it is applied to single-shot detectors that have a feature pyramid structure
     	* its implementation is simple and the increased computational cost is marginal.
	* **Weighted Bi-directional Feature Pyramid Networks** ([BiFPN](https://arxiv.org/pdf/1911.09070v7)) - type of feature pyramid network which allows easy and fast multi-scale feature fusion. It incorporates the multi-level feature fusion idea from FPN, PANet and NAS-FPN that enables information to flow in both the top-down and bottom-up directions, while using regular and efficient connections. It also utilizes a fast normalized fusion technique. Traditional approaches usually treat all features input to the FPN equally, even those with different resolutions. However, input features at different resolutions often have unequal contributions to the output features. Thus, the BiFPN adds an additional weight for each input feature allowing the network to learn the importance of each. All regular convolutions are also replaced with less expensive depthwise separable convolutions. Comparing with PANet, PANet added an extra bottom-up path for information flow at the expense of more computational cost. Whereas BiFPN optimizes these cross-scale connections by removing nodes with a single input edge, adding an extra edge from the original input to output node if they are on the same level, and treating each bidirectional path as one feature network layer (repeating it several times for more high-level future fusion).
* **Activation function**
	* **Rectified Linear Unit** ([ReLU](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)) - solve the gradient vanish problem which is frequently encountered in traditional tanh and sigmoid activation function
	* **Leaky ReLU** ([LReLU](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)) and Parametric ReLU ([PReLU](https://arxiv.org/pdf/1502.01852)) - solve the problem that the gradient of ReLU is zero when the output is less than zero
	* [**ReLU6**](https://arxiv.org/pdf/1704.04861) and [**hard-Swish**](https://arxiv.org/pdf/1905.02244) - specially designed for quantization networks
	* Scaled Exponential Linear Unit ([SELU](https://arxiv.org/pdf/1706.02515v5)) - for self-normalizing a neural network
	* [**Swish**](https://arxiv.org/pdf/1710.05941v2) and [**Mish**](https://arxiv.org/pdf/1908.08681) - continuously differentiable activation functions
* **Post-Processing**
	* **Non Maximum Supression** (NMS / [Greedy NMS](https://arxiv.org/pdf/1311.2524)) - Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores (Greedy NMS). The detection box $M$ with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with **M** are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. 
	* [**Soft-NMS**](https://arxiv.org/pdf/1704.04503v2) - it considers the problem that the occlusion of an object may cause the degradation of confidence score in greedy NMS with IoU score. Soft-NMS solves classic NMS problem by decaying the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process.
	* [**DIoU-NMS**](https://arxiv.org/pdf/1911.08287v1) - added the information of the center point distance to the bbox screening process on the basis of soft NMS. In original NMS, the IoU metric is used to suppress the redundant detection boxes, where the overlap area is the unique factor, often yielding false suppression for the cases with occlusion. With DIoU-NMS, we not only consider the overlap area but also central point distance between two boxes.

# YOLO




