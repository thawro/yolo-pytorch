# CBAM

2018 | [paper](https://arxiv.org/pdf/1807.06521.pdf) | _CBAM: Convolutional Block Attention Module_

Convolutional Block Attention Module (**CBAM**) is a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, CBAM sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/055969ff-3ef1-43bc-9d19-1da921f87680" alt="CBAM_overview" height="300"/>
</p>

Given an intermediate feature map $F$ (shape: $C × H × W$) as input, CBAM sequentially infers a $1D$ channel attention map $M_c$ (shape: $C × 1 × 1$) and a $2D$ spatial attention map $M_s$ (shape: $1 × H × W$) as illustrated above. The overall attention process can be summarized as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/503f9b0d-ed52-4094-bc7e-6988e1765968" alt="CBAM_overview_eq" height="80"/>
</p>

where $⊗$ denotes element-wise multiplication. During multiplication, the attention values are broadcasted (copied) accordingly: channel attention values are broadcasted along the spatial dimension, and vice versa. $F^{′′}$ is the final refined output. The figure below depicts the computation process of each attention map. The following describes the details of each attention module.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/12a9a4ed-4484-4207-99f0-c826c8b87253" alt="CBAM_detail" height="400"/>
</p>

**Channel attention module** - channel attention map is produced by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on **‘what’** is meaningful given an input image. The spatial dimension of the input feature map is squeezed to compute the channel attention efficiently. For aggregating spatial information, _average-pooling_ has been commonly adopted so far. Authors argued that _max-pooling_ gathers another important clue about distinctive object features to infer finer channel-wise attention. Thus, they used both average-pooled and max-pooled features simultaneously and empirically confirmed that exploiting both features greatly improves representation power of networks rather than using each independently.

The first step is to aggregate spatial information of a feature map by using both _average-pooling_ and _max-pooling_ operations, generating two different spatial context descriptors: _Fc{avg}_ and _Fc{max}_, which denote average-pooled features and max-pooled features respectively. Both descriptors are then forwarded to a shared network to produce the channel attention map $M_c$ (shape $C × 1 × 1$). The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $C/r × 1 × 1$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, the output feature vectors are merged using element-wise summation. In short, the channel attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/605f2500-ef28-4e3b-a0e8-b63fc4f5fdcc" alt="CBAM_channel_att" height="80"/>
</p>

where $σ$ denotes the sigmoid function, $W_0$ has shape $C/r × C$, and $W_1$ has shape $C × C/r$. Note that the MLP weights, $W_0$ and $W_1$, are shared for both inputs and the ReLU activation function is followed by $W_0$.


**Spatial attention module** - the spatial attention map is generated by utilizing the inter-spatial relationship of features. Different from the channel attention, the spatial attention focuses on **‘where’** is an informative part, which is complementary to the channel attention. To compute the spatial attention, the first step is to apply _average-pooling_ and _max-pooling_ operations along the channel axis and concatenate them to generate an efficient feature descriptor. Applying pooling operations along the channel axis is shown to be effective in highlighting informative regions. On the concatenated feature descriptor, the convolution layer is  applied to generate a spatial attention map $M_s(F)$ (shape $H × W$) which encodes where to emphasize or suppress.

The channel information of a feature map is aggregated by using two pooling operations, generating two 2D maps: $F^s_{avg}$ (shape $1 × H × W$) and $F^s_{max}$ (shape $1 × H × W$). Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the $2D$ spatial attention map. In short, the spatial attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20b00305-c0af-42ea-9d5b-a449f2f7d825" alt="CBAM_spatial_att" height="80"/>
</p>

where $σ$ denotes the sigmoid function and $f$ $7 × 7$ represents a convolution operation with the filter size of $7 × 7$.

**Arrangement of attention modules** - given an input image, two attention modules, channel and spatial, compute complementary attention, focusing on **‘what’** and **‘where’** respectively. Considering this, two modules can be placed in a parallel or sequential manner. Authours have found that the sequential arrangement gives a better result than a parallel arrangement. For the arrangement of the sequential process, the experimental result shows that the channel-first order is slightly better than the spatial-first

Example of CBAM integrated with a ResBlock in ResNet:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5f286642-9e32-4823-8b21-2d8debe9c269" alt="resblock_CBAM" height="270"/>
</p>

> **_NOTE:_** Some of the new YOLO approaches use either **Channel-Attention** or **Spatial-Attention** to enhance feature maps representations.
