# ELAN

2022 | [paper](https://arxiv.org/pdf/2211.04800) | _Designing Network Design Strategies Through Gradient Path Analysis_

Designing a high-efficiency and high-quality expressive network architecture has always been the most important research topic in the field of deep learning. Most of today’s network design strategies focus on how to integrate features extracted from different layers, and how to design computing units to effectively extract these features, thereby enhancing the expressiveness of the network. This paper proposes a new network design strategy, i.e., to design the network architecture based on gradient path analysis. On the whole, most of today’s mainstream network design strategies are based on feed forward path, that is, the network architecture is designed based on the _data path_. In this paper, authors hope to enhance the expressive ability of the trained model by improving the network learning ability. The mechanism driving the network parameter learning is the backward propagation algorithm, so authors design network design strategies based on back propagation path. They propose the gradient path design strategies for the **_layer-level_**, the **_stage-level_**, and the **_network-level_**, and the design strategies are proved to be superior and feasible from theoretical analysis and experiments.

**How objective function affects the update of network weights**. At present, the main weight update method is the backpropagation algorithm, which uses partial differentiation to generate gradients, and then updates the weights by gradient decent. This algorithm propagates gradient information to the shallow layers in chain rule manner, and repeats such steps until the weights of all layers are updated. In other words, the information that an objective function teaches is propagated between layers in the form of gradients. In this paper, authors propose that by analyzing the gradient generated through the guidance of objective function, it is possible to design the network architecture by the gradient paths when executing the backpropagation process. Authors design the network architecture for three different levels of strategies such as **_layer-level_** design, **_stage-level_** design, and **_network-level_** design, which are described below:

1. **Layer-level**: At this level authors design gradient flow shunting strategies and use them to confirm the validity of the hypothesis. They adjust the number of layers and calculate the channel ratio of residual connection, and then design **Partial Residual Network (PRN)**
2. **Stage-level**: Authors add hardware characters to speed up inference on the network. They maximize gradient combinations while minimizing hardware computational cost, and thus design **Cross Stage Partial Network (CSPNet)**
3. **Network-level**: Authors add the consideration of gradient propagation efficiency to balance the leaning ability of the network. When they design the network architecture, they also consider the gradient propagation path length of the network as a whole, and therefore design **Efficient Layer Aggregation Network (ELAN)**

## Partial Residual Networks (PRN)

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5dff1f96-fcb9-4fb2-9d65-1968a98558f5" alt="elan_prn" height="150"/>
</p>

Partial Residual Network (PRN) was proposed by the same team in 2019, and its design concept belongs to the layer-level design strategy. In the design of PRN, the main concept is to **maximize the combination of gradients used to update the weights of each layer**. There are two main factors that affect the combination of gradients:

* the source layer of the gradient - the source layer is composed of the nodes connected the indegree edges of the gradient path
* the time it takes for the gradient flow to arrive at a particular layer from the loss layer through the operation of the chain rule

One thing to be noted is that when the gradient changes during the process of the chain rule update, the amount of loss information it covers will gradually fade as the chain grows. The above time duration is defined as the number of layers that the gradient flow needs to travel from the loss layer to a specific layer. In PRN, the following two structures are proposed to improve ResNet:

**Masked residual layer**. In the design of ResNet, the output of each computational block is added together with an identity connection, and such a structure is called residual layer. In PRN, identity connection is multiplied by a binary mask and only allow the feature map of some of the channels to be added to the output of the computational block. This structure is called _masked residual layer_, and its architecture is shown in figure above. Using the mechanism of a masked residual layer allows the feature map to be divided into two parts, in which the weights corresponding to the channels that are masked and the weights corresponding to the channels with identity connection will significantly increase the number of gradient combinations due to the aforementioned masking effect. In addition, differences in gradient sources will simultaneously affect the overall gradient timestamp (time node along time axis), thus making gradient combinations more abundant.

**Asymmetric residual layer**. Under the ResNet architecture, only feature map of the same size can be added, which is why it is a very restricted architecture. Generally, when the calculation amount and inference speed of the optimized architecture are performed, we are often limited by this architecture and cannot design an architecture that meets the requirements. Under the architecture of PRN, the masked residual layer proposed by authors can regard the inconsistency of the number of channels as some channels being blocked, and thus allow feature map with different number of channels to perform masked residual operations. The layer that operates in the above manner is called an _asymmetric residual layer_. An asymmetric residual layer is designed in such a way that the network architecture is more flexible and more able to maintain the properties of a gradient path-based model. For example, when feature integration is done, the general approach requires additional transition layers to project different feature maps to the same dimension, and then perform the addition operation. However, the above-mentioned operation will increase a large number of parameters and amount of computations, and will also make the gradient path longer, and thus affect the convergence of the network. The introduction of asymmetric residual layer can perfectly solve similar issues.

## Cross Stage Partial Networks

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/29adbd29-831a-4baa-81ca-21128cfe3698" alt="elan_csp" height="350"/>
</p>

CSPNet was proposed by the same team in 2019, and it is a stage-level gradient path-based network. Like PRN, CSP-Net is based on the concept of maximizing gradient combinations. The difference between CSPNet and PRN is that the latter focuses on confirming the improvement of network learning ability by gradient combination from theoretical perspective, while the former is additionally designed for further architecture optimization for hardware inference speed. Therefore, when designing CSPNet, authors extend the architecture from layer-level to stage-level, and optimize the overall architecture. CSPNet mainly has the following two structures:

**Cross stage partial operation**. From the perspective of maximizing the source of the gradient, we can easily find that the source of the gradient can be maximized when each channel has a different gradient path. Also, from the perspective of maximizing gradient timestamps, we know that the number of gradient timestamps can be maximized when each channel has computational blocks of different depths. Following the above concept, it is possible to derive an architecture designed to maximize both the gradient source and gradient timestamp. And this architecture will be the Inception-like architecture and the fractal-like architecture with depth-wise convolution. Although the above design can effectively improve the parameter utilization, it will greatly reduce the parallelization ability. In addition, it will cause the model to significantly reduce the inference speed on inference engines such as GPU and TPU. From the previous analysis, we know that dividing the channel can increase the number of gradient sources, and making the sub-networks connected by different channels with different layers can increase the number of gradient timestamps. The cross stage partial (CSP) operation can maximize the combination of gradients and increase the inference speed without breaking the architecture and can be parallelized. CSP is shown in figure above - authors divide a stage’s input feature map into two parts, and use this manner to increase the number of gradient sources. The detailed procedure is as follows:

1. First divide the input feature map into two parts
2. One of them passes through the computational block, and this computational block can be any computational block such as Res block, ResX block, or Dense block
3. The other part directly crosses the entire stage
4. Integrate both parts

Since only part of the feature map enters the computational black for operation, this kind of design can effectively reduce the amount of parameters, operation, memory traffic, and memory peak, allowing the system to achieve faster inference speed.

**Gradient flow truncate operation**. In order to make the designed network architecture more powerful, authors further analyze the gradient flow used to update the CSPNet. Since shortcut connections are often used in computational blocks, we know that the gradient sources that provide the two paths are bound to overlap a lot. We know that when a feature map passes through a kernel function, it is equivalent to a spatial projection. Usually we can insert a transition layer at the end of both paths to truncate the duplicated gradient flow. Through the above steps, we can make the information learned from the two paths and adjacent stages have more obvious diversity. Authors designed three different combinations of duplicate gradient flow truncate operations, as shown in figure below. These operations can be matched with different architectures, such as computational blocks and down-sampling blocks to achieve better results.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/43cdaee1-a375-4560-877c-1ae20477fa7a" alt="elan_csp_fusions" height="350"/>
</p>

## Efficient Layer Aggregation Networks

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/647c29ef-52df-44e3-ac78-86e37015641b" alt="elan_elan" height="350"/>
</p>

The codes of Efficient Layer Aggregation Networks (ELAN) was released by the same team in July 2022. It falls into the category of the gradient path designed network at the network-level. The main purpose of designing ELAN is to solve the problem that the convergence of the deep model will gradually deteriorate when executing model scaling. Authors analyze the shortest gradient path and the longest gradient path through each layer in the overall network, thereby designing a layer aggregation architecture with efficient gradient propagation paths. ELAN is mainly composed of VoVNet combined with CSPNet, and optimizes the gradient length of the overall network with the structure of **_stack in computational block_**

**Stack in computational block**. When we are doing model scaling, there will be a phenomenon, that is, when the network reaches a certain depth, if we continue to stack computational blocks, the accuracy gain will be less and less. To make matters worse, when the network reaches a certain critical depth, its convergence begins to deteriorate, resulting in an overall accuracy that is worse than shallow networks. One of the best examples is scaled- YOLOv4, we see that its P7 model uses expensive parameters and operations, but only a small amount of accuracy gain, and the same phenomenon occurs in many popular networks. For example, ResNet-152 is about three times as computationally intensive as ResNet-50, but offers less than 1% improvement in accuracy on ImageNet. When ResNet is stacked to 200 layers, its accuracy is even worse than ResNet-152. Also, when VoVNet is stacked to 99 layers, its accuracy is even much lower than that of VoVNet-39. From the gradient path design strategy point of view, authors speculate that the reason why the accuracy of VoVNet degenerates much faster than ResNet is because the stacking of VoVNet is based on the OSA module. We know that every OSA module contains a transition layer, so every time we stack an OSA module, the shortest gradient path of all layers in the network increases by one. As for ResNet, it is stacked by residual blocks, and the stacking of residual layers will only increase the longest gradient path, and will not increase the shortest gradient path. In order to verify the possible effects of model scaling, authors did some experiments based on YOLOR-CSP. From the experimental results they found that:

* when the stacking layer reaches 80+ layers, the accuracy of CSP fusion first starts to perform better than the normal CSPNet. At this point, the shortest gradient path of the computational block of each stage will be reduced by 1.
* As the network continues to widen and deepen, CSP fusion last will get the highest accuracy, but at this point the shortest gradient path of all layers will be reduced by 1.

The above experimental results confirmed previous hypothesis. With the support of the above experiments, authors designed the **_stack in computational block_** strategy in ELAN, as shown in figure above. The purpose of this design is to avoid the problem of using too many transition layers and making the shortest gradient path of the whole network quickly become longer. The above design strategy should allow ELAN to be successfully trained when the network is stacked deeper.
