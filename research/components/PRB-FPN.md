# PRB-FPN

2023 | [paper](https://arxiv.org/pdf/2012.01724) | _Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection_

This paper proposes the **Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN)** for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weakened as deeper backbones with more layers are used. In addition, it cannot keep up accurate detection of both small and large objects at the same time. To address these issues, authors propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. The following design improvements are provided:

1. A parallel bifusion FP structure with a **Bottom-up Fusion Module (BFM)** to detect both small and large objects at once with high accuracy
2. A **COncatenation and RE-organization (CORE)** module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps
3. The CORE feature is further purified to retain richer contextual information. Such CORE purification in both top-down and bottom-up pathways can be finished in only a few iterations
4. The adding of a residual design to CORE leads to a new **Re-CORE** module that enables easy training and integration with a wide range of deeper or lighter backbones

Authors propose a new Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) with a parallel design and multiple improvements that can retain both deeper and shallower features for fast and accurate single-shot object detection. Different from other bi-fusion FPN structures such as PANet, NAS-FPN, and BiFPN, authors create a parallel bi-fusion structure to fuse three-layers of feature maps in parallel to generate three prediction maps at the same time (see figure 1 below). Without losing efficiency, these three-way prediction maps can retain more accurate semantic and localization information to better detect both tiny and large objects. In this parallel structure, authors introduce a new concatenation and re-organization (CORE) module for data fusion, where output features can be further purified to retain contextual information. A "residual" design is introduced (motivated from the spirit of ResNet) into the bi-fusion pipeline, which enables easy training and integration with a number of popular backbones. The proposed residual FP design outperforms other bi-directional methods. In comparison, methods based on traditional FPs can only learn un-referenced features, thus they are not suitable for detecting both large and small objects. The proposed residual FP retains semantic richer features in higher layers that can better detect small objects. A key novelty in this design is the adding of parallelization to the bi-fusion FPN architecture. This parallel design is more effective in feature representation, i.e. for capturing features to identify and localize objects in either small or large sizes without losing efficiency. In comparison, most existing bi-directional FP methods directly concatenate large feature maps in a memory-consuming way, which ends up with an even larger feature map. The proposed PRB-FPN is simple, efficient, and suitable for generic object detection for multiple object classes and sizes (small, mid, and large). PRB-FPN approach is generalizable in combining with mainstream backbones including Pelee and DarkNet53. It can run in real-time and is easily deployable to edge devices.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5423b966-e7c4-4490-ae07-9f3dd3249cff" alt="prb_fpn_overview" height="300">
</p>
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ce607961-8f84-48c0-9d28-b0b4a25c8098" alt="prb_fpn_detailed" height="450">
</p>
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5e0810b3-f15a-4a43-a8e9-8327f66e50ae" alt="prb_fpn_re_core" height="400">
</p>

Figures above (1, 2, 3) depict the complete pipeline of our proposed network architecture. Details are provided in the following sessions.

## Parallel Concatenation and Re-organization Feature Bi-Fusion Architecture

Feature Pyramid (FP) is widely-used in top-down feature aggregation that can collect semantically rich features to effectively discriminate objects with scale invariance. However, it is well-known that FP cannot preserve accurate localization for small objects due to pooling and quantization. The winning methods of LPIRC 2019 challenge show improvements on detecting general-sized objects but not on small objects. Their object prediction was carried out using information from both each pyramid layer and the respective lower layers. This coincide with thoughts from several SoTA bi-directional methods in leveraging new feature streams from lower feature layers (or the raw image itself) to keep track of features from smaller objects and achieve more accurate localization. Such bi-fusion modules specially designed for improving small object detection still lack capabilities in detecting larger objects.

In this paper, authors proposed an effective parallel FP fusion design to tackle this difficult problem of object detection considering all object scales. This is done by creating **multiple bi-fusion paths** to keep tracks of features that are suitable to detect objects of all sizes (including tiny and large objects). Each bi-fusion path keeps track of size-dependent features to represent objects at a specific scale. Assume that there are $N$ prediction maps (where $N = 3$ for YOLOv4), authors propose to execute the $N$ different concurrent fusion paths to generate $N$ fused feature maps for the $N$ prediction maps. They use $n$ to index the bi-fusion modules (thus $n ≤ N$). Let $L$ be the level of the top layer in the FP. As shown in figure 1, the $n$-th bi-fusion module will bi-fuse feature maps from the $(L − n + 1)$-th layer to the $(L − n − N + 2)$-th layer in the backbone. The $s$-th output will be fed into the $s$-th prediction map for object detection, which will integrate feature maps from the $s$-th layer of all bi-fusion modules. Noticeably, the 1st bi-fusion module in this model corresponds to the sole bi-fusion module in SoTA bi-directional methods.

## Concatenation and Re-organization for Feature Bi-Fusion

In the figure 2, each bi-fusion module consists of three **concatenation and re-organization (CORE)** blocks and two skip connections. Details of the bi-fusion module are shown in figure 2a. The CORE design (shown in figure 2b) brings a major advantage that feature fusion can be recursively applied in both top-down and bottom-up fashions to:

* concatenate semantic features from top layers (top-down)
* re-organize spatially rich localization features from bottom layers (bottom-up)

To avoid using too many dithering operations (i.e., point-wise convolutions) and to avoid computationally expensive operations (i.e., pooling and addition), aurhors adopt an $1 × 1$ depth-wise convolution in the CORE module. This enables effective fusion of pathways coming from deeper and shallower layers in each layer of the FP. The $1 × 1$ depth-wise convolution in CORE is very different from most of SoTA bi-directional methods, where feature fusion is carried out by concatenating all feature maps. Their simple concatenations result in a large feature map proportional to the total feature size. In contrast, the proposed $1 × 1$ conv filter in CORE is automatically learned, such that features can be fused more effectively via a feature map of fixed size.

In each layer of the used backbone, CORE fuses features of each layer with its two adjacent (immediately shallower and deeper) layers. In other words, feature bi-fusion is performed in the feature pyramid of CORE. In the bottom-up fusion with the shallower layer, similar to YOLOv2, a **Re-Org** block from figure 3b is adopted in figure 2b to re-organize the feature map into 4 channels. However, instead of using a concatenation operation, the $1 × 1$ convolution filter is then performed to fuse all feature maps as the output.

## Residual Bi-Fusion Feature Pyramid

Authors further adopted the residual concept inspired from ResNet to the CORE block, and created a new **Residual CORE (Re-CORE)** block. Re-CORE enables the fusion of four adjacent scales (namely, the shallow, current, deep, and deeper layers) for better detection of small objects. Specifically, by recursively injecting the output of the $(i + 1)$-th CORE module to the $i$-th CORE module, the Bi-Fusion FP becomes a fully-featured Residual Bi-Fusion FPN as in figure 3a. Figure 2c depicts the connection between the Re-CORE and Convolution modules, in which $F_i$ and $∆F_i$ denote the outputs of the $i$-th Re-CORE and Convolution modules, respectively.

Figure 3a shows the detailed Re-CORE architecture. The Re-CORE module performs bi-fusion to integrate features from the four input layers with residual design. The output of the previous Re-CORE module becomes the input of the current Re-CORE module via a skip connection, which is depicted as a red line in figure 2c and figure 3a, respectively. Features from the $i$-th, $(i − 1)$-th, and $(i + 1)$-th layers are fused by an $1 × 1$ convolution and then added to an up-sampled version of the skip connection to produce a new feature map. This map is then fed into a convolution block to produce the final output of this Re-CORE module.

Working with popular backbones: Similar to ResNet, the residual nature of Re-CORE module enables easy training and integration of the FP with a wide range of backbones that works particularly well for small object detection. Instead of learning un-referenced features, Re-CORE obtains better accuracy from the largely increased feature depths when compared with traditional FPs. Note that SoTA FPs often learn redundant features and perform poorly on small object detection.

The Re-CORE module provides a new effective fusion approach for collecting localization information from bottom layers that can improve the accuracy of small object detection. In comparison, the naive approach in some other works detects small objects by generating high-resolution images as inputs to the detection module, which comes with a cost of large computational burden. Another approach for small object detection is to leverage contextual information, by sending semantic features from a top-down way via a FP as in YOLOv3. However, in these methods without the use of residual property, the learning will include un-referenced features and thus bound the number of FP layers that can actually contribute to object detection.

In summary, the residual design and bi-directional fusion make the Re-CORE module suitable for detecting small and even tiny objects without notable computation overheads.

## Bottom-up Feature Fusion

As aforementioned, the top winning method in LPIRC 2019 improved detection on large and medium-sized objects, but not able to keep up the performance for small objects. To address this issue, authors propose the adding of a **bottom-up fusion module (BFM)** to the PRB-FPN network to further improve the localization of both small and large objects. Figure 3c depicts the proposed BFM architecture. Instead of using convolution with stride 2 (adopted in PANet, Bi-FPN, or YOLOv4), the BFM adopts a **Re-Org** block to split $C$ channels of feature map into $4C$ channels to better preserve spatial information and generate robust semantic features via $1 × 1$ convolution, which improves small object detection. As for the bidirectional FPN-BPN work, convolutions with stride 2 are used for down-sampling, while de-convolutions are adopted for up-sampling. However, this design results in lower accuracy for small object detection due to the stride 2 operator, and the use of de-convolution leads to low efficiency in object detection.

In summary PRB-FPN design:

* Introduces Parallel Bi-fusion paths, that run concurrently for effective detection of both small and large objects
* Adds the Re-CORE module for improving the detection of small objects
* Adds BFM, that brings specific local information from a bottom-up pathway to localize the objects more accurately. The BFM pathway works particularly well for detecting both large and mid-sized objects.
