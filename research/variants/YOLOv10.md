# YOLOv10

2024 | [paper](https://arxiv.org/pdf/2405.14458v1) | _YOLOv10: Real-Time End-to-End Object Detection_

Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computa- tional cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the _Non-Maximum Suppression_ (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model’s capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, authors aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, authors first present the _consistent dual assignments_ for **NMS-free** training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, they introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. Authors comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome is a new generation of YOLO series for real-time end-to-end object detection, named **_YOLOv10_**. Extensive experiments show that YOLOv10 achieves the SoTA performance and efficiency across various model scales.

In recent years, researchers have concentrated on devising CNN-based object detectors to achieve real-time detection. Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency. The detection pipeline of YOLOs consists of two parts: the model forward process and the NMS post-processing. However, both of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\
Specifically, YOLOs usually employ one-to-many (_o2m_) label assignment strategy during training, whereby one ground-truth object corresponds to multiple positive samples. Despite yielding superior performance, this approach necessitates NMS to select the best positive prediction during inference. This slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS, thereby preventing YOLOs from achieving optimal end-to-end deployment.\
Furthermore, the model architecture design remains a fundamental challenge for YOLOs, which exhibits an important impact on the accuracy and speed. To achieve more efficient and effective model architectures, researchers have explored different design strategies. Various primary computational units are presented for the backbone to enhance the feature extraction ability, including _DarkNet_, _CSPNet_, _EfficientRep_ and _ELAN_ , etc. For the neck, _PAN_, _BiC_, _GD_ and _RepGFPN_, etc., are explored to enhance the multi-scale feature fusion. Besides, model scaling strategies  and re-parameterization techniques are also investigated. While these efforts have achieved notable advancements, a comprehensive inspection for various components in YOLOs from both the efficiency and accuracy perspectives is still lacking.

In this work, authors aim to address these issues and further advance the accuracy-speed boundaries of YOLOs. They target both the post-processing and the model architecture throughout the detection pipeline. To this end, this work first tackle the problem of redundant predictions in the post-processing by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label assignments and consistent matching metric. It allows the model to enjoy rich and harmonious supervision during training while eliminating the need for NMS during inference, leading to competitive performance with high efficiency. Secondly, authors propose the holistic efficiency-accuracy driven model design strategy for the model architecture by performing the comprehensive inspection for various components in YOLOs. For efficiency, they propose the **_lightweight classification head_**, **_spatial-channel decoupled downsampling_**, and **_rank-guided block design_**, to reduce the manifested computational redundancy and achieve more efficient architecture. For accuracy, they explore the **_large-kernel convolution_** and present the **_effective partial self-attention module_** to enhance the model capability, harnessing the potential for performance improvements under low cost.

## Consistent Dual Assignments for NMS-free Training

During training, YOLOs usually leverage Task-Alignment Learning (TAL) to allocate multiple positive samples for each instance. The adoption of one-to-many (_o2m_) assignment yields plentiful supervisory signals, facilitating the optimization and achieving superior performance. However, it necessitates YOLOs to rely on the NMS post-processing, which causes the suboptimal inference efficiency for deployment. While previous works explore one-to-one (_o2o_) matching to suppress the redundant predictions, they usually introduce additional inference overhead or yield suboptimal performance. In this work, authors present an NMS-free training strategy for YOLOs with dual label assignments and consistent matching metric, achieving both high efficiency and competitive performance.

### Dual label assignments


<p align="center">
  <img src="" alt="yolo_v10_dual_label" height="300"/>
</p>

Unlike one-to-many assignment, one-to-one matching assigns only one prediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak supervision, which causes suboptimal accuracy and convergence speed. Fortunately, this deficiency can be compensated by the one-to-many assignment. To achieve this, authors introduce dual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown in above figure, authors incorporate another one-to-one head for YOLOs. It retains the identical structure and adopts the same optimization objectives as the original one-to-many branch but leverages the one-to-one matching to obtain label assignments. During training, two heads are jointly optimized with the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-to-many assignment. During inference, YOLOv10 discards the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost. Besides, in the one-to-one matching, authors adopt the top one selection, which achieves the same performance as _Hungarian matching_ with less extra training time.

### Consistent matching metric

During assignments, both one-to-one and one-to-many approaches leverage a metric to quantitatively assess the level of concordance between predictions and instances. To achieve prediction aware matching for both branches, authors employ a uniform matching metric, i.e.

$$ m(α, β) = s · p^α · IoU(\hat{b}, b)^β $$

where $p$ is the classification score, $\hat{b}$ and $b$ denote the bounding box of prediction and instance, respectively. $s$ represents the spatial prior indicating whether the anchor point of prediction is within the instance. $α$ and $β$ are two important hyperparameters that balance the impact of the semantic prediction task and the location regression task. Authors denote the one-to-many and one-to-one metrics as $m_{o2m} = m(α_{o2m}, β_{o2m})$ and $m_{o2o} = m(α_{o2o}, β_{o2o})$, respectively. These metrics influence the label assignments and supervision information for the two heads.

In dual label assignments, the one-to-many branch provides much richer supervisory signals than one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that of one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many head’s optimization. As a result, the one-to-one head can provide improved quality of samples during inference, leading to better performance. To this end, authors first analyze the supervision gap between the two heads. Due to the randomness during training, they initiate the examination in the beginning with two heads initialized with the same values and producing the same predictions, i.e., one-to-one head and one-to-many head generate the same $p$ and IoU for each prediction-instance pair. Authors note that the regression targets of two branches do not conflict, as matched predictions share the same targets and unmatched predictions are ignored. The supervision gap thus lies in the different classification targets. Given an instance, we denote its largest IoU with predictions as $u^\∗$, and the largest one-to-many and one-to-one matching scores as $m_{o2m}^\∗$ and $m_{o2o}^\∗$, respectively. Suppose that one-to-many branch yields the positive samples $Ω$ and one-to-one branch selects $i$-th prediction with the metric $m_{o2o,i} = m_{o2o}^\∗$, we can then derive the classification target $t_{o2m,j} = u^\∗ · \frac{m_{o2m,j}}{m_{o2m}^\*} ≤ u^\∗$ for $j ∈ Ω$ and $t_{o2o,i} = u^\∗ · \frac{m_{o2o,i}^\*}{m_{o2o}^\*} = u^\∗$ for task aligned loss as in other works. The supervision gap between two branches can thus be derived by the _1-Wasserstein_ distance of different classification objectives, i.e.

$$ A = t_{o2o,i} − I (i ∈ Ω) t_{o2m,i} + \sum_{k ∈ Ω \ {i}} t_{o2m,k} $$

One can observe that the gap decreases as $t_{o2m,i}$ increases, i.e., $i$ ranks higher within $Ω$. It reaches the minimum when $t_{o2m,i} = u^\∗$, i.e., $i$ is the best positive sample in $Ω$, as shown in figure above (a). To achieve this, authors present the consistent matching metric, i.e., $α_{o2o} = r · α_{o2m}$ and $β_{o2o} = r · β_{o2m}$, which implies $m_{o2o} = m_{o2m}^r$. Therefore, the best positive sample for one-to-many head is also the best for one-to-one head. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, authors take $r = 1$, by default, i.e., $α_{o2o} = α_{o2m}$ and $β_{o2o} = β_{o2m}$. To verify the improved supervision alignment, authors count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results after training. As shown in figure above (b), the alignment is improved under the consistent matching metric.

## Holistic Efficiency-Accuracy Driven Model Design

<p align="center">
  <img src="" alt="yolo_v10_ranks_cib_psa" height="400"/>
</p>

In addition to the post-processing, the model architectures of YOLOs also pose great challenges to the efficiency-accuracy trade-offs. Although previous works explore various design strategies the comprehensive inspection for various components in YOLOs is still lacking. Consequently, the model architecture exhibits non-negligible computational redundancy and constrained capability, which impedes its potential for achieving high efficiency and performance. Here, authors aim to holistically perform model designs for YOLOs from both efficiency and accuracy perspectives.

### Efficiency driven model design

The components in YOLO consist of the stem, downsampling layers, stages with basic building blocks, and the head. The stem incurs few computational cost and authors thus perform efficiency driven model design for other three parts:

1. **Lightweight classification head** - The classification and regression heads usually share the same architecture in YOLOs. However, they exhibit notable disparities in computational overhead. For example, the FLOPs and parameter count of the classification head (5.95G/1.51M) are $2.5×$ and $2.4×$ those of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing the impact of classification error and the regression error, authors find that the regression head undertakes more significance for the performance of YOLOs. Consequently, one can reduce the overhead of classification head without worrying about hurting the performance greatly. Therefore, authors simply adopt a lightweight architecture for the classification head, which consists of two _depthwise separable convolutions_ with the kernel size of $3 × 3$ followed by a $1 × 1$ _convolution_.

2. **Spatial-channel decoupled downsampling** - YOLOs typically leverage regular $3 × 3$ standard convolutions with stride of 2, achieving spatial downsampling (from $H × W$ to $\frac{H}{2} × \frac{W}{2}$) and channel transformation (from $C$ to $2C$) simultaneously. This introduces non-negligible computational cost of $O(\frac{9}{2}HWC^2)$ and parameter count of $O(18C^2)$. Instead, authors propose to decouple the spatial reduction and channel increase operations, enabling more efficient downsampling. Specifically, they firstly leverage the _pointwise convolution_ to modulate the channel dimension and then utilize the _depthwise convolution_ to perform spatial downsampling. This reduces the computational cost to $O(2HWC^2 + \frac{9}{2}HWC)$ and the parameter count to $O(2C^2 + 18C)$. Meanwhile, it maximizes information retention during downsampling, leading to competitive performance with latency reduction.

3. **Rank-guided block design** - YOLOs usually employ the same basic building block for all stages, e.g., the bottleneck block in YOLOv8. To thoroughly examine such homogeneous design for YOLOs, authors utilize the intrinsic rank to analyze the redundancy of each stage. Specifically, they calculate the numerical rank of the last convolution in the last basic block in each stage, which counts the number of singular values larger than a threshold. Figure above (a) presents the results of YOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off. To tackle this, authors propose a **_rank-guided block design_** scheme which aims to decrease the complexity of stages that are shown to be redundant using compact architecture design. Authors first present a **_Compact Inverted Block (CIB)_** structure, which adopts the cheap _depthwise convolutions_ for spatial mixing and cost-effective _pointwise convolutions_ for channel mixing, as shown in figure above (b). It can serve as the efficient basic building block, e.g., embedded in the ELAN structure (figure above (b)). Then, authors advocate a rank-guided block allocation strategy to achieve the best efficiency while maintaining competitive capacity. Specifically, given a model, they sort its all stages based on their intrinsic ranks in ascending order and further inspect the performance variation of replacing the basic block in the leading stage with CIB. If there is no performance degradation compared with the given model, they proceed with the replacement of the next stage and halt the process otherwise. Consequently, one can implement adaptive compact block designs across stages and model scales, achieving higher efficiency without compromising performance.

### Accuracy driven model design

Authors further explore the **_large-kernel convolution_** and **_self-attention_** for accuracy driven design, aiming to boost the performance under minimal cost:

1. **Large-kernel convolution** - Employing _large-kernel depthwise convolution_ is an effective way to enlarge the receptive field and enhance the model’s capability. However, simply leveraging them in all stages may introduce contamination in shallow features used for detecting small objects, while also introducing significant I/O overhead and latency in high-resolution stages. Therefore, authors propose to leverage the _large-kernel depthwise convolutions_ in CIB within the deep stages. Specifically, they increase the kernel size of the second $3 × 3$ _depthwise convolution_ in the CIB to $7 × 7$, following. Additionally, they employ the structural reparameterization technique to bring another $3 × 3$ _depthwise convolution_ branch to alleviate the optimization issue without inference overhead. Furthermore, as the model size increases, its receptive field naturally expands, with the benefit of using large-kernel convolutions diminishing. Therefore, authors only adopt large-kernel convolution for small model scales.

2. **Partial self-attention (PSA)** - Self-attention is widely employed in various visual tasks due to its remarkable global modeling capability. However, it exhibits high computational complexity and memory footprint. To address this, in light of the prevalent attention head redundancy, authors present an efficient **_Partial Self-Attention (PSA)_** module design, as shown in figure above (c). Specifically, they evenly partition the features across channels into two parts after the $1 × 1$ convolution. Only one part is fed into the NPSA blocks comprised of _Multi-Head Self-Attention module (MHSA)_ and _Feed-Forward Network (FFN)_. Two parts are then concatenated and fused by a $1 × 1$ convolution. Besides, authors follow to assign the dimensions of the query and key to half of that of the value in MHSA and replace the _LayerNorm_ with _BatchNorm_ for fast inference. Furthermore, PSA is only placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the quadratic computational complexity of self-attention. In this way, the global representation learning ability can be incorporated into YOLOs with low computational costs, which well enhances the model’s capability and leads to improved performance.


