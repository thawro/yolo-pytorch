# YOLO papers
|                                                                                                                                                                                                                                                                                    Link | Year |                                           Title                                            |     Approach     | Features |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--: | :----------------------------------------------------------------------------------------: | :--------------: | :------- |
|                                                                                                                                                                                                                                           [paper](https://arxiv.org/pdf/1506.02640.pdf) | 2016 |                  You Only Look Once: Unified, Real-Time Object Detection                   |      YOLOv1      | TODO     |
|                                                                                                                                                                                                                                           [paper](https://arxiv.org/pdf/1612.08242.pdf) | 2016 |                             YOLO9000: Better, Faster, Stronger                             | YOLOv2, YOLO9000 | TODO     |
|                                             [paper](https://arxiv.org/pdf/1804.02767.pdf), [video](https://www.youtube.com/watch?v=Grir6TZbc1M), [blog](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/), [code](https://github.com/ultralytics/yolov3) | 2018 |                             YOLOv3: An Incremental Improvement                             |      YOLOv3      | TODO     |
| [paper](https://arxiv.org/pdf/2004.10934.pdf) , [blog](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [code_1](https://github.com/WongKinYiu/PyTorch_YOLOv4), [code_2](https://github.com/Tianxiaomo/pytorch-YOLOv4) | 2020 |                   YOLOv4: Optimal Speed and Accuracy of Object Detection                   |      YOLOv4      | TODO     |
|                                                                                                                                                                                                                                           [code](https://github.com/ultralytics/yolov5) | 2020 |                                           YOLOv5                                           |      YOLOv5      |          |
|                                                [paper](https://arxiv.org/pdf/2011.08036.pdf) , [blog](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982), [code](https://github.com/WongKinYiu/ScaledYOLOv4) | 2021 |                     Scaled-YOLOv4: Scaling Cross Stage Partial Network                     |  Scaled-YOLOv4   | TODO     |
|                                                                                                                                                                                                  [paper](https://arxiv.org/pdf/2105.04206), [code](https://github.com/WongKinYiu/yolor) | 2021 |           You Only Learn One Representation: Unified Network for Multiple Tasks            |      YOLOR       |          |
|                                                                                                                                                                                                                                           [paper](https://arxiv.org/pdf/2107.08430.pdf) | 2021 |                            YOLOX: Exceeding YOLO Series in 2021                            |      YOLOX       | TODO     |
|                                                                                                                                                                                             [paper](https://arxiv.org/pdf/2207.02696.pdf), [code](https://github.com/WongKinYiu/yolov7) | 2022 | YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors |      YOLOv7      | TODO     |
|                                                                                                                                                                                                    [paper](https://arxiv.org/abs/2209.02976), [code](https://github.com/meituan/YOLOv6) | 2022 |       YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications        |      YOLOv6      | TODO     |
|                                                                                                                                                                                                                                           [paper](https://arxiv.org/pdf/2301.05586.pdf) | 2023 |                            YOLOv6 v3.0: A Full-Scale Reloading                             |   YOLOv6 v3.0    | TODO     |
|                                                                                                                                                                                                                                               [paper](https://arxiv.org/pdf/2304.05552) | 2023 |              DynamicDet: A Unified Dynamic Architecture for Object Detection               |  Dynamic YOLOv7  |          |
|                                                                                                                                                                                                                                           [paper](https://arxiv.org/pdf/2304.00501.pdf) | 2023 |                   A comprehensive review of YOLO: from YOLOv1 and beyond                   |      Review      | TODO     |
|                                                                                                                                                                                                                                      [code](https://github.com/ultralytics/ultralytics) | 2023 |                                           YOLOv8                                           |      YOLOv8      |          |
|                                                                                                                                                                                                 [paper](https://arxiv.org/pdf/2402.13616), [code](https://github.com/WongKinYiu/yolov9) | 2024 |      YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information       |      YOLOv9      | TODO     |

# Used in YOLO papers:
|                                                             Link | Year |                                                 Title                                                  |        Approach        | Features | Used in |
| ---------------------------------------------------------------: | :--: | :----------------------------------------------------------------------------------------------------: | :--------------------: | :------- | ------- |
|                     [paper](https://arxiv.org/pdf/1406.4729.pdf) | 2015 |             Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition              |          SPP           |          | v4      |
|                    [paper](https://arxiv.org/pdf/1612.03144.pdf) | 2017 |                             Feature Pyramid Networks for Object Detection                              |          FPN           |          | v4      |
|                    [paper](https://arxiv.org/pdf/1704.04503.pdf) | 2017 |                            Improving Object Detection With One Line of Code                            |        Soft NMS        |          | v4      |
|                    [paper](https://arxiv.org/pdf/1608.03983.pdf) | 2017 |                          SGDR: Stochastic Gradient Descent with Warm Restarts                          |    Cosine Annealing    |          | v4      |
|                    [paper](https://arxiv.org/pdf/1810.12890.pdf) | 2018 |                     DropBlock: A regularization method for convolutional networks                      |       DropBlock        |          | v4      |
|                         [paper](http://arxiv.org/pdf/1710.09412) | 2018 |                               MixUp: Beyond Empirical Risk Minimization                                |         MixUp          |          | X       |
|                    [paper](https://arxiv.org/pdf/1807.06521.pdf) | 2018 |                               CBAM: Convolutional Block Attention Module                               |       CBAM / SAM       |          | v4      |
|                  [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | 2018 |                                 Focal Loss for Dense Object Detection                                  | Focal Loss / RetinaNet |          | v4      |
|                    [paper](https://arxiv.org/pdf/1911.08287.pdf) | 2018 |               Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression                |          DIoU          |          | v4      |
| [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | 2019 |                   Mish: A Self Regularized Non-Monotonic Neural Activation Function                    |          Mish          |          | v4      |
|                  [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | 2018 |                           Path Aggregation Network for Instance Segmentation                           |          PAN           | TODO     | v6      |
|                    [paper](https://arxiv.org/pdf/1911.11929.pdf) | 2019 |                   CSPNet: A new backbone that can enhance learning capability of CNN                   |          CSP           | TODO     | v6, v9  |
|                        [paper](https://arxiv.org/pdf/1912.02424) | 2020 | Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection |          ATSS          |          | v6.3    |
|                  [paper](https://arxiv.org/pdf/1905.11946v5.pdf) | 2020 |                EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks                |      EfficientNet      |          | v4      |
|                        [paper](https://arxiv.org/pdf/1911.09070) | 2020 |                         EfficientDet: Scalable and Efficient Object Detection                          |      EfficientDet      |          | v6.3    |
|                    [paper](https://arxiv.org/pdf/2101.03697.pdf) | 2021 |                             RepVGG: Making VGG-style ConvNets Great Again                              |         RepVGG         | TODO     | v6      |
|                        [paper](https://arxiv.org/pdf/2211.04800) | 2022 |                   Designing Network Design Strategies Through Gradient Path Analysis                   |          ELAN          | TODO     | v9      |
|                        [paper](https://arxiv.org/pdf/2012.01724) | 2023 |     Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection      |        PRB-FPN         |          | v6.3    |

---


# Components

## **SPP**
2015 | [paper](https://arxiv.org/pdf/1406.4729.pdf) | _Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition_
TODO

## **FPN**
2017 | [paper](https://arxiv.org/pdf/1612.03144.pdf) | _Feature Pyramid Networks for Object Detection_
TODO

## **Soft NMS**
2017 | [paper](https://arxiv.org/pdf/1704.04503.pdf) | _Improving Object Detection With One Line of Code_
TODO

## **Cosine Annealing**
2017 | [paper](https://arxiv.org/pdf/1608.03983.pdf) | _SGDR: Stochastic Gradient Descent with Warm Restarts_
TODO

## **DropBlock**
2018 | [paper](https://arxiv.org/pdf/1810.12890.pdf) | _DropBlock: A regularization method for convolutional networks_
TODO

## **MixUp**
2018 | [paper](http://arxiv.org/pdf/1710.09412) | _MixUp: Beyond Empirical Risk Minimization_
TODO

## **CBAM**
2018 | [paper](https://arxiv.org/pdf/1807.06521.pdf) | _CBAM: Convolutional Block Attention Module_
TODO

## **Focal Loss / RetinaNet**
2018 | [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | _Focal Loss for Dense Object Detection_
TODO

## **DIoU**
2018 | [paper](https://arxiv.org/pdf/1911.08287.pdf) | _Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression_
TODO

## **PAN**
2018 | [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | _Path Aggregation Network for Instance Segmentation_
TODO

## **Mish**
2019 | [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | _Mish: A Self Regularized Non-Monotonic Neural Activation Function_
TODO

## **CSP**
2019 | [paper](https://arxiv.org/pdf/1911.11929.pdf) | _CSPNet: A new backbone that can enhance learning capability of CNN_
TODO

## **ATSS**
2020 | [paper](https://arxiv.org/pdf/1912.02424) | _Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection_
TODO

## **EfficientNet**
2020 | [paper](https://arxiv.org/pdf/1905.11946v5.pdf) | _EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks_
TODO

## **EfficientDet**
2020 | [paper](https://arxiv.org/pdf/1911.09070) | _EfficientDet: Scalable and Efficient Object Detection_
TODO

## **RepVGG**
2021 | [paper](https://arxiv.org/pdf/2101.03697.pdf) | _RepVGG: Making VGG-style ConvNets Great Again_
TODO

## **ELAN**
2022 | [paper](https://arxiv.org/pdf/2211.04800) | _Designing Network Design Strategies Through Gradient Path Analysis_
TODO

## **PRB-FPN**
2023 | [paper](https://arxiv.org/pdf/2012.01724) | _Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection_
TODO

---

# YOLO

## **YOLO v1**
2016 | [paper](https://arxiv.org/pdf/1506.02640.pdf) | _You Only Look Once: Unified, Real-Time Object Detection_

YOLOv1 is a **single-stage** object detection model. Object detection is framed as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/596ae49e-22d6-4a11-85da-d89a50df9a3b" alt="yolo_v1" height="250"/>
</p>

### How it works:
* YOLO divides the input image into an S × S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.
* Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how
confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. The confidence score is defined as _Pr(Object) ∗ IoU(pred, gt)_. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.
* Each bounding box consists of 5 predictions: _x_, _y_, _w_, _h_, and confidence. The _(x, y)_ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IoU between the predicted box and any ground truth box.
* Each grid cell also predicts _C_ conditional class probabilities, _Pr(Class_i|Object)_. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes _B_.
* predictions are encoded as an _S × S × (B ∗ 5 + C)_ tensor (_S_ - grid size, _B_ - number of bboxes, _C_ - number of classes). In paper: 7 × 7 × 30, that is _S = 7_, _B = 2_, _C = 20_ (PASCAL VOC has 20 labels)
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2304ce9a-56e2-450e-be12-6eb5294b561d" alt="yolo_how" width="500"/>
</p>

### Model architecture:
* The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.
* The network architecture is inspired by the GoogLeNet. It has 24 convolutional layers followed by 2 fully connected layers
* Instead of the inception modules used by GoogLeNet, the _1 × 1_ conv reduction layers followed by _3 × 3_ convolutional layers are used
* To avoid overfitting use dropout and extensive data augmentation. A dropout layer with _rate = 0.5_ after the first connected layer prevents co-adaptation between layers

### Loss function
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/a29f820b-b37f-472e-b5ea-52456b4383eb" alt="yolo_loss" width="500"/>
</p>

### Training details:
* pretrain first 20 conv layers (+ global pool and fc) as classifier on the ImageNet -> val top-5 88% accuracy
* add 4 conv layers and 2 fc layers on top of the pretrained backbone and increase input size from 224 to 448 to train the detector
* normalize bbox width and height to _[0, 1]_ and parametrize bbox x and y to be offsets of a particular grid cell location so they are also bounded in _[0, 1]_
* LeakyReLU(0.1)
* use different loss weights for case of no object cell and for coordinate losses
* use squared root of width and height in loss (to reflect that small deviations in large boxes matter less than in small boxes)
* YOLO predicts multiple bounding boxes per grid cell. At training time only one bbox predictor
is responsible for each object. One predictor is assigned to be “responsible” for predicting an object based on which prediction has the highest current IoU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall
* The loss function only penalizes classification error if an object is present in that grid cell. It also only penalizes bbox coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IoU of any predictor in that grid cell)
* Training scheme:
	* 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012 (when testing on 2012 we also include the VOC 2007 test data for training)
	* batch size = 64
	* momentum = 0.9 
	* weight decay = 0.0005
	* Learning rate schedule:
		* For the first epochs slowly raise the learning rate from 0.001 to 0.01 (If started at a high learning rate the model often diverges due to unstable gradients)
		* Continue training with 0.01 for 75 epochs
		* Then 0.001 for 30 epochs
		* Finally 0.0001 for 30 epochs
	* data augmentation:
		* random scaling,
		* translations of up to 20% of the original image size
		* randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space

## **YOLO v2**
2016 | [paper](https://arxiv.org/pdf/1612.08242.pdf) | _YOLO9000: Better, Faster, Stronger_

YOLOv2 (or YOLO9000) is a single-stage real-time object detection model. It improves upon YOLOv1 in several ways, including the use of Darknet-19 as a backbone, batch normalization, use of a high-resolution classifier, multi-scale training, the use of dimension clusters, fine-grained features and direct location prediction to predict bounding boxes, and more

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3956c34f-f205-4366-9cbb-83f2308689d6" alt="yolo_v2" height="250"/>
</p>

### How it works:
It works similar to the YOLOv1, the main differences include each predicted bbox has its own C class probabilities, so the predictions are encoded as an _S × S × (B ∗ (5 + C))_ tensor and added passtrough layer, dimension clusters prior and direct location prediction for easier training.
Main improvements:
* **Batch Normalization** - leads to significant improvements in convergence while eliminating the need for other forms of regularization (removed dropout from fc). By adding batch normalization on all of the convolutional layers in YOLO mAP improved by more than 2%
* **High Resolution Classifier** - the original YOLO trains the classifier network at _224 × 224_ and increases the resolution to 448 for detection. This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution. For YOLOv2 we first fine tune the classification network at the full _448 × 448_ resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better
on higher resolution input. Then finetune the resulting network on detection. This high resolution classification network gives an increase of almost 4% mAP
* **Dimension Clusters** - authors encountered two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. If better priors for anchor boxes are picked for the network to start with it can make it easier to learn to predict good detections. Instead of choosing priors by hand, _k-means_ clustering is used on the training set bounding boxes to automatically find good priors. Distance metric is based on IoU, that is: _d(box, centroid) = 1 − IoU(box, centroid)_. By analysis, the _k = 5_ value is chosen for the number of priors.
* **Direct location prediction** - Instead of predicting offsets (for anchors) the similar to YOLOv1 approach is used, that is predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall in _[0, 1]_. The Logistic activation function is used to constrain the network’s predictions to fall in this range. The network predicts 5 bboxes at each cell in the output feature map. The network predicts 5 coordinates for each bbox, _tx_, _ty_ , _tw_, _th_, and to. If the cell is offset from the top left corner of the image by _(cx, cy )_ and the bbox prior has width and height _pw_, _ph_, then the predictions correspond to:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/848f9316-b3a4-49db-b8bc-e6be7acbeeeb" alt="yolov2_bbox_pic" height="200"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ca70fb43-1cfd-4b93-baf3-4c64b0c707cf" alt="yolov2_bbox_def" height="200"/>
</p>

* **Fine-Grained Features** - YOLOv2 predicts detections on a _13 × 13_ feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. The passthrough layer is added that brings features from an earlier layer at _26 × 26_ resolution. The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the _26 × 26 × 512_ feature map into a _13 × 13 × 2048_ feature map, which can be concatenated with the original features
* **Multi-Scale Training**. The original YOLO uses an input resolution of _448 × 448_. With the addition of anchor boxes the resolution is changed to _416 × 416_. However, since the model only uses convolutional and pooling layers it can be resized on the fly. The YOLOv2 is designed to be robust to running on images of different sizes. Instead of fixing the input image size, it is changed every few iterations. Every 10 batches a new image dimension size is chosen from multiples of 32 (model downsamples by a factor of 32): {320, 352, ..., 608}  Thus the smallest option is _320 × 320_ and the largest is _608 × 608_.

### Model architecture:
**Darknet-19** - a new classification model to be used as the backbone of YOLOv2. Similar to the VGG models, Darknet-19 use mostly _3 × 3_ filters and double the number of channels after every pooling step. Following the work on Network in Network (NIN) the global average pooling to is used make predictions as well as _1 × 1_ filters to compress the feature representation between _3 × 3_ convolutions. Batch normalization is used to stabilize training, speed up convergence, and regularize the model. The final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling It achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet.

### Training details:
**Training for classification** - the network is trained on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent (SGD) with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. During training the standard data augmentation tricks are used, including random crops, rotations, and hue, saturation, and exposure shifts. As discussed above, after initial training on images at _224 × 224_ the model is finetuned at a larger size, _448_ for 10 epochs starting at learning rate of 0.001. After finetuning the model achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.

**Training for detection** - for detection the last convolutional layer is removed and instead three _3 × 3_ convolutional layers are added (each with 1024 filters) and a final _1 × 1_ convolutional layer with the number of outputs needed for detection (for VOC 5 boxes are predicted, each with 5 coordinated and 20 classes, so 125 filters in total). The passtrough layer is also added from the final _3 × 3 × 512_ layer to the second to last convolutional layer so that the model can use fine grain features. The network is trained for 160 epochs with a starting
learning rate of 0.001, dividing it by 10 at 60 and 90 epochs. The weight decay is 0.0005, momentum is 0.9 and data augmentation includes random crops, color shifting, and other similar to YOLO and SSD

## **YOLO v3**
2018 | [paper](https://arxiv.org/pdf/1804.02767.pdf) | _YOLOv3: An Incremental Improvement_
YOLOv3 is a real-time, single-stage object detection model that builds on YOLOv2 with several improvements. Improvements include the use of a new backbone network, Darknet-53 that utilises residual connections, as well as some improvements to the bbox prediction step, and use of three different scales from which to extract features. Ultralytics implementation can be found [here](https://github.com/ultralytics/yolov3).

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c86504c6-7712-4ceb-a358-279ae53ebb0e" alt="yolo_v3" height="300"/>
</p>

### How it works:
* Similar to YOLOv2, the model outputs a tensor of size _S × S × [B ∗ (5 + C)]_, but now the model outputs bboxes at three different scales
* Different to previous versions, YOLOv3 uses multiple independent logistic classifiers rather than one softmax layer for each class. During training, they use binary cross-entropy loss in a one vs all setup (using a softmax imposes the assumption that each box has exactly one class which is often not the case - the multilabel approach better models the data)
* The bigger backbone is used (DarkNet-53) for feature extraction - the architecture has alternative _1 × 1_ and _3 × 3_ convolution layers and skip/residual connections inspired by the ResNet model. Although DarkNet53 is smaller than ResNet101 or RestNet-152, it is faster and has equivalent or better performance
* The idea of FPN (Feature Pyramid Networks) is added to leverage the benefit from all the prior computations and fine-grained features early on in the network
* The bboxes anchors are defined using k-means (same as in YOLOv2), but YOLOv3 uses three prior boxes for different scales

### Model architecture:
* **Darknet-53 classifier** - The network is a hybrid approach between the network used in YOLOv2 (Darknet-19), and the residual networks. It uses successive _3 × 3_ and _1 × 1_ convolutional layers but now has some shortcut connections as well and is significantly larger (53 layers)

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/fb6d5b7d-9d6d-4c93-8fa1-314e76f96b2d" alt="darknet_53" height="400"/>
</p>

* **Detector** - From the base feature extractor several convolutional layers are added:
	* The last of these predicts a _3-d_ tensor encoding bounding box, objectness, and class predictions. For COCO the predictions include 3 boxes at each scale so the tensor is _S × S × [3 ∗ (4 + 1 + 80)]_ for the 4 bbox offsets, 1 objectness prediction, and 80 class predictions
 	* Next the feature map from 2 layers previous is taken and is upsampled by 2x. The feature map from earlier point in the network is also taken and it is merged with the upsampled features using concatenation. This method allows to get more meaningful semantic information from the upsampled features and finer-grained information from the earlier feature map. On top of that a few more convolutional layers are added to process this combined feature map, and eventually predict a similar tensor (_S × S × [3 ∗ (4 + 1 + 80)]_), although now twice the size.
  	* The same design if performed one more time to predict boxes for the final scale. Thus the predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.

### Training details:
The network is trained similar to YOLOv2


## **YOLO v4**
2020 | [paper](https://arxiv.org/pdf/2004.10934.pdf) | _YOLOv4: Optimal Speed and Accuracy of Object Detection_
TODO


## **YOLOv5**
2020 | [code](https://github.com/ultralytics/yolov5)[docs](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/) | _YOLOv5_

YOLOv5 is built on top of the work of [YOLOv3](#yolo-v3) and YOLOv4. All the YOLOv5 models are composed of the same 3 components: _CSP-Darknet53_ as a backbone, _SPP_ and _PANet_ in the model neck and the head used in YOLOv4.

### How it works:


### Model architecture:
![yolo_v5](https://github.com/thawro/yolo-pytorch/assets/50373360/c7660fce-8a0c-41b5-a429-1ca77d0f597c)

### Training details:


## **Scaled YOLO v4**
2021 | [paper](https://arxiv.org/pdf/2011.08036.pdf) | _Scaled-YOLOv4: Scaling Cross Stage Partial Network_
TODO

## **YOLO R**
2021 | [paper](https://arxiv.org/pdf/2105.04206) | _You Only Learn One Representation: Unified Network for Multiple Tasks_
TODO

## **YOLO X**
2021 | [paper](https://arxiv.org/pdf/2107.08430.pdf) | _YOLOX: Exceeding YOLO Series in 2021_
TODO

## **YOLO v6**
2022 | [paper](https://arxiv.org/abs/2209.02976) | _YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications_
TODO

## **YOLO v7**
2022 | [paper](https://arxiv.org/pdf/2207.02696.pdf) | _YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors_
TODO

## **YOLO v6.3**
2023 | [paper](https://arxiv.org/pdf/2301.05586.pdf) | _YOLOv6 v3.0: A Full-Scale Reloading_
TODO

## **Dynamic YOLO** 
2023 | [paper](https://arxiv.org/pdf/2304.05552) | _DynamicDet: A Unified Dynamic Architecture for Object Detection_
TODO

## **Review**
2023 | [paper](https://arxiv.org/pdf/2304.00501.pdf) | _A comprehensive review of YOLO: from YOLOv1 and beyond_
TODO

## **YOLOv8**
2023 | [paper](https://github.com/ultralytics/ultralytics) | _YOLOv8_
TODO

## **YOLO v9**
2023 | [paper](https://arxiv.org/pdf/2402.13616) | _YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information_
TODO



