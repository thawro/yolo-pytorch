# YOLO papers
| Year | Approach | Link | Title |
| :--- | :--------------- | :---- | :---- |
| 2016 | YOLOv1 | [paper](https://arxiv.org/pdf/1506.02640.pdf) | You Only Look Once: Unified, Real-Time Object Detection |
| 2016 | YOLOv2, YOLO9000 | [paper](https://arxiv.org/pdf/1612.08242.pdf) | YOLO9000: Better, Faster, Stronger |
| 2018 | YOLOv3 | [paper](https://arxiv.org/pdf/1804.02767.pdf), [video](https://www.youtube.com/watch?v=Grir6TZbc1M), [blog](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/), [code](https://github.com/ultralytics/yolov3) | YOLOv3: An Incremental Improvement |
| 2019 | Gaussian YOLO | [paper](https://arxiv.org/pdf/1904.04620) | Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving |
| 2020 | PP-YOLO | [paper](https://arxiv.org/pdf/1904.04620) | PP-YOLO: An Effective and Efficient Implementation of Object Detector |
| 2020 | YOLOv4 | [paper](https://arxiv.org/pdf/2004.10934.pdf) , [blog](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [code_1](https://github.com/WongKinYiu/PyTorch_YOLOv4), [code_2](https://github.com/Tianxiaomo/pytorch-YOLOv4) | YOLOv4: Optimal Speed and Accuracy of Object Detection |
| 2020 | YOLOv5 | [code](https://github.com/ultralytics/yolov5) | YOLOv5 |
| 2021 | Scaled-YOLOv4 | [paper](https://arxiv.org/pdf/2011.08036.pdf) , [blog](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982), [code](https://github.com/WongKinYiu/ScaledYOLOv4) | Scaled-YOLOv4: Scaling Cross Stage Partial Network |
| 2021 | YOLOR | [paper](https://arxiv.org/pdf/2105.04206), [code](https://github.com/WongKinYiu/yolor) | You Only Learn One Representation: Unified Network for Multiple Tasks |
| 2021 | YOLOX | [paper](https://arxiv.org/pdf/2107.08430.pdf) | YOLOX: Exceeding YOLO Series in 2021 |
| 2022 | YOLOv7 | [paper](https://arxiv.org/pdf/2207.02696.pdf), [code](https://github.com/WongKinYiu/yolov7) | YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors |
| 2022 | YOLOv6 | [paper](https://arxiv.org/abs/2209.02976), [code](https://github.com/meituan/YOLOv6) | YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications |
| 2023 | YOLOv6 v3.0 | [paper](https://arxiv.org/pdf/2301.05586.pdf) | YOLOv6 v3.0: A Full-Scale Reloading |
| 2023 | Dynamic YOLOv7 | [paper](https://arxiv.org/pdf/2304.05552) | DynamicDet: A Unified Dynamic Architecture for Object Detection |
| 2023 | Review | [paper](https://arxiv.org/pdf/2304.00501.pdf) | A comprehensive review of YOLO: from YOLOv1 and beyond |
| 2023 | YOLOv8 | [code](https://github.com/ultralytics/ultralytics) | YOLOv8 |
| 2024 | YOLOv9 | [paper](https://arxiv.org/pdf/2402.13616), [code](https://github.com/WongKinYiu/yolov9) | YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information |

# Used in YOLO papers:
| Year | Approach | Link | Used in | Title |
| :--- | :---- | :--- | :--- | :---- |
| 2015 | SPP | [paper](https://arxiv.org/pdf/1406.4729.pdf) | v4 | Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition |
| 2017 | ResNext | [paper](https://arxiv.org/pdf/1611.05431) | TODO | Aggregated Residual Transformations for Deep Neural Networks |
| 2017 | FPN | [paper](https://arxiv.org/pdf/1612.03144.pdf) | v4 | Feature Pyramid Networks for Object Detection |
| 2017 | Soft NMS | [paper](https://arxiv.org/pdf/1704.04503.pdf) | v4 | Improving Object Detection With One Line of Code |
| 2017 | Cosine Annealing | [paper](https://arxiv.org/pdf/1608.03983.pdf) | v4 | SGDR: Stochastic Gradient Descent with Warm Restarts |
| 2018 | DropBlock | [paper](https://arxiv.org/pdf/1810.12890.pdf) | v4 | DropBlock: A regularization method for convolutional networks |
| 2018 | MixUp | [paper](http://arxiv.org/pdf/1710.09412) | X | MixUp: Beyond Empirical Risk Minimization |
| 2018 | Focal Loss / RetinaNet | [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | v4 | Focal Loss for Dense Object Detection |
| 2018 | DIoU | [paper](https://arxiv.org/pdf/1911.08287.pdf) | v4 | Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression |
| 2019 | Mish | [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | v4 | Mish: A Self Regularized Non-Monotonic Neural Activation Function |
| 2018 | PAN | [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | v6 | Path Aggregation Network for Instance Segmentation |
| 2019 | CSP | [paper](https://arxiv.org/pdf/1911.11929.pdf) | v6, v9  | CSPNet: A new backbone that can enhance learning capability of CNN |
| 2020 | ATSS | [paper](https://arxiv.org/pdf/1912.02424) | v6.3 | Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection |
| 2020 | EfficientDet | [paper](https://arxiv.org/pdf/1911.09070) | v6.3 | EfficientDet: Scalable and Efficient Object Detection |
| 2021 | RepVGG | [paper](https://arxiv.org/pdf/2101.03697.pdf) | v6 | RepVGG: Making VGG-style ConvNets Great Again |
| 2022 | ELAN | [paper](https://arxiv.org/pdf/2211.04800) | v9 | Designing Network Design Strategies Through Gradient Path Analysis |
| 2023 | PRB-FPN | [paper](https://arxiv.org/pdf/2012.01724) | v6.3 | Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection |


# Recommended papers before diving into object detection
| Year | Approach | Link | Title |
| :--- | :----- | :--- | :--- |
| 2014 | GoogLeNet/InceptionV1 | [paper](https://arxiv.org/pdf/1409.4842) | Going deeper with convolutions |
| 2015 | InceptionV2/V3 | [paper](https://arxiv.org/pdf/1512.00567) | Rethinking the Inception Architecture for Computer Vision |
| 2015 | ResNet | [paper](https://arxiv.org/pdf/1512.03385) | Deep Residual Learning for Image Recognition |
| 2016 | InceptionV4 | [paper](https://arxiv.org/pdf/1602.07261) | Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning  |
| 2016 | SqueezeNet | [paper](https://arxiv.org/pdf/1602.07360) | SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size |
| 2017 | WideResNet | [paper](https://arxiv.org/pdf/1605.07146v4) | Wide Residual Networks |
| 2017 | ResNext | [paper](https://arxiv.org/pdf/1611.05431) | Aggregated Residual Transformations for Deep Neural Networks |
| 2017 | MobileNetV1 | [paper](https://arxiv.org/pdf/1704.04861) | MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications |
| 2018 | CBAM / SAM | [paper](https://arxiv.org/pdf/1807.06521.pdf) | CBAM: Convolutional Block Attention Module |
| 2018 | DenseNet | [paper](https://arxiv.org/pdf/1608.06993v5) | Densely Connected Convolutional Networks |
| 2019 | MobileNetV2 | [paper](https://arxiv.org/pdf/1801.04381) | MobileNetV2: Inverted Residuals and Linear Bottlenecks |
| 2019 | MobileNetV3 | [paper](https://arxiv.org/pdf/1905.02244) | Searching for MobileNetV3 |
| 2019 | SE | [paper](https://arxiv.org/pdf/1709.01507) | Squeeze-and-Excitation Networks |
| 2020 | EfficientNet | [paper](https://arxiv.org/pdf/1905.11946v5.pdf) | EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks |

---

# Object detectors

## Object detection pipelines

### One-stage 

Examples: YOLO, SSD, RetinaNet, CenterNet

### Two-stage 

Examples: RCNN, FasterRCNN

During inference, an object detection network performs a sequence of convolution operations on an image using a deep convolutional neural network (CNN). The network bifurcates into two branches at a layer L — one branch generates region proposals while the other performs classification and regression by pooling convolutional features inside regions of interest (RoIs) generated by the proposal network. The proposal network generates classification scores and regression offsets for anchor boxes of multiple scales and aspect ratios placed at each pixel in the convolutional feature map. It then ranks these anchor boxes and selects the top K (≈ 6000) anchors to which the bounding box regression offsets are added to obtain image level co-ordinates for each anchor. Greedy non-maximum suppression is applied to top K anchors which eventually generates region proposals.

The classification network generates classification and regression scores for each proposal generated by the proposal network. Since there is no constraint in the network which forces it to generate a unique RoI for an object, multiple proposals may correspond to the same object. Hence, other than the first correct bounding-box, all other boxes on the same object would generate false positives. To alleviate this problem, non-maximum-suppression is performed on detection boxes of each class independently, with a specified overlap threshold. Since the number of detections is typically small and can be further reduced by pruning detections which fall below a very small threshold, applying non-maximum suppression at this stage is not computationally expensive. We present an alternative approach to this non-maximum suppression algorithm in the object detection pipeline. An overview of the object detection pipeline is shown below

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6785054f-050d-441e-b15a-f17996e9c3d3" alt="two_stage_det" height="350"/>
</p>

## Bag of Freebies (BoF) and Bag of Specials (BoS)
Object detectors are constantly upgraded by addressing two main concepts, that is the Bag of Freebies (**BoF**) and Bag of Specials (**BoS**). 

### Bag of Freebies
Methods that only change the training strategy or only increase the training cost. Usually, a conventional object detector is trained offline. Therefore, researchers always like to take this advantage and develop better training methods which can make the object detector receive better accuracy without increasing the inference cost. The possible BoF include:
* **Data augmentation**
	* **photometric distortions** - adjust the brightness, contrast, hue, saturation and noise of an image
	* **geometric distortions** - add random scaling, cropping, flipping and rotating 
	* [**random erase**](100), [CutOut](https://arxiv.org/pdf/1708.04552) - randomly select the rectangle region in an image and fill a random or zero value
	* [**Hide and Seek**](https://arxiv.org/pdf/1811.02545), [Grid Mask](https://arxiv.org/pdf/2001.04086) - randomly or evenly select multiple rectangle regions in an image and replace with zeros
	* [**DropOut**](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), [DropConnect](http://proceedings.mlr.press/v28/wan13.pdf), [DropBlock](https://arxiv.org/pdf/1810.12890) - randomly or evenly select multiple rectangle regions in a feature map and replace with zeros
	* [**MixUp**](http://arxiv.org/pdf/1710.09412) - multiply and superimpose two images with different coefficients ratios and adjust the label with these superimposed ratios
	* [**CutMix**](https://arxiv.org/pdf/1905.04899) - cover the cropped image to rectangle region of other images and adjust the label according to the size of the mix area 
	* [**style transfer GAN**](https://arxiv.org/pdf/1811.12231) - use GAN to change style of the image (helps to reduce the texture bias learned by CNN)
* **Data imbalance** - Solving the problem of biased dataset semantic distribution (problem of data imbalance between different classes)
	* [**hard negative example mining**](https://ieeexplore.ieee.org/document/655648)
	* **Online Hard Example Mining** ([OHEM](https://arxiv.org/pdf/1604.03540)) - bootstrapping technique that modifies SGD to sample from examples in a non-uniform way depending on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution that favors diverse, high loss instances
 	* [**Focal loss**](https://arxiv.org/pdf/1708.02002) - reshaping the standard cross entropy loss such that it downweights the loss assigned to well classified examples
* **Target labels** - express the relationship of the degree of association between different categories with the one-hot hard representation:
	* [**Label smoothing**](https://arxiv.org/pdf/1512.00567) - convert hard label into soft label for training, which can make model more robust
 	* [**Label smoothing with refinement network**](https://arxiv.org/pdf/1703.00551) - via knowledge distillation
* **Objective function** of Bounding Box regression
	* MSE - directly perform regression on the center point coordinates and height and width of the bbox
 	* [**IoU loss**](https://arxiv.org/pdf/1608.01471) - puts the coverage of predicted bbox area and ground truth BBox area into consideration. The IoU loss computing process will trigger the calculation of the four coordinate points of the bbox by executing IoU with the ground truth, and then connecting the generated results into a whole code. Because IoU is a scale invariant representation, it can solve the problem that when traditional methods calculate the l1 or l2 loss of {x, y, w, h} (MSE treat these points as independent variables), the loss will increase with the scale.
	* **General IoU Loss** ([GIoU loss](https://arxiv.org/pdf/1902.09630)) - include the shape and orientation of object in addition to the coverage area. GIoU proposes to find the smallest area BBox that can simultaneously cover the predicted bbox and ground truth bbox, and use this bbox as the denominator to replace the denominator originally used in IoU loss
 	* **Distance IoU Loss** ([DIoU loss](https://arxiv.org/pdf/1911.08287)) - additionally considers the distance of the center of an object
  	* **Complete IoU Loss** ([CIoU loss](https://arxiv.org/pdf/1911.08287)) - simultaneously considers the overlapping area, the distance between center points, and the aspect ratio. CIoU can achieve better convergence speed and accuracy on the bbox regression problem
 
### Bag of Specials
Those plugin modules and post-processing methods that only increase the inference cost by a small amount but can significantly improve the accuracy of object detection. Generally speaking, these plugin modules are for enhancing certain attributes in a model, such as enlarging receptive field, introducing attention mechanism, or strengthening feature integration capability, etc. The post-processing is a method for screening model prediction results. The possible BoS include:
* **Enhance receptive field**
	* **Spatial Pyramid Pooling** ([SPP](25)) - originated from Spatial Pyramid Matching ([SPM](39)). SPMs original method was to split feature map into several d × d equal blocks, where d can be {1, 2, 3, ...}, thus forming spatial pyramid, and then extracting bag-of-word features. SPP integrates SPM into CNN and use max-pooling operation instead of bag-of-word operation. Since the SPP module will output one dimensional feature vector, it is infeasible to be applied in Fully Convolutional Network (FCN). Thus in the design of [YOLOv3](63), Redmon and Farhadi improved SPP module to the concatenation of max-pooling outputs with kernel size k × k, where k = {1, 5, 9, 13}, and stride equals to 1. Under this design, a relatively large k × k max-pooling effectively increase the receptive field of backbone feature
	* **Atrous Spatial Pyramid Pooling** ([ASPP](5)) - exploits multi-scale features by employing multiple dilated convolutions with kernel size = 3 × 3, dilatet ratio = k, stride = 1
	* **Receptive Field Block** ([RFB](47)) - use several dilated convolutions of k × k kernel, dilated ratio equals to k, and stride equals to 1 to obtain a more comprehensive spatial coverage than ASPP. RFB makes use of multi-branch pooling with varying kernels corresponding to RFs of different sizes, applies dilated convolution layers to control their eccentricities, and reshapes them to generate final representation
* **Attention Module**
	* **Squeeze-and-Excitation** ([SE](https://arxiv.org/pdf/1709.01507)) - architectural unit designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature recalibration. Spatial dimensions of block input feature maps are squeezed into a single numeric value using average pooling, then a dense layer (with ReLU) adds non-linearity and output channel complexity is reduced by a ratio and another dense layer (with sigmoid) gives each channel a smooth gating (weight, attention) function to finally weight each input feature map of the block based on using the attention weights. 
	* **Spatial Attention Module** ([SAM](https://arxiv.org/pdf/1807.06521v2)) - A Spatial Attention Module (introduced in [CBAM](#cbam)) is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the channel attention, the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a convolution layer to generate a spatial attention map which encodes where to emphasize or suppress.
* **Feature Integration**
	* **Feature Pyramid Networks** ([FPN](https://arxiv.org/pdf/1612.03144)) -  feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection. The construction of the pyramid involves a bottom-up pathway and a top-down pathway. The **bottom-up** pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For ResNets we use the feature activations output by each stage’s last residual block. The **top-down** pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.
	* **Path Aggregation Networks** ([PAN](https://arxiv.org/pdf/1803.01534v4)) - aims to boost information flow in a proposal-based instance segmentation framework. Specifically, the feature hierarchy is enhanced with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. Additionally, adaptive feature pooling is employed, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.
	* **Scale-wise Feature Aggregation** Module ([SFAM](https://arxiv.org/pdf/1811.04533v3)) - use SE module to execute channel wise level re-weighting on multi-scale concatenated feature maps
	* **Adaptively Spatial Feature Fusion** ([ASFF](https://arxiv.org/pdf/1911.09516v2)) - learns the way to spatially filter conflictive information to suppress inconsistency across different feature scales, thus improving the scale-invariance of features. ASFF enables the network to directly learn how to spatially filter features at other levels so that only useful information is kept for combination. For the features at a certain level, features of other levels are first integrated and resized into the same resolution and then trained to find the optimal fusion. At each spatial location, features at different levels are fused adaptively, i.e., some features may be filter out as they carry contradictory information at this location and some may dominate with more discriminative clues. ASFF offers several advantages: (1) as the operation of searching the optimal fusion is differential, it can be conveniently learned in back-propagation; (2) it is agnostic to the backbone model and it is applied to single-shot detectors that have a feature pyramid structure; and (3) its implementation is simple and the increased computational cost is marginal.
	* **Weighted Bi-directional Feature Pyramid Networks** ([BiFPN](https://arxiv.org/pdf/1911.09070v7)) - type of feature pyramid network which allows easy and fast multi-scale feature fusion. It incorporates the multi-level feature fusion idea from FPN, PANet and NAS-FPN that enables information to flow in both the top-down and bottom-up directions, while using regular and efficient connections. It also utilizes a fast normalized fusion technique. Traditional approaches usually treat all features input to the FPN equally, even those with different resolutions. However, input features at different resolutions often have unequal contributions to the output features. Thus, the BiFPN adds an additional weight for each input feature allowing the network to learn the importance of each. All regular convolutions are also replaced with less expensive depthwise separable convolutions. Comparing with PANet, PANet added an extra bottom-up path for information flow at the expense of more computational cost. Whereas BiFPN optimizes these cross-scale connections by removing nodes with a single input edge, adding an extra edge from the original input to output node if they are on the same level, and treating each bidirectional path as one feature network layer (repeating it several times for more high-level future fusion).
* **Activation function**
	* **Rectified Linear Unit** ([ReLU](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)) - solve the gradient vanish problem which is frequently encountered in traditional tanh and sigmoid activation function
	* **Leaky ReLU** ([LReLU](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)) and Parametric ReLU ([PReLU](https://arxiv.org/pdf/1502.01852)) - solve the problem that the gradient of ReLU is zero when the output is less than zero
	* [**ReLU6**](https://arxiv.org/pdf/1704.04861) and [**hard-Swish**](https://arxiv.org/pdf/1905.02244) - specially designed for quantization networks
	* Scaled Exponential Linear Unit ([SELU](https://arxiv.org/pdf/1706.02515v5)) - for self-normalizing a neural network
	* [**Swish**](https://arxiv.org/pdf/1710.05941v2) and [**Mish**](https://arxiv.org/pdf/1908.08681) - continuously differentiable activation functions
* **Post-Processing**
	* **Non Maximum Supression** (NMS / [Greedy NMS](https://arxiv.org/pdf/1311.2524)) - Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores (Greedy NMS). The detection box **M** with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with **M** are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. 
	* [**soft NMS**](https://arxiv.org/pdf/1704.04503v2) - it considers the problem that the occlusion of an object may cause the degradation of confidence score in greedy NMS with IoU score. Soft-NMS solves classic NMS problem by decaying the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process.
	* [**DIoU NMS**](https://arxiv.org/pdf/1911.08287v1) - added the information of the center point distance to the bbox screening process on the basis of soft NMS. In original NMS, the IoU metric is used to suppress the redundant detection boxes, where the overlap area is the unique factor, often yielding false suppression for the cases with occlusion. With DIoU-NMS, we not only consider the overlap area but also central point distance between two boxes.


# Components

## SPP
2015 | [paper](https://arxiv.org/pdf/1406.4729.pdf) | _Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition_

Authors of the paper introduced a Spatial Pyramid Pooling (SPP) layer to remove the fixed-size constraint of the network. Specifically, added an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed length outputs, which are then fed into the fullyconnected layers (or other classifiers). In other words, we perform some information “aggregation” at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20e4130d-8b7d-40be-9e03-3de729989654" alt="SPP" height="350"/>
</p>

Spatial pyramid pooling (also knows as spatial pyramid matching or SPM), as an extension of the Bag-of-Words (BoW) model, is one of the most successful methods in computer vision. It partitions the image into divisions from finer to coarser levels, and aggregates local features in them. SPP has long been a key component in the leading and competition-winning systems for classification and detection  before the prevalence of CNNs. Nevertheless, SPP has not been considered in the context of CNNs before this paper. Authors noted that SPP has several remarkable properties for deep CNNs: 

*  SPP is able to generate a fixed-length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot
* SPP uses multi-level spatial bins, while the sliding window pooling uses only a single window size. Multi-level pooling has been shown to be robust to object deformations
* SPP can pool features extracted at variable scales thanks to the flexibility of input scales. Through experiments authors have shown that all these factors elevate the recognition accuracy of deep networks.

SPP-net not only makes it possible to generate representations from arbitrarily sized images/windows for testing, but also allows us to feed images with varying sizes or scales during training. Training with variable-size images increases scale-invariance and reduces over-fitting. 

## FPN
2017 | [paper](https://arxiv.org/pdf/1612.03144.pdf) | _Feature Pyramid Networks for Object Detection_

Authors of the paper exploited the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. 

Pyramid approaches comparison:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/4ee4eeee-c9a4-4873-8dce-150ab73c7262" alt="pyramids_comparison" height="350"/>
</p>

A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi- scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition

FPN is a feature pyramid that has strong semantics at all scales and is built quickly from a single input image scale. It combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections.

FPN takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. The construction of the pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, as introduced in the following:

FPN compared to [similar](https://arxiv.org/pdf/1603.08695) approach:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8b5553e8-d1f0-4a4e-afab-91cd51e6b079" alt="FPN" height="350"/>
</p>



* **Bottom-up pathway** - the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. There are often many layers producing output maps of the same size - these layers are in the same network stage. For the feature pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is chosen as the reference set of feature maps, which will be enriched to create the pyramid. This choice is natural since the deepest layer of each stage should have the strongest features. Specifically, for ResNets the feature activations output by each stage’s last residual block are used. The output of these last residual blocks are denoted as _{C2, C3, C4, C5}_ for _conv2_, _conv3_, _conv4_, and _conv5_ outputs, with corresponding strides of _{4, 8, 16, 32}_ pixels with respect to the input image. The _conv1_ is not included into the pyramid due to its large memory footprint.

* **Top-down pathway and lateral connections** - it hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times. The building block shown below constructs the top-down feature maps.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c20d0886-7ea0-4e47-89ef-3655d6e8e5df" alt="FPN_block" height="350"/>
</p>

With a coarser-resolution feature map, the spatial resolution is upsampled by a factor of 2 (using nearest neighbor upsampling for simplicity). The upsampled map is then merged with the corresponding bottom-up map (which undergoes a _1 × 1_ convolutional layer to reduce channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated. Iteration is started by simply attaching a _1×1_ convolutional layer on _C5_ to produce the coarsest resolution map. Finally, a _3×3_ convolution is appended on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. This final set of feature maps is called _{P2, P3, P4, P5}_, corresponding to _{C2, C3, C4, C5}_ that are respectively of the same spatial sizes. Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, the feature dimension (numbers of channels, denoted as _d_) is fixed in all the feature maps. _d = 256_ in this paper and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which has been empirically found to have minor impacts. The more sophisticated blocks (e.g., using multi-layer residual blocks as the connections) have been tested and observed marginally better results.
* 

## Soft NMS
2017 | [paper](https://arxiv.org/pdf/1704.04503.pdf) | _Improving Object Detection With One Line of Code_

Non-maximum suppression (NMS) is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box _M_ with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with _M_ are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. 

Authors proposed an improved version of NMS, that is a _Soft-NMS_, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with _M_. Hence, no object is eliminated in this process.

During the analysis of old NMS, authors wanted the improved metric to take the following conditions into account:
* Score of neighboring detections should be decreased to an extent that they have a smaller likelihood of increasing the false positive rate, while being above obvious false positives in the ranked list of detections.
* Removing neighboring detections altogether with a low NMS threshold would be sub-optimal and would increase the miss-rate when evaluation is performed at high overlap thresholds.
* Average precision measured over a range of overlap thresholds would drop when a high NMS threshold is used

It would be ideal if the penalty function was continuous, otherwise it could lead to abrupt changes to the ranked list of detections. A continuous penalty function should have no penalty when there is no overlap and very high penalty at a high overlap. Also, when the overlap is low, it should increase the penalty gradually, as _M_ should not affect the scores of boxes which have a very low overlap with it. However, when overlap of a box _bi_ with _M_ becomes close to one, _bi_ should be significantly penalized. Taking this into consideration, authors proposed a Gaussian penalty function:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/24ac7cb8-9152-49b3-9127-b13b03f98bf7" alt="soft_NMS_eq" height="60"/>
</p>

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/db925ea6-ff65-4af1-9be3-023f1b4f5a0f" alt="soft_NMS_algo" height="350"/>
</p>


This update rule (top) is applied in each iteration and scores of all remaining detection boxes are updated


## Cosine Annealing
2017 | [paper](https://arxiv.org/pdf/1608.03983.pdf) | _SGDR: Stochastic Gradient Descent with Warm Restarts_

Cosine Annealing with warm restarts is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a "warm restart" in contrast to a "cold restart" where a new set of small random numbers may be used as a starting point.

Authors of the work considered one of the simplest warm restart approaches. They simulated a new warm-started run / restart of SGD once _Ti_ epochs are performed, where _i_ is the index of the run. Importantly, the restarts are not performed from scratch but emulated by increasing the learning rate _ηt_ while the old value of xt is used as an initial solution. The amount of this increase controls to which extent the previously acquired information (e.g., momentum) is used. Within the _i_-th run, the learning rate is decayed with a cosine annealing for each batch as follows:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/a9c1d5c7-2912-451c-bd63-5b0006052bd8" alt="cosine_annealing_eq" height="70"/>
</p>

where _ηi\_min_ and _ηi\_max_ are ranges for the learning rate, and _Tcur_ accounts for how many epochs have been performed since the last restart. Since _Tcur_ is updated at each batch iteration _t_, it can take discredited values such as _0.1_, _0.2_, etc. Thus, _ηt = _ηi\_max_ when _t = 0_ and _Tcur = 0_. Once _Tcur = Ti_, the cos function will output _−1_ and thus _ηt = _ηi\_min_. The decrease of the learning rate is shown below (left - without restart, right - with restart every ~2k iterations).

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2d2c4971-fd4a-4ab6-a6f8-bc068b60c624" alt="cosine_annealing" height="350"/>
</p>

## DropBlock
2018 | [paper](https://arxiv.org/pdf/1810.12890.pdf) | _DropBlock: A regularization method for convolutional networks_

Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers (in most cases it was used at the FC layers of the conv networks). This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. The main drawback of dropout is that it drops out features randomly. While this can be effective for fully connected layers, it is less effective for convolutional layers, where features are correlated spatially. When the features are correlated, even with dropout, information about the input can still be sent to the next layer, which causes the networks to overfit. This intuition suggests that a more structured form of dropout is needed to better regularize convolutional networks

DropBlock is a structured form of dropout, that is particularly effective to regularize convolutional networks. In DropBlock, features in a block, i.e., a contiguous region of a feature map, are dropped together. As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d8718292-fb1e-44f8-9103-4034e1b73ade" alt="drop_block" height="350"/>
</p>

DropBlock is inspired by [Cutout](https://arxiv.org/pdf/1708.04552), a data augmentation method where parts of the input examples are zeroed out. DropBlock generalizes Cutout by applying Cutout at every feature map in a convolutional networks. In our experiments, having a fixed zero-out ratio for DropBlock during training is not as robust as having an increasing schedule for the ratio during training. In other words, it’s better to set the DropBlock ratio to be small initially during training, and linearly increase it over time during training.

DropBlock is a simple method similar to dropout. Its main difference from dropout is that it drops contiguous regions from a feature map of a layer instead of dropping out independent random units. Pseudocode of DropBlock is shown below. 

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/eab7cb36-5aad-40bb-8b61-3d06a3e9744f" alt="drop_block_algo" height="450"/>
</p>

DropBlock has two main parameters which are _block_size_ and _γ_. _block_size_ is the size of the block to be dropped, and _γ_, controls how many activation units to drop. Authors experimented with a shared DropBlock mask across different feature channels or each feature channel with its DropBlock mask. The above Algorithm corresponds to the latter, which tends to work better in the experiments

Applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices


## MixUp
2018 | [paper](http://arxiv.org/pdf/1710.09412) | _MixUp: Beyond Empirical Risk Minimization_

Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. The paper introduces _**mixup**_, a simple learning principle to alleviate these issues. In essence, _mixup_ trains a neural network on convex combinations of pairs of examples and their labels. By doing so, _mixup_ regularizes the neural network to favor simple linear behavior in-between training examples

The contribution of _mixup_ paper is to propose a generic vicinal distribution:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/fcfb7d1e-8fc7-4a49-a48c-5652314015c8" alt="mixup_hard_eq" height="60"/>
</p>


where _λ ∼ Beta(α, α)_, for _α ∈ (0, ∞)_. In a nutshell, sampling from the _mixup_ vicinal distribution produces virtual feature-target vectors:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5992a2fd-34a6-4565-aa5c-9687e1dee702" alt="mixup" height="90"/>
</p>

where _(xi, yi)_ and _(xj , yj)_ are two feature-target vectors drawn at random from the training data, and _λ ∈ [0, 1]_. The mixup hyper-parameter α controls the strength of interpolation between feature-target pairs, recovering the ERM (Empirical Risk Minimization) principle as _α → 0_.

**What is mixup doing?** 

The mixup vicinal distribution can be understood as a form of data augmentation that encourages the model f to behave linearly in-between training examples. Authors of the paper argue that this linear behaviour reduces the amount of undesirable oscillations when predicting outside the training examples. Also, linearity is a good inductive bias from the perspective of Occam’s razor since it is one of the simplest possible behaviors. _mixup_ leads to decision boundaries that transition linearly from class to class, providing a smoother estimate of uncertainty. Experiments show, that the models trained with mixup are more stable in terms of model predictions and gradient norms in-between training samples.

Mixup example:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6f5f48d7-74cd-4da0-b953-191a12521b0e" alt="mixup_example" height="350"/>
</p>


## CBAM
2018 | [paper](https://arxiv.org/pdf/1807.06521.pdf) | _CBAM: Convolutional Block Attention Module_

Convolutional Block Attention Module (**CBAM**) is a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, CBAM sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/055969ff-3ef1-43bc-9d19-1da921f87680" alt="CBAM_overview" height="300"/>
</p>

Given an intermediate feature map $F$ (shape: $C × H × W$) as input, CBAM sequentially infers a $1D$ channel attention map $M_c$ (shape: $C × 1 × 1$) and a $2D$ spatial attention map $M_s$ (shape: $1 × H × W$) as illustrated above. The overall attention process can be summarized as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/503f9b0d-ed52-4094-bc7e-6988e1765968" alt="CBAM_overview_eq" height="80"/>
</p>

where $⊗$ denotes element-wise multiplication. During multiplication, the attention values are broadcasted (copied) accordingly: channel attention values are broadcasted along the spatial dimension, and vice versa. $F^{′′}$ is the final refined output. The figure below depicts the computation process of each attention map. The following describes the details of each attention module.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/12a9a4ed-4484-4207-99f0-c826c8b87253" alt="CBAM_detail" height="400"/>
</p>

**Channel attention module** - channel attention map is produced by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on **‘what’** is meaningful given an input image. The spatial dimension of the input feature map is squeezed to compute the channel attention efficiently. For aggregating spatial information, _average-pooling_ has been commonly adopted so far. Authors argued that _max-pooling_ gathers another important clue about distinctive object features to infer finer channel-wise attention. Thus, they used both average-pooled and max-pooled features simultaneously and empirically confirmed that exploiting both features greatly improves representation power of networks rather than using each independently.

The first step is to aggregate spatial information of a feature map by using both _average-pooling_ and _max-pooling_ operations, generating two different spatial context descriptors: _Fc{avg}_ and _Fc{max}_, which denote average-pooled features and max-pooled features respectively. Both descriptors are then forwarded to a shared network to produce the channel attention map $M_c$ (shape $C × 1 × 1$). The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $C/r × 1 × 1$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, the output feature vectors are merged using element-wise summation. In short, the channel attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/605f2500-ef28-4e3b-a0e8-b63fc4f5fdcc" alt="CBAM_channel_att" height="80"/>
</p>

where $σ$ denotes the sigmoid function, $W_0$ has shape $C/r × C$, and $W_1$ has shape $C × C/r$. Note that the MLP weights, $W_0$ and $W_1$, are shared for both inputs and the ReLU activation function is followed by $W_0$.


**Spatial attention module** - the spatial attention map is generated by utilizing the inter-spatial relationship of features. Different from the channel attention, the spatial attention focuses on **‘where’** is an informative part, which is complementary to the channel attention. To compute the spatial attention, the first step is to apply _average-pooling_ and _max-pooling_ operations along the channel axis and concatenate them to generate an efficient feature descriptor. Applying pooling operations along the channel axis is shown to be effective in highlighting informative regions. On the concatenated feature descriptor, the convolution layer is  applied to generate a spatial attention map $M_s(F)$ (shape $H × W$) which encodes where to emphasize or suppress.

The channel information of a feature map is aggregated by using two pooling operations, generating two 2D maps: $F^s_{avg}$ (shape $1 × H × W$) and $F^s_{max}$ (shape $1 × H × W$). Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the $2D$ spatial attention map. In short, the spatial attention is computed as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/20b00305-c0af-42ea-9d5b-a449f2f7d825" alt="CBAM_spatial_att" height="80"/>
</p>

where _σ_ denotes the sigmoid function and $f$ $7 × 7$ represents a convolution operation with the filter size of $7 × 7$.

**Arrangement of attention modules** - given an input image, two attention modules, channel and spatial, compute complementary attention, focusing on **‘what’** and **‘where’** respectively. Considering this, two modules can be placed in a parallel or sequential manner. Authours have found that the sequential arrangement gives a better result than a parallel arrangement. For the arrangement of the sequential process, the experimental result shows that the channel-first order is slightly better than the spatial-first

Example of CBAM integrated with a ResBlock in ResNet:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/5f286642-9e32-4823-8b21-2d8debe9c269" alt="resblock_CBAM" height="270"/>
</p>


## Focal Loss / RetinaNet
2018 | [paper](https://arxiv.org/pdf/1708.02002v2.pdf) | _Focal Loss for Dense Object Detection_

In this paper, the authors addressed the challenge of low accuracy in one-stage object detectors compared to two-stage detectors. They identify the imbalance between foreground and background classes during training as the main issue. To mitigate this, they introduce Focal Loss, which prioritizes hard examples, leading to their proposed detector, RetinaNet, achieving both the speed of traditional one-stage detectors and surpassing the accuracy of all existing two-stage detectors

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/17c08773-46e7-46c2-8ce4-724c6498e899" alt="focal_loss_fig" height="270"/>
</p>

In R-CNN-like detectors, class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g., Selective Search, EdgeBoxes, DeepMask, RPN) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k). In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM), are performed to maintain a manageable balance between foreground and background

In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating ∼100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping or hard example mining.

Focal Loss proposed in this paper acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases (see above). Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that the proposed Focal Loss enables to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous SoTA techniques for training one-stage detectors. To demonstrate the effectiveness of the proposed focal loss, authors designed a simple one-stage object detector called _RetinaNet_, named for its dense sampling of object locations in an input image. Its design features an efficient in-network feature pyramid and use of anchor boxes. It draws on a variety of recent ideas from [SSD, DeepMultiBox, RPN, FPN].

The Focal Loss is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000).

More formally, authors added a modulating factor _(1 − pt)^γ_ to the cross entropy loss, with tunable focusing parameter _γ ≥ 0_ and defined the Focal Loss as:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2c67cbcc-6837-4135-8e66-0bf1a28aec7a" alt="focal_loss_eq" height="60"/>
</p>

The focal loss is visualized for several values of _γ ∈ [0, 5]_ in figure above. Authors noted two properties of the focal loss:
* When an example is misclassified and _pt_ is small, the modulating factor is near 1 and the loss is unaffected. As _pt → 1_, the factor goes to _0_ and the loss for well-classified examples is down-weighted
* The focusing parameter _γ_ smoothly adjusts the rate at which easy examples are down-weighted. When _γ = 0_, _FL_ is equivalent to _CE_, and as _γ_ is increased the effect of the modulating factor is likewise increased (found _γ = 2_ to work best in experiments). Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with _γ = 2_, an example classified with _pt = 0.9_ would have _100×_ lower loss compared with _CE_ and with _pt ≈ 0.968_ it would have _1000×_ lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most _4×_ for _pt ≤ .5_ and _γ = 2_). In practice authors used an _α-balanced_ variant of the focal loss:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/372b592a-7464-40ac-96d0-1004801da292" alt="focal_loss_alpha_eq" height="60"/>
</p>

and adopted this form in the experiments as it yields slightly improved accuracy over the _non-α-balanced_ form. Finally, they note that the implementation of the loss layer combines the sigmoid operation for computing _p_ with the loss computation, resulting in greater numerical stability.

Another huge contribution of the paper is the RetinaNet architecture shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/0b044e91-ea60-4801-a6be-d74f8bc999b5" alt="retina_net" height="400"/>
</p>


## DIoU
2018 | [paper](https://arxiv.org/pdf/1911.08287.pdf) | _Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression_

Intersection over Union (IoU) is the most popular metric for object detection:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/cdb085ad-22ab-4f06-8bea-433f4ae96177" alt="iou" height="60"/>
</p>

where $B_{gt} = (x^{gt}, y^{gt}, w^{gt}, h^{gt})$ is the ground-truth, and
$B = (x, y, w, h)$ is the predicted box. Conventionally, $l_n$-norm (e.g., n = 1 or 2) loss is adopted on the coordinates of $B$ and $B_{gt}$ to measure the distance between bounding boxes. However, $l_n$-norm loss is not a suitable choice to obtain the optimal IoU metric. In earlier works, the IoU loss was suggested to be adopted for improving the IoU metric:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/eab21c19-7365-4626-adc2-755f5cf0087c" alt="iou_loss" height="60"/>
</p>

However, IoU loss only works when the bounding boxes have overlap, and would not provide any moving gradient for non-overlapping cases. And then generalized IoU loss
(GIoU) is proposed by adding a penalty term:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8435920f-e666-41c8-8eac-5c933a11178b" alt="giou_loss" height="60"/>
</p>

where $C$ is the smallest box covering $B$ and $B_{gt}$. Due to the introduction of penalty term, the predicted box will move towards the target box in non-overlapping cases. Although GIoU can relieve the gradient vanishing problem for non-overlapping cases, it still has several limitations. As shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/51db5636-3187-4b90-8e68-2dfb450a23e0" alt="giou_vs_diou" height="300"/>
</p>


 one can see that GIoU loss intends to increase the size of predicted box at first, making it have overlap with target box, and then the IoU term in GIoU equation will work to maximize the overlap area of bounding box. And from figure below:
 
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/9298cee5-1c55-4549-956d-8b9899a73803" alt="iou_vs_giou_vs_diou" height="400"/>
</p>
 
GIoU loss will totally degrade to IoU loss for enclosing bounding boxes. Due to heavily relying on the IoU term, GIoU empirically needs more iterations to converge, especially for horizontal and vertical bounding boxes. Usually GIoU loss cannot well converge in the SoTA detection algorithms, yielding inaccurate detection. 


To sum up, IoU loss converges to bad solutions for nonoverlapping cases, while GIoU loss is with slow convergence especially for the boxes at horizontal and vertical orientations. And when incorporating into object detection pipeline, both IoU and GIoU losses cannot guarantee the accuracy of regression


**Distance-IoU (DIoU)**
In this paper, authors proposed a Distance-IoU (DIoU) loss for bounding box regression. In particular, they simply added a penalty term on IoU loss to directly minimize the normalized distance between central points of two bounding boxes, leading to much faster convergence than GIoU loss. From figure above it can be seen, that DIoU loss can be deployed to directly minimize the distance between two bounding boxes:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/1c60033c-eac0-481e-afed-f47936cc5286" alt="diou" height="60"/>
</p>

where $b$ and $b_{gt}$ denote the central points of $B$ and $B_{gt}$, $ρ(·)$ is the Euclidean distance, and c is the diagonal length of the smallest enclosing box covering the two boxes. 

**Comparison with IoU and GIoU losses** -  The proposed DIoU loss inherits some properties from IoU and GIoU loss:
1. DIoU loss is still invariant to the scale of regression problem.
2. Similar to GIoU loss, DIoU loss can provide moving directions for bounding boxes when non-overlapping with target box.
3. When two bounding boxes perfectly match, $L_{IoU} = L_{GIoU} = L_{DIoU} = 0$. When two boxes are far away, $L_{GIoU} = L_{DIoU} → 2$.

And DIoU loss has several merits over IoU loss and GIoU loss, which can be evaluated by simulation experiment.

**Complete IoU Loss (CIoU)**
A good loss for bounding box regression should consider three important geometric factors, i.e., overlap area, central point distance and aspect ratio. By uniting the coordinates, IoU loss considers the overlap area, and GIoU loss heavily relies on IoU loss. The proposed DIoU loss aims at considering simultaneously the overlap area and central point distance of bounding boxes. However, the consistency of aspect ratios for bounding boxes is also an important geometric factor. Therefore, based on DIoU loss, the CIoU loss is proposed by imposing the consistency of aspect ratio:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/1413b442-6736-4a7f-b8eb-df968287a0fd" alt="ciou_loss" height="60"/>
</p>

where $α$ is a positive trade-off parameter, and $v$ measures the consistency of aspect ratio:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/f38a6af0-6008-40cd-96da-eea27fe2e568" alt="ciou_alpha" height="60"/>
</p>
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2e8976f5-3792-4cd1-a73b-db5b96fdcc78" alt="ciou_v" height="60"/>
</p>


## PAN
2018 | [paper](https://arxiv.org/pdf/1803.01534v4.pdf) | _Path Aggregation Network for Instance Segmentation_

The way that information propagates in neural networks is of great importance. In this paper, authors proposed Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, they enhanced the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. The adaptive feature pooling was presented, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.

During the research authors notes that information propagation in SoTA Mask R-CNN can be further improved. Specifically, features in low levels are helpful for large instance identification. But there is a long path from low-level structure to topmost features, increasing difficulty to access accurate localization information. Further, each proposal is predicted based on feature grids pooled from one feature level, which is assigned heuristically. This process can be updated since information discarded in other levels may be helpful for final prediction. Finally, mask prediction is made on a single view, losing the chance to gather more diverse information.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/affbff14-7d90-4515-812d-1c6965e9fd82" alt="PANet" height="350"/>
</p>

Inspired by these principles and observations, wthey proposed PANet, illustrated above, for instance segmentation with followint properties:
* To shorten information path and enhance feature pyramid with accurate localization signals existing in low-levels, bottom-up path augmentation is created. In fact, features in low-layers were utilized in the earlier works, but propagating low-level features to enhance entire feature hierarchy for instance recognition was not explored
* Second, to recover broken information path between each proposal and all feature levels, they develop adaptive feature pooling. It is a simple component to aggregate features from all feature levels for each proposal, avoiding arbitrarily assigned results. With this operation, cleaner paths are created.
* Finally, to capture different views of each proposal, they augmented mask prediction with tiny fully-connected (fc) layers, which possess complementary properties to FCN originally used by Mask R-CNN. By fusing predictions from these two views, information diversity increases and masks with better quality are produced. 

The first two components are shared by both object detection and instance segmentation, leading to much enhanced performance of both tasks

### Framework

The framework is illustrated in figure above. Path augmentation and aggregation is conducted for improving performance. A bottom-up path is augmented to make low-layer information easier to propagate. adaptive feature pooling is designed to allow each proposal to access information from all levels for prediction. A complementary path is added to the mask-prediction branch. This new structure leads to decent performance. Similar to FPN, the improvement is independent of the CNN structure.

#### Bottom-up Path Augmentation

**Motivation** - The insightful point from earlier works is that neurons in high layers strongly respond to entire objects while other neurons are more likely to be activated by local texture and patterns manifests the necessity of augmenting a top-down path to propagate semantically strong features and enhance all features with reasonable classification capability in FPN. PANet framework further enhances the localization capability of the entire feature hierarchy by propagating strong responses of low-level patterns based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances. PANet path is built with clean lateral connections from the low level to top ones. Therefore, there is a “shortcut” (dashed green line in figure above), which consists of less than 10 layers, across these levels. In comparison, the CNN trunk in FPN gives a long path (dashed red line) passing through even 100+ layers from low layers to the topmost one. 

**Augmented Bottom-up Structure** - PANet framework first accomplishes bottom-up path augmentation. Authors followed FPN to define that layers producing feature maps with the same spatial size are in the same network stage. Each feature level corresponds to one stage. ResNet is used as the basic structure and ${P_2, P_3, P_4, P_5}$ denote feature levels generated by FPN. The augmented path starts from the lowest level $P_2$ and gradually approaches $P_5$ as shown in figure above. From $P_2$ to $P_5$, the spatial size is gradually down-sampled with factor 2. Authors use ${N_2, N_3, N_4, N_5}$ to denote newly generated feature maps corresponding to ${P_2, P_3, P_4, P_5}$. Note that $N_2$ is simply $P_2$, without any processing. The building block of PFANet is shown below:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ec075dfe-c9db-4999-8d1a-80c6e1287281" alt="PANet_block" height="350"/>
</p>

Each building block takes a higher resolution feature map $N_i$ and a coarser map $P_{i+1}$ through lateral connection and generates the new feature map $N_{i+1}$. Each feature map $N_i$ first goes through a $3 × 3$ convolutional layer with stride 2 to reduce the spatial size. Then each element of feature map $P_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 × 3$ convolutional layer to generate $N_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P_5$. In these building blocks, we consistently use channel 256 of feature maps. All convolutional layers are followed by a ReLU. The feature grid for each proposal is then pooled from new feature maps, i.e., ${N_2, N_3, N_4, N_5}$.

#### Adaptive Feature Pooling

Adaptive Feature Pooling is a component of the PANet framework designed for instance segmentation. It involves pooling features from all levels of the feature hierarchy for each proposal and fusing them for prediction. It was added to PANet to address the limitations of traditional methods where proposals were assigned to different feature levels based on their size, which could lead to suboptimal results. By pooling features from all levels for each proposal, Adaptive Feature Pooling allows the network to access context information from multiple levels, leading to more accurate predictions. This approach ensures that both small proposals can access high-level features with rich context information, and large proposals can access low-level features with fine details and high localization accuracy. Ultimately, Adaptive Feature Pooling enhances the network's ability to make accurate predictions for instance segmentation tasks

#### Fully Connected Fusion

Fully-connected Fusion is a technique used in the PANet framework to improve mask prediction in instance segmentation. It involves adding a fully-connected branch to the mask prediction branch, which fuses predictions from both fully-connected layers and convolutional layers. It was added because fully-connected layers have different properties compared to convolutional layers. Fully-connected layers are location-sensitive and can adapt to different spatial locations, making them effective in predicting masks for instances. By fusing predictions from both fully-connected and convolutional layers, PANet aims to leverage the strengths of each type of layer to enhance mask prediction accuracy and differentiate between instances more effectively. The figure below shows how fully connected fusion works.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/9fa8c975-9f08-46c6-92c9-1c9aaa60d1d9" alt="PANet_fc_fusion" height="350"/>
</p>

## Mish
2019 | [paper](https://arxiv.org/vc/arxiv/papers/1908/1908.08681v1.pdf) | _Mish: A Self Regularized Non-Monotonic Neural Activation Function_
TODO



## CSP
2019 | [paper](https://arxiv.org/pdf/1911.11929.pdf) | _CSPNet: A new backbone that can enhance learning capability of CNN_
TODO



## ATSS
2020 | [paper](https://arxiv.org/pdf/1912.02424) | _Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection_
TODO



## EfficientDet
2020 | [paper](https://arxiv.org/pdf/1911.09070) | _EfficientDet: Scalable and Efficient Object Detection_
TODO



## RepVGG
2021 | [paper](https://arxiv.org/pdf/2101.03697.pdf) | _RepVGG: Making VGG-style ConvNets Great Again_
TODO



## ELAN
2022 | [paper](https://arxiv.org/pdf/2211.04800) | _Designing Network Design Strategies Through Gradient Path Analysis_
TODO



## PRB-FPN
2023 | [paper](https://arxiv.org/pdf/2012.01724) | _Parallel Residual Bi-Fusion Feature Pyramid Network For Accurate Single-Shot Object Detection_
TODO



---

# YOLO

## **YOLO v1**
2016 | [paper](https://arxiv.org/pdf/1506.02640.pdf) | _You Only Look Once: Unified, Real-Time Object Detection_

YOLOv1 is a **single-stage** object detection model. Object detection is framed as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/596ae49e-22d6-4a11-85da-d89a50df9a3b" alt="yolo_v1" height="250"/>
</p>

### How it works

* YOLO divides the input image into an $S × S$ grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.
* Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how
confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. The confidence score is defined as $Pr(Object) ∗ IoU(pred, gt)$. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.
* Each bounding box consists of 5 predictions: $x, y, w, h$, and confidence. The $(x, y)$ coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. Finally the confidence prediction represents the IoU between the predicted box and any ground truth box.
* Each grid cell also predicts $C$ conditional class probabilities, $Pr(Class_i|Object)$. These probabilities are conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes $B$.
* predictions are encoded as an $S × S × (B ∗ 5 + C)$ tensor ($S$ - grid size, $B$ - number of bboxes, $C$ - number of classes). In paper: $7 × 7 × 30$, that is $S = 7, B = 2, C = 20$ (PASCAL VOC has 20 labels)

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/2304ce9a-56e2-450e-be12-6eb5294b561d" alt="yolo_how" width="500"/>
</p>

### Model architecture

* The initial convolutional layers of the network extract features from the image while the fully connected layers predict the output probabilities and coordinates.
* The network architecture is inspired by the GoogLeNet. It has 24 convolutional layers followed by 2 fully connected layers
* Instead of the inception modules used by GoogLeNet, the $1 × 1$ conv reduction layers followed by $3 × 3$ convolutional layers are used
* To avoid overfitting use dropout and extensive data augmentation. A dropout layer with $rate = 0.5$ after the first connected layer prevents co-adaptation between layers

### Loss function
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/a29f820b-b37f-472e-b5ea-52456b4383eb" alt="yolo_loss" width="500"/>
</p>

### Training details

* pretrain first 20 conv layers (+ global pool and fc) as classifier on the ImageNet -> val top-5 88% accuracy
* add 4 conv layers and 2 fc layers on top of the pretrained backbone and increase input size from 224 to 448 to train the detector
* normalize bbox width and height to _[0, 1]_ and parametrize bbox x and y to be offsets of a particular grid cell location so they are also bounded in _[0, 1]_
* LeakyReLU(0.1)
* use different loss weights for case of no object cell and for coordinate losses
* use squared root of width and height in loss (to reflect that small deviations in large boxes matter less than in small boxes)
* YOLO predicts multiple bounding boxes per grid cell. At training time only one bbox predictor
is responsible for each object. One predictor is assigned to be “responsible” for predicting an object based on which prediction has the highest current IoU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall
* The loss function only penalizes classification error if an object is present in that grid cell. It also only penalizes bbox coordinate error if that predictor is “responsible” for the ground truth box (i.e. has the highest IoU of any predictor in that grid cell)
* Training scheme:
	* 135 epochs on the training and validation data sets from PASCAL VOC 2007 and 2012 (when testing on 2012 we also include the VOC 2007 test data for training)
	* batch size = 64
	* momentum = 0.9 
	* weight decay = 0.0005
	* Learning rate schedule:
		* For the first epochs slowly raise the learning rate from 0.001 to 0.01 (If started at a high learning rate the model often diverges due to unstable gradients)
		* Continue training with 0.01 for 75 epochs
		* Then 0.001 for 30 epochs
		* Finally 0.0001 for 30 epochs
	* data augmentation:
		* random scaling,
		* translations of up to 20% of the original image size
		* randomly adjust the exposure and saturation of the image by up to a factor of 1.5 in the HSV color space

## **YOLO v2**
2016 | [paper](https://arxiv.org/pdf/1612.08242.pdf) | _YOLO9000: Better, Faster, Stronger_

YOLOv2 (or YOLO9000) is a single-stage real-time object detection model. It improves upon [_YOLOv1_](#yolo-v1) in several ways, including the use of Darknet-19 as a backbone, batch normalization, use of a high-resolution classifier, multi-scale training, the use of dimension clusters, fine-grained features and direct location prediction to predict bounding boxes, and more

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3956c34f-f205-4366-9cbb-83f2308689d6" alt="yolo_v2" height="250"/>
</p>

### How it works

It works similar to the YOLOv1, the main differences include each predicted bbox has its own C class probabilities, so the predictions are encoded as an _S × S × (B ∗ (5 + C))_ tensor and added passtrough layer, dimension clusters prior and direct location prediction for easier training.
Main improvements:
* **Batch Normalization** - leads to significant improvements in convergence while eliminating the need for other forms of regularization (removed dropout from fc). By adding batch normalization on all of the convolutional layers in YOLO mAP improved by more than 2%
* **High Resolution Classifier** - the original YOLO trains the classifier network at _224 × 224_ and increases the resolution to 448 for detection. This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution. For YOLOv2 we first fine tune the classification network at the full _448 × 448_ resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better
on higher resolution input. Then finetune the resulting network on detection. This high resolution classification network gives an increase of almost 4% mAP
* **Dimension Clusters** - authors encountered two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. If better priors for anchor boxes are picked for the network to start with it can make it easier to learn to predict good detections. Instead of choosing priors by hand, _k-means_ clustering is used on the training set bounding boxes to automatically find good priors. Distance metric is based on IoU, that is: _d(box, centroid) = 1 − IoU(box, centroid)_. By analysis, the _k = 5_ value is chosen for the number of priors.
* **Direct location prediction** - Instead of predicting offsets (for anchors) the similar to YOLOv1 approach is used, that is predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall in _[0, 1]_. The Logistic activation function is used to constrain the network’s predictions to fall in this range. The network predicts 5 bboxes at each cell in the output feature map. The network predicts 5 coordinates for each bbox, _tx_, _ty_ , _tw_, _th_, and to. If the cell is offset from the top left corner of the image by _(cx, cy )_ and the bbox prior has width and height _pw_, _ph_, then the predictions correspond to:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/848f9316-b3a4-49db-b8bc-e6be7acbeeeb" alt="yolov2_bbox_pic" height="200"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/ca70fb43-1cfd-4b93-baf3-4c64b0c707cf" alt="yolov2_bbox_def" height="200"/>
</p>

* **Fine-Grained Features** - YOLOv2 predicts detections on a _13 × 13_ feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. The passthrough layer is added that brings features from an earlier layer at _26 × 26_ resolution. The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the _26 × 26 × 512_ feature map into a _13 × 13 × 2048_ feature map, which can be concatenated with the original features
* **Multi-Scale Training**. The original YOLO uses an input resolution of _448 × 448_. With the addition of anchor boxes the resolution is changed to _416 × 416_. However, since the model only uses convolutional and pooling layers it can be resized on the fly. The YOLOv2 is designed to be robust to running on images of different sizes. Instead of fixing the input image size, it is changed every few iterations. Every 10 batches a new image dimension size is chosen from multiples of 32 (model downsamples by a factor of 32): {320, 352, ..., 608}  Thus the smallest option is _320 × 320_ and the largest is _608 × 608_.

### Model architecture

**Darknet-19** - a new classification model to be used as the backbone of YOLOv2. Similar to the VGG models, Darknet-19 use mostly _3 × 3_ filters and double the number of channels after every pooling step. Following the work on Network in Network (NIN) the global average pooling to is used make predictions as well as _1 × 1_ filters to compress the feature representation between _3 × 3_ convolutions. Batch normalization is used to stabilize training, speed up convergence, and regularize the model. The final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling It achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet.

### Training details:
**Training for classification** - the network is trained on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent (SGD) with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. During training the standard data augmentation tricks are used, including random crops, rotations, and hue, saturation, and exposure shifts. As discussed above, after initial training on images at _224 × 224_ the model is finetuned at a larger size, _448_ for 10 epochs starting at learning rate of 0.001. After finetuning the model achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.

**Training for detection** - for detection the last convolutional layer is removed and instead three _3 × 3_ convolutional layers are added (each with 1024 filters) and a final _1 × 1_ convolutional layer with the number of outputs needed for detection (for VOC 5 boxes are predicted, each with 5 coordinated and 20 classes, so 125 filters in total). The passtrough layer is also added from the final _3 × 3 × 512_ layer to the second to last convolutional layer so that the model can use fine grain features. The network is trained for 160 epochs with a starting
learning rate of 0.001, dividing it by 10 at 60 and 90 epochs. The weight decay is 0.0005, momentum is 0.9 and data augmentation includes random crops, color shifting, and other similar to YOLO and SSD

## **YOLO v3**
2018 | [paper](https://arxiv.org/pdf/1804.02767.pdf) | _YOLOv3: An Incremental Improvement_
_YOLOv3_ is a real-time, single-stage object detection model that builds on [_YOLOv2_](#yolo-v2) with several improvements. Improvements include the use of a new backbone network, Darknet-53 that utilises residual connections, as well as some improvements to the bbox prediction step, and use of three different scales from which to extract features. Ultralytics implementation can be found [here](https://github.com/ultralytics/yolov3).

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c86504c6-7712-4ceb-a358-279ae53ebb0e" alt="yolo_v3" height="400"/>
</p>

### How it works

* Similar to YOLOv2, the model outputs a tensor of size _S × S × [B ∗ (5 + C)]_, but now the model outputs bboxes at three different scales
* Different to previous versions, YOLOv3 uses multiple independent logistic classifiers rather than one softmax layer for each class. During training, they use binary cross-entropy loss in a one vs all setup (using a softmax imposes the assumption that each box has exactly one class which is often not the case - the multilabel approach better models the data)
* The bigger backbone is used (DarkNet-53) for feature extraction - the architecture has alternative _1 × 1_ and _3 × 3_ convolution layers and skip/residual connections inspired by the ResNet model. Although DarkNet53 is smaller than ResNet101 or RestNet-152, it is faster and has equivalent or better performance
* The idea of FPN (Feature Pyramid Networks) is added to leverage the benefit from all the prior computations and fine-grained features early on in the network
* The bboxes anchors are defined using k-means (same as in YOLOv2), but YOLOv3 uses three prior boxes for different scales

### Model architecture

* **Darknet-53 classifier** - The network is a hybrid approach between the network used in YOLOv2 (Darknet-19), and the residual networks. It uses successive _3 × 3_ and _1 × 1_ convolutional layers but now has some shortcut connections as well and is significantly larger (53 layers)

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/fb6d5b7d-9d6d-4c93-8fa1-314e76f96b2d" alt="darknet_53" height="400"/>
</p>

* **Detector** - From the base feature extractor several convolutional layers are added:
	* The last of these predicts a _3-d_ tensor encoding bounding box, objectness, and class predictions. For COCO the predictions include 3 boxes at each scale so the tensor is _S × S × [3 ∗ (4 + 1 + 80)]_ for the 4 bbox offsets, 1 objectness prediction, and 80 class predictions
 	* Next the feature map from 2 layers previous is taken and is upsampled by 2x. The feature map from earlier point in the network is also taken and it is merged with the upsampled features using concatenation. This method allows to get more meaningful semantic information from the upsampled features and finer-grained information from the earlier feature map. On top of that a few more convolutional layers are added to process this combined feature map, and eventually predict a similar tensor (_S × S × [3 ∗ (4 + 1 + 80)]_), although now twice the size.
  	* The same design if performed one more time to predict boxes for the final scale. Thus the predictions for the 3rd scale benefit from all the prior computation as well as fine-grained features from early on in the network.

### Training details:
The network is trained similar to YOLOv2

## **Gaussian YOLO**
2019 | [paper](https://arxiv.org/pdf/1904.04620) | _Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving_
TODO


## **PP-YOLO**
2020 | [paper](https://arxiv.org/pdf/1904.04620) | _PP-YOLO: An Effective and Efficient Implementation of Object Detector_
TODO


## **YOLO v4**
2020 | [paper](https://arxiv.org/pdf/2004.10934.pdf) | _YOLOv4: Optimal Speed and Accuracy of Object Detection_

YOLOv4 addresses the need for real-time object detection systems that can achieve high accuracy while maintaining fast inference speeds and possibility to train the object detector on a single customer GPU. Authors of the paper put a great emphasis on analyzing and evaluating the possible training and architectural choices for object detector training and architecture. Two sets of concepts were evaluated separately, that is the Bag of Freebies ([**BoF**](#bag-of-freebies)) and Bag of Specials ([**BoS**](#bag-of-specials)). 

Authors noticed that the reference model which is optimal for classification is not always optimal for a detector. In contrast to the classifier, the detector requires the following:
* Higher input network size (resolution) – for detecting multiple small-sized objects
* More layers – for a higher receptive field to cover the increased size of input network
* More parameters – for greater capacity of a model to detect multiple objects of different sizes in a single image
They also noticed that the influence of the receptive field with different sizes is summarized as follows:
* Up to the object size - allows viewing the entire object
* Up to network size - allows viewing the context around the object
* Exceeding the network size - increases the number of connections between the image point and the final activation


### How it works

Different settings of BoF and BoS were tested for two backbones, that is CSPResNext50 and CSPDarknet53 (CSP module applied to ResNext50 and Darknet53 architectures). The settings include:
* Activations: ReLU, leaky-ReLU, Swish, or Mish
* bbox regression loss: MSE, IoU, GIoU, CIoU, DIoU
* Data augmentation: CutOut, MixUp, CutMix, Mosaic, Self-Adversarial Training
* Regularization method: DropBlock
* Normalization of the network activations by their mean and variance: Batch Normalization ([BN](https://arxiv.org/pdf/1502.03167)), Cross-Iteration Batch Normalization ([CBN](https://arxiv.org/pdf/2002.05712))
* Skip-connections: Residual connections, Weighted residual connections, Multiinput weighted residual connections, or Cross stage partial connections (CSP)
* Eliminate grid sensitivity problem - equation used in YOLOv3 to calculate object coordinates had a flaw related to the need of very high absolute values predictions for points on grid. Authors solved this problem through multiplying the sigmoid by a factor exceeding 1.0, so eliminating the effect of grid on which the object is undetectable
* IoU threshold - using multiple anchors for a single ground truth IoU (truth, anchor) > IoU threshold
* Optimized Anchors - using the optimized anchors for training with the 512x512 network resolution

In order to make the designed detector more suitable for training on single GPU, authors made additional design and improvement as follows:
* Introduced a new method of data augmentation - Mosaic, and Self-Adversarial Training (SAT)
	* Mosaic - represents a new data augmentation method that mixes 4 training images. Thus 4 different contexts are while CutMix mixes only 2 input images. This allows detection of objects outside their normal context. In addition, batch normalization calculates activation statistics from 4 different images on each layer. This significantly reduces the need for a large mini-batch size
	* Self-Adversarial Training (SAT) - new data augmentation technique that operates in 2 forward backward stages. In the 1st stage the neural network alters the original image instead of the network weights. In this way the neural network executes an adversarial attack on itself, altering the original image to create the deception that there is no desired object on the image. In the 2nd stage, the neural network is trained to detect an object on this modified image in the normal way
* Selected optimal hyper-parameters while applying genetic algorithms
* Modified some exsiting methods to make YOLOv4 design suitble for efficient training and detection - modified SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)

### Model architecture

YOLOv4 consists of:
* Backbone - CSPDarknet53
* Neck - SPP and PAN
* Head - YOLOv3 head

YOLOv4 Backbone uses:
* BoF: CutMix and Mosaic data augmentation, DropBlock regularization, Class label smoothing
* BoS: Mish activation, Cross-stage partial connections (CSP), Multiinput weighted residual connections (MiWRC)

YOLOv4  Detector uses:
* BoF: CIoU-loss, CmBN, DropBlock regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid
sensitivity, Using multiple anchors for a single ground truth, [Cosine annealing scheduler](https://arxiv.org/pdf/1608.03983v5), Optimal hyperparameters, Random training shapes
* BoS: Mish activation, modified SPP-block, modified SAM-block, modified PAN path-aggregation block, DIoU-NMS

Modified SPP, PAN and PAN:
<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/cf3549cd-b72f-4369-9c86-50feef5ffc42" alt="modified_SPP" height="300"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3aaee815-a819-41dd-9b0f-a11c0223af92" alt="modified_SAM" height="300"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/72e5c007-20ff-44e9-a42d-b90d0ee2e019" alt="modified_PAN" height="300"/>
</p>

### Training details

**Classification** (ImageNet): 
* training steps is 8,000,000
* batch size and the mini-batch size are 128 and 32, respectively
* polynomial decay learning rate scheduling strategy is adopted with initial learning rate 0.1
* warm-up steps is 1000
* the momentum and weight decay are respectively set as 0.9 and 0.005
* All BoS experiments use the same hyper-parameter as the default setting, and in the BoF experiments, additional 50% training steps are added
* BoF experiments: MixUp, CutMix, Mosaic, Bluring data augmentation, and label smoothing regularization methods
* BoS experiments: LReLU, Swish, and Mish activation function. All experiments are trained with a 1080 Ti or 2080 Ti GPU.

**Object detection** (MS COCO): 
* training steps is 500,500
* step decay learning rate scheduling strategy is adopted with initial learning rate 0.01 and multiply with a factor 0.1 at the 400,000 steps and the 450,000 steps, respectively
* momentum and weight decay are respectively set as 0.9 and 0.0005
* All architectures use a single GPU to execute multi-scale training in the batch size of 64
* mini-batch size is 8 or 4 depend on the architectures and GPU memory limitation
* Except for using genetic algorithm for hyper-parameter search experiments, all other experiments use default setting
* Genetic algorithm used YOLOv3-SPP to train with GIoU loss and search 300 epochs for min-val 5k sets. Searched settings are adopted:  learning rate 0.00261, momentum 0.949, IoU threshold for assigning ground truth 0.213, and loss normalizer 0.07 for genetic algorithm experiments
* For all experiments, only one GPU is used for training, so techniques such as syncBN that optimizes multiple GPUs are not used
* BoF experiments: grid sensitivity elimination, mosaic data augmentation, IoU threshold, genetic algorithm, class label smoothing, cross mini-batch normalization, self-adversarial training, cosine annealing scheduler, dynamic mini-batch size, DropBlock, Optimized Anchors, different kind of IoU losses
* BoS experiments: Mish, SPP, SAM, RFB, BiFPN, and [Gaussian YOLO](https://arxiv.org/pdf/1904.04620)


## **YOLOv5**
2020 | [code](https://github.com/ultralytics/yolov5), [docs](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/) | _YOLOv5_

YOLOv5 is built on top of the work of [_YOLO v3_](#yolo-v3) and [_YOLO v4_](#yolo-v4). All the YOLOv5 models are composed of the same 3 components: _CSP-Darknet53_ as a backbone, [_SPP_](#SPP) (**S**patial **P**yramid **P**ooling) and [_PANet_](#PAN) (**P**ath **A**ggregation **Net**works) in the model neck and the head used in YOLOv4. Most of the YOLOv5 blocks are improved using the [_CSP_](#CSP) (**C**ross **S**tage **P**artial) module.

The key changes in YOLOv5 that didn't exist in previous version are: applying the CSPNet to the Darknet53 backbone, the integration of the Focus layer (conv 6x6) to the CSP-Darknet53 backbone, replacing the SPP block by the SPPF block in the model neck and applying the CSPNet strategy on the PANet model. YOLOv5 and YOLOv4 tackled the problem of grid sensitivity and can now detect easily bounding boxes having center points in the edges. Finally, YOLOv5 is lighter and faster than previous versions.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d3563351-69f1-4a3c-ad5f-704d744b057b" alt="yolo_v5_short" height="350"/>
</p>

### How it works

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/821b0953-8822-4cfe-bde4-fdfcfc6f2fa1" alt="yolo_v5_brief" height="200"/>
</p>

* The image is processed through an input layer (input) and sent to the backbone for feature extraction.
* Backbone - creates feature maps of different sizes
* Neck - fuses these features through the feature fusion network to finally generate three feature maps P3, P4, and P5 (in the YOLOv5, the dimensions are expressed with the size of _80 × 80_, _40 × 40_ and _20 × 20_) to detect small, medium, and large objects in the picture, respectively.
* Head - three feature maps are sent to the prediction head, the confidence calculation and bbox regression are executed for each pixel in the feature map using the prior anchors, so as to obtain a multi-dimensional array (bboxes) including object class, class confidence, box coordinates, width, and height information.
* Postprocessing (NMS) - by setting the corresponding thresholds (confthreshold, objthreshold) to filter the useless information in the array, and performing a non-maximum suppression (NMS) process, the final detection is done

### Model architecture

An example of yolov5L architecture:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c7660fce-8a0c-41b5-a429-1ca77d0f597c" alt="yolo_v5_L" height="600"/>
</p>

* **Bakcbone** - _CSP-Darknet53_ architecture based on Darknet53 from YOLOv3 to which the authors applied the Cross Stage Partial (_CSP_) network strategy.
YOLO is a deep network, it uses residual and dense blocks in order to enable the flow of information to the deepest layers and to overcome the vanishing gradient problem. However one of the perks of using dense and residual blocks is the problem of redundant gradients. _CSPNet_ helps tackling this problem by truncating the gradient flow. According to the authors of CSP:

> _CSP_ network preserves the advantage of _DenseNet's_ feature reuse characteristics and helps reducing the excessive amount of redundant gradient information by truncating the gradient flow.

_YOLOv5_ employs _CSPNet_ strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy as shown in the figure bellow

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/e7bf01c8-a015-4f49-a216-e3549a4c3612" alt="yolo_v5_CSP" height="250"/>
</p>

Applying this strategy comes with big advantages to YOLOv5, since it helps reducing the number of parameters and helps reducing an important amount of computation (less FLOPS) which lead to increasing the inference speed that is crucial parameter in real-time object detection models

* **Neck** - YOLOv5 brought two major changes to the model neck. First a variant of Spatial Pyramid Pooling (_SPP_) has been used, and the Path Aggregation Network (_PANet_) has been modified by incorporating the _BottleNeckCSP_ in its architecture

_PANet_ is a feature pyramid network, it has been used in previous version of YOLO (YOLOv4) to improve information flow and to help in the proper localization of pixels in the task of mask prediction. In YOLOv5 this network has been modified by applying the CSPNet strategy to it as shown in the network's architecture figure

_SPP_ block performs an aggregation of the information that receives from the inputs and returns a fixed length output. Thus it has the advantage of significantly increasing the receptive field and segregating the most relevant context features without lowering the speed of the network. This block has been used in previous versions of YOLO (YOLOv3 and YOLOv4) to separate the most important features from the backbone, however in YOLOv5(6.0/6.1) SPPF has been used , which is just another variant of the SPP block, to improve the speed of the network (same outputs, but faster).

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8f95fa80-0184-48c5-97fb-55a5c08f123a" alt="yolo_v5_SPP" height="200"/>
</p>

* **Head** - _YOLOv5_ uses the same head as _YOLOv3_ and _YOLOv4_. It is composed from three convolution layers that predicts the location of the bounding boxes _(x, y, height, width)_, the scores and the objects classes. The equation to compute the target coordinates for the bounding boxes have changed from previous versions, the difference is shown in the figure bellow.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/8d62b83e-0916-4a2c-a37f-ae3e731f5301" alt="yolo_v5_head_eq" height="150"/>
</p>

* **Activation** - for YOLOv5 the authors went with _SiLU_ and _Sigmoid_ activation function. _SiLU_ stands for Sigmoid Linear Unit and it is also called the swish activation function. It has been used with the convolution operations in the hidden layers. While the Sigmoid activation function has been used with the convolution operations in the output layer

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/3e2142d1-4b9e-4816-b5d5-0aacb69a3bb9" alt="yolo_v5_activation" height="200"/>
</p>

### Loss function
YOLOv5 returns three outputs: the classes of the detected objects, their bounding boxes and the objectness scores. Thus, it uses _BCE_ (Binary Cross Entropy) to compute the classes loss and the objectness loss. While [_CIoU_](#ciou) (Complete Intersection over Union) loss to compute the location loss. The loss in YOLOv5 is computed as a combination of three individual loss components:

* Classes Loss (**BCE Loss**) - Binary Cross-Entropy loss, measures the error for the classification task.
* Objectness Loss (**BCE Loss**) - Another Binary Cross-Entropy loss, calculates the error in detecting whether an object is present in a particular grid cell or not.
* Location Loss (**CIoU Loss**) - Complete IoU loss, measures the error in localizing the object within the grid cell.

The formula for the final loss is given by the following equation

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/57d86a5a-2f08-4a34-a7bd-21fd39a9988b" alt="yolo_v5_loss" height="40"/>
</p>

The objectness losses of the three prediction layers (P3, P4, P5) are weighted differently. The balance weights are _[4.0, 1.0, 0.4]_ respectively. This approach ensures that the predictions at different scales contribute appropriately to the total loss.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d1aaf623-29bf-45dd-9574-7d103a260646" alt="yolo_v5_loss_balance" height="70"/>
</p>

### Other improvements

* **First conv layers** - replaced the three first layers of the network with single 6x6 convolution. It helped reducing the number of parameters, the number of FLOPS and the CUDA memory while improving the speed of the forward and backward passes with minor effects on the mAP.
* **Eliminating grid sensitivity** - It was hard for the previous versions of YOLO to detect bounding boxes on image corners mainly due to the equations used to predict the bounding boxes, but the new equations presented above helped solving this problem by expanding the range of the center point offset from [0, 1] to _[-0.5, 1.5]_ (left) therefore the offset can be easily 1 or 0 (coordinates can be in the image's edge) as shown in the image in the left. Also the height and width scaling ratios (right) were unbounded in the previous equations which may lead to training instabilities but now this problem has been reduced.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/d49af842-4256-4ae9-83f4-754d2ef335fb" alt="yolo_v5_center" height="200"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/91d65749-93af-4eb7-827e-dc8b974f8fcf" alt="yolo_v5_hw" height="200"/>
</p>

* **Targets building** - the targets building process in YOLOv5 is critical for training efficiency and model accuracy. It involves assigning ground truth boxes to the appropriate grid cells in the output map and matching them with the appropriate anchor boxes. This process follows these steps:
    1. Calculate the ratio of the ground truth box dimensions and the dimensions of each anchor template
    2. If the calculated ratio is within the threshold, match the ground truth box with the corresponding anchor
    3. Assign the matched anchor to the appropriate cells, keeping in mind that due to the revised center point offset, a ground truth box can be assigned to more than one anchor. Because the center point offset range is adjusted from _[0, 1]_ to _[-0.5, 1.5]_. GT Box can be assigned to more anchors.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/96994899-f1f9-46fe-8ad6-e3f3071208b9" alt="yolo_v5_target_build_1" height="150"/>
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/fce7cd74-24e4-4351-ac53-841f12137e9a" alt="yolo_v5_target_build_2" height="150"/>
</p>

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/c65841ad-51c0-47f5-af45-fcfe9eee20bd" alt="yolo_v5_target_build_3" height="250"/>
</p>

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/6164c3e2-4c92-4dbf-aeaf-34f1eb1326f9" alt="yolo_v5_target_build_4" height="200"/>
</p>

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/e47185fd-3a7d-4896-b0ba-ecb2fbcf5a19" alt="yolo_v5_target_build_5" height="280"/>
</p>



### Training details

YOLOv5 applies several sophisticated training strategies to enhance the model's performance. They include
* **Multiscale Training** - the input images are randomly rescaled within a range of 0.5 to 1.5 times their original size during the training process
* **Warmup** and **Cosine LR Scheduler** - a method to adjust the learning rate to enhance model performance
* **AutoAnchor** - this strategy optimizes the prior anchor boxes to match the statistical characteristics of the ground truth boxes for custom data
* **Exponential Moving Average (EMA)** - A strategy that uses the average of parameters over past steps to stabilize the training process and reduce generalization error
* **Mixed Precision Training** - A method to perform operations in half-precision format, reducing memory usage and enhancing computational speed
* **Hyperparameter Evolution** - A strategy to automatically tune hyperparameters to achieve optimal performance.
* **Data augmentation**:
	* _Mosaic Augmentation_ - combines four training images into one in ways that encourage object detection
 	* _Copy-Paste Augmentation_ - copies random patches from an image and pastes them onto another randomly chosen image, effectively generating a new training sample models to better handle various object scales and translations
	* _Random Affine Transformations_ - this includes random rotation, scaling, translation, and shearing of the images
 	* _MixUp Augmentation_ - creates composite images by taking a linear combination of two images and their associated labels
	* _HSV Augmentation_ - random changes to the Hue, Saturation, and Value of the images
	* _Random Horizontal Flip_ - randomly flips images horizontally

Sources: [[1](https://sh-tsang.medium.com/brief-review-yolov5-for-object-detection-84cc6c6a0e3a)], [[2](https://iq.opengenus.org/yolov5/)], [[3](https://github.com/ultralytics/yolov5/issues/6998#1)]

## **Scaled YOLO v4**
2021 | [paper](https://arxiv.org/pdf/2011.08036.pdf) | _Scaled-YOLOv4: Scaling Cross Stage Partial Network_
Scaled version of YOLOv4 built on top of the [YOLOv4](#yolo-v4). Authors have shown that the YOLOv4 object detection neural network based on the [CSP](#csp) approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. They proposed a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network.

### How it works
When designing the efficient model scaling methods, authors main principles are:
* when the scale is up, the lower the quantitative cost we want to increase, the better
* when the scale is down, the higher the quantitative cost we want to decrease, the better

The [CSP](https://arxiv.org/pdf/1911.11929) can be applied to various CNN architectures, while reducing the amount of parameters and computations. In addition, it also improves accuracy and reduces inference time. CSP connection is extremely efficient, simple, and can be applied to any neural network. The idea is:
* half of the output signal goes along the main path (generates more semantic information with a large receiving field)
* and the other half of the signal goes bypass (preserves more spatial information with a small perceiving field)

Due to that, authors used CSP-ized models to perform model scaling. 

In case of the _tiny_ model, authors wanted the computations to be even smaller, so they adapted the [OSANet](https://arxiv.org/pdf/1904.09730) upgraded by [CSP](https://arxiv.org/pdf/1911.11929) module to create the CSPOSANet backbone

Usually, it is possible to adjust the scaling factors of an object detector’s input, backbone, and neck. The potential scaling factors that can be adjusted are summarized:
* Input: _size_
* Backbone: _width_, _depth_, _#stage_ (number of stages)
* Neck: _width_, _depth_, _#stage_ (number of stages)

The biggest difference between image classification and object detection is that the former only needs to identify the category of the largest component in an image, while the latter needs to predict the position and size of each object in an image. In one-stage object detector, the feature vector corresponding to each location is used to predict the category and size of an object at that location. The ability to better predict the size of an object basically depends on the receptive field of the feature vector. In the CNN architecture, the thing that is most directly related to receptive field is the stage, and the feature pyramid network (FPN) architecture tells us that higher stages are more suitable for predicting large objects. When the input image size is increased, if one wants to have a better prediction effect for large objects, he/she must increase the depth or number of stages of the network. When performing scaling up, authors first perform compound scaling on input size, and number of stages, and then according to real-time requirements, we further perform scaling on depth and width respectively
 
### Model architecture

* **Backbone** - in the design of CSPDarknet53, the computation of down-sampling convolution for cross-stage process is not included in a residual block. Therefore, we can deduce that CSPDarknet stage will have a better computational advantage over Darknet stage only when number of layers in a block is greater than 1. The number of residual layer owned by each stage in CSPDarknet53 is 1-2-8-8-4 respectively. In order to get a better speed/accuracy trade-off, the first CSP stage is converted into original Darknet residual layer
* **Neck** - in order to effectively reduce the amount of computation, [CSP](https://arxiv.org/pdf/1911.11929) is applied to the [PAN](https://arxiv.org/pdf/1803.01534) architecture in YOLOv4. The computation list of a PAN architecture is illustrated below (left). It mainly integrates the features coming from different feature pyramids, and then passes through two sets of reversed Darknet residual layer without shortcut connections. After applying [CSP](https://arxiv.org/pdf/1911.11929) (right), the computation is cut down by 40% and the neck is called CSPPAN.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/dfa8d3b9-34d6-4244-a370-a3f2834f9037" alt="reversed_CSP_dark_layers_SPP" height="300"/>
</p>

* **SPP** - the [SPP](https://arxiv.org/pdf/1406.4729) module was originally inserted in the middle position of the first computation list group of the neck. Therefore, in Scaled YOLOv4, the SPP is also inserted in the middle position of the first computation list group of the CSPPAN

Improvements in Scaled YOLOv4 over YOLOv4:
* Improved network architecture - backbone is optimized and Neck (PAN) uses CSP connections and Mish activation
* Exponential Moving Average (EMA) is used during training — this is a special case of Stochastic Weight Averaging ([SWA](https://arxiv.org/pdf/1803.05407), [SWA-PyTorch](https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/))
* Multiresolution - For each resolution of the network, a separate neural network is trained (in YOLOv4, only one neural network was trained for all resolutions)
* Improved objectness normalizers in prediction layers
* Changed activations for Width and Height, which allows faster network training
* Pre Processing - the Letter Box pre processing is used (?) to keep the aspect ratio of the input image (tiny version doesnt use Letter Box)

#### YOLOv4-tiny
YOLOv4-tiny is designed for low-end GPU device. The _CSPOSANet_ with _PCB_ (Partial in Computational Block) architecture is used as backbone. Growth rate (_g_) is set to _b/2_  (_b_ - base width) and it grows to _b/2 + kg = 2b_ at the end (_k_ - number of layers). Through calculation,  _k = 3_ is set, and its architecture and the computational block of YOLOv4-tiny is shown below. As for the number of channels of each stage and the part of neck, the design of YOLOv3-tiny is followed.

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/81da6cf6-6ce8-4e41-a5d4-aa47038c4b17" alt="yolo_v4_tiny" height="300"/>
</p>

#### YOLOv4-large
YOLOv4-large is designed for cloud GPU, the main purpose is to achieve high accuracy for object detection. Authors designed a fully CSP-ized model YOLOv4-P5 and scaled it up to YOLOv4-P6 and YOLOv4-P7. The structure of YOLOv4-P5, YOLOv4-P6, and YOLOv4-P7 is shown below. The compound scaling of input size and number of stages was performed. The depth scale of each stage was set to to 2^dsi , and ds to [1, 3, 15, 15, 7, 7, 7]. Finally, the width scaling was performed with the use of inference time as constraint.



<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/9c5e0666-d82d-4944-8877-889783e02090" alt="yolo_v4_large" height="400"/>
</p>

### Training details
The models are trained directly on MS COCO dataset (no pre-training on ImageNet). The training choices include:
* SGD optimizer
* Training time:
	* YOLOv4-tiny: 600 epochs,
	* YOLOv4-CSP: 300 epochs, 
	* YOLOv4-large: 300 epochs first and then followed by using stronger data augmentation method to train 150 epochs
* Hyperparameters (anchors, learning rate, degree of data augmentation) were found using k-means and genetic algorithms

### Loss functions
There are different Losses in YOLOv3, YOLOv4 and Scaled-YOLOv4:

<p align="center">
  <img src="https://github.com/thawro/yolo-pytorch/assets/50373360/7005d806-0928-4686-8dfa-cb79252a5ee4" alt="scaled_yolo_v4_losses" height="250"/>
</p>

* for _bx_ and _by_ — this eliminates grid sensitivity in the same way as in YOLOv4, but more aggressively
* for _bw_ and _bh_ — this limits the size of the bounded-box to _4*Anchor_size_

## **YOLO R**
2021 | [paper](https://arxiv.org/pdf/2105.04206) | _You Only Learn One Representation: Unified Network for Multiple Tasks_
TODO

## **YOLO X**
2021 | [paper](https://arxiv.org/pdf/2107.08430.pdf) | _YOLOX: Exceeding YOLO Series in 2021_
TODO

## **YOLO v6**
2022 | [paper](https://arxiv.org/abs/2209.02976) | _YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications_
TODO

## **YOLO v7**
2022 | [paper](https://arxiv.org/pdf/2207.02696.pdf) | _YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors_
TODO

## **YOLO v6.3**
2023 | [paper](https://arxiv.org/pdf/2301.05586.pdf) | _YOLOv6 v3.0: A Full-Scale Reloading_
TODO

## **Dynamic YOLO** 
2023 | [paper](https://arxiv.org/pdf/2304.05552) | _DynamicDet: A Unified Dynamic Architecture for Object Detection_
TODO

## **Review**
2023 | [paper](https://arxiv.org/pdf/2304.00501.pdf) | _A comprehensive review of YOLO: from YOLOv1 and beyond_
TODO

## **YOLOv8**
2023 | [paper](https://github.com/ultralytics/ultralytics) | _YOLOv8_
TODO

## **YOLO v9**
2023 | [paper](https://arxiv.org/pdf/2402.13616) | _YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information_
TODO



